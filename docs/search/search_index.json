{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to My ML Notes","title":"Welcome to My ML Notes"},{"location":"#welcome-to-my-ml-notes","text":"","title":"Welcome to My ML Notes"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/","text":"Linear Regression Linear regression Intro Dataset: \\(\\mathcal{D}=\\left\\{\\left(y_1, \\mathbf{x}_1\\right), \\ldots,\\left(y_n, \\mathbf{x}_n\\right)\\right\\}\\) Assume \\(y=h(\\mathbf{x})+z\\) Our ultimate goal is to predict the value \\(y\\) corresponding to a feature vector \\(\\mathbf{x}\\) that is not in the training set. Towards this goal, we \"learn\" the function from the dataset, which yields an estimate \\(\\hat{h}\\) of the function \\(h^*\\) , and then predict \\(y=\\hat{h}(\\mathbf{x})\\) . Since learning an arbitrary function is intractable and not a well posed problem, we assume that the function \\(h\\) lies in some hypothesis class \\(\\mathcal{H}\\) of allowable functions. Typically, we assume that the set of functions \\(\\mathcal{H}\\) is parameterized, meaning that there is some (parameter or weight) vector \\(\\boldsymbol{\\theta}\\) which determines a function in the class \\(\\mathcal{H}\\) . A concrete example is the class of all linear function, which is given as the set: suppose the function \\(h\\) is fully determined by the vector \\(\\boldsymbol{\\theta}\\) ; we write \\(h_{\\boldsymbol{\\theta}}\\) to make this dependence explicit. With the class of functions parameterized by a vector \\(\\boldsymbol{\\theta}\\) , the problem of learning \\(h\\) reduces to the problem of estimating the parameter \\(\\boldsymbol{\\theta}\\) . Learn the parameter We choose a cost function loss: \\(\\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}\\) that measures how well (for a fixed \\(\\boldsymbol{\\theta}\\) ) the predictions \\(h_{\\boldsymbol{\\theta}}(\\mathbf{x})\\) describe the output \\(y\\) , then find the parameter that minimizes that loss: $$ \\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\sum_{i=1}^n \\operatorname{loss}\\left(h_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_i\\right), y_i\\right) $$ Here, and in the following we use the hat-notation \\(\\hat{\\boldsymbol{\\theta}}\\) to indicate that this variable is estimated based on data. Loss function The most popular metric in linear regression is the sum of the squares of the fitting error: ( least square cost function/MSE ) $$ \\hat{R}(\\boldsymbol{\\theta})=\\frac{1}{n} \\sum_{i=1}^n\\left(\\left\\langle\\mathbf{x}_i, \\boldsymbol{\\theta}\\right\\rangle-y_i\\right)^2 \\=\\frac{1}{n}|\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\theta}|_2^2 $$ The least squares cost function is convenient for a variety of reasons: - it can be numerically minimized efficiently as it is convex (convexity is defined below) - it has intuitive geometric and probabilistic interpretations, as we will see later. Least squares estimate: the parameter vector that minimizes this cost function: $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\hat{R}(\\boldsymbol{\\theta}) $$ \u8ba8\u8bba\u7ebf\u6027\u56de\u5f52\u4e2dn\u548cd\u7684\u5173\u7cfb Before discussing how \\(\\hat{\\theta}_{\\mathrm{LS}}\\) can be computed, let us comment on the relation of the number of examples, \\(n\\) , and the number of features, \\(d\\) . Suppose that at least \\(n\\) columns of \\(\\mathbf{X}\\) are linearly independent. Then we must have \\(n \\leq d\\) , and regardless of \\(\\mathbf{y}\\) , we can find a parameter vector \\(\\boldsymbol{\\theta}\\) (or many different ones) such that \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) , and thus describe the independent variable \\(y\\) exactly. - In the linear regression model, this is only possible if there are fewer examples than number of model parameters \\((n \\leq d)\\) . In that regime, even if \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) have nothing to do with each other, we achieve a perfect fit or zero loss. In this regime, we are likely overfitting: the model is too flexible given the data. Thus, fitting a linear model without additional assumptions only makes sense in the so called over-determined or under-parameterized regime, where the number of training examples is larger than the number of features \\((n \\geq d)\\) . If we find a good approximation in the \\((n \\geq d)\\) -regime, then this is an indication that the linear model is capturing structure of the problem. n<=d \u4e0d\u8bbax\u548cy\u7684\u5173\u7cfb\uff0c \u90fd\u80fd\u627e\u5230\u4e00\u4e2a \\(\\theta\\) , \u4f7f\u5f97 \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) overfitting not make senses n>=d: \u5982\u679c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u627e\u5230\u4e86\u4e00\u4e2a\u597d\u7684\u8fd1\u4f3c\u6a21\u578b\uff0c \u5219\u8be5\u6a21\u578b\u6bd4\u8f83\u51c6\u786e Least squares estimation We next show that \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) can be computed in closed form. Assume that \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) has full column rank. - This implies that the number or training examples, \\(n\\) , is larger than the number of model parameters, \\(d\\) . - features of \\(x\\) are linearly independent In the regime \\(n \\geq d\\) , if \\(\\mathbf{X}\\) does not have full column rank, then the features are linearly dependent and thus redundant. Proposition 1. Suppose that \\(\\mathbf{X}\\) has full column rank \\(d\\) . Then for any \\(\\mathbf{y}\\) , we have $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y} $$ There are at least two proofs of this proposition. Analysis of the least squares estimate and in what sense co-linear features are \"bad\" Thus far, we have not commented on the features of the regression model. Intuitively, if the features are very similar to each other, least squares estimation should be difficult. For example, suppose all the features (i.e., the entries of the \\(\\mathbf{x}_i\\) ) are linear functions of each other. Then the columns of \\(\\mathbf{X}\\) are linearly dependent and \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) is not unique. In statistics it is common to refer to features with this property as co-linear . A set of variables is said to be perfectly multi-colinear if there is one or more exact linear relationship among some of the variables \\(x_{i 1}, \\ldots, x_{i d}\\) ; for example the first two variables are co-linear if \\(x_{i 1}=-2 x_{i 2}\\) for all \\(i\\) . In practice, features are usually not co-linear but can be highly correlated, which leads to instable least squares estimates. Case1. \\(X\\) has full column rank and \\(y\\) is generated according to a linear model with additive Gaussian noise: $$ \\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}^*+\\mathbf{z} $$ Here, the entries of \\(\\mathbf{z}\\) are independent zero-mean Gaussian noise with variance \\(1 / n\\) , Then: the expected error is 0, or in statistical terms, the LSE is unbiased Case2. Features of \\(X\\) are strongly correlated where \\(\\sigma_i\\) are the singular values of \\(\\mathbf{X}\\) , and expectation is over the Gaussian noise \\(\\mathbf{z}\\) then \\(1 / \\sigma_{\\min }^2\\) is large and the bound becomes large. This is the case if some of the features are highly correlated, known as multicollinearity in the statistics literature. On the other hand, as expected, the error decays in \\(n\\) , the larger \\(n\\) , the smaller the error. proof of the error function:","title":"Linear Regression"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#linear-regression","text":"","title":"Linear Regression"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#linear-regression_1","text":"","title":"Linear regression"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#intro","text":"Dataset: \\(\\mathcal{D}=\\left\\{\\left(y_1, \\mathbf{x}_1\\right), \\ldots,\\left(y_n, \\mathbf{x}_n\\right)\\right\\}\\) Assume \\(y=h(\\mathbf{x})+z\\) Our ultimate goal is to predict the value \\(y\\) corresponding to a feature vector \\(\\mathbf{x}\\) that is not in the training set. Towards this goal, we \"learn\" the function from the dataset, which yields an estimate \\(\\hat{h}\\) of the function \\(h^*\\) , and then predict \\(y=\\hat{h}(\\mathbf{x})\\) . Since learning an arbitrary function is intractable and not a well posed problem, we assume that the function \\(h\\) lies in some hypothesis class \\(\\mathcal{H}\\) of allowable functions. Typically, we assume that the set of functions \\(\\mathcal{H}\\) is parameterized, meaning that there is some (parameter or weight) vector \\(\\boldsymbol{\\theta}\\) which determines a function in the class \\(\\mathcal{H}\\) . A concrete example is the class of all linear function, which is given as the set: suppose the function \\(h\\) is fully determined by the vector \\(\\boldsymbol{\\theta}\\) ; we write \\(h_{\\boldsymbol{\\theta}}\\) to make this dependence explicit. With the class of functions parameterized by a vector \\(\\boldsymbol{\\theta}\\) , the problem of learning \\(h\\) reduces to the problem of estimating the parameter \\(\\boldsymbol{\\theta}\\) .","title":"Intro"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#learn-the-parameter","text":"We choose a cost function loss: \\(\\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}\\) that measures how well (for a fixed \\(\\boldsymbol{\\theta}\\) ) the predictions \\(h_{\\boldsymbol{\\theta}}(\\mathbf{x})\\) describe the output \\(y\\) , then find the parameter that minimizes that loss: $$ \\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\sum_{i=1}^n \\operatorname{loss}\\left(h_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_i\\right), y_i\\right) $$ Here, and in the following we use the hat-notation \\(\\hat{\\boldsymbol{\\theta}}\\) to indicate that this variable is estimated based on data.","title":"Learn the parameter"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#loss-function","text":"The most popular metric in linear regression is the sum of the squares of the fitting error: ( least square cost function/MSE ) $$ \\hat{R}(\\boldsymbol{\\theta})=\\frac{1}{n} \\sum_{i=1}^n\\left(\\left\\langle\\mathbf{x}_i, \\boldsymbol{\\theta}\\right\\rangle-y_i\\right)^2 \\=\\frac{1}{n}|\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\theta}|_2^2 $$ The least squares cost function is convenient for a variety of reasons: - it can be numerically minimized efficiently as it is convex (convexity is defined below) - it has intuitive geometric and probabilistic interpretations, as we will see later. Least squares estimate: the parameter vector that minimizes this cost function: $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\hat{R}(\\boldsymbol{\\theta}) $$","title":"Loss function"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#nd","text":"Before discussing how \\(\\hat{\\theta}_{\\mathrm{LS}}\\) can be computed, let us comment on the relation of the number of examples, \\(n\\) , and the number of features, \\(d\\) . Suppose that at least \\(n\\) columns of \\(\\mathbf{X}\\) are linearly independent. Then we must have \\(n \\leq d\\) , and regardless of \\(\\mathbf{y}\\) , we can find a parameter vector \\(\\boldsymbol{\\theta}\\) (or many different ones) such that \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) , and thus describe the independent variable \\(y\\) exactly. - In the linear regression model, this is only possible if there are fewer examples than number of model parameters \\((n \\leq d)\\) . In that regime, even if \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) have nothing to do with each other, we achieve a perfect fit or zero loss. In this regime, we are likely overfitting: the model is too flexible given the data. Thus, fitting a linear model without additional assumptions only makes sense in the so called over-determined or under-parameterized regime, where the number of training examples is larger than the number of features \\((n \\geq d)\\) . If we find a good approximation in the \\((n \\geq d)\\) -regime, then this is an indication that the linear model is capturing structure of the problem. n<=d \u4e0d\u8bbax\u548cy\u7684\u5173\u7cfb\uff0c \u90fd\u80fd\u627e\u5230\u4e00\u4e2a \\(\\theta\\) , \u4f7f\u5f97 \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) overfitting not make senses n>=d: \u5982\u679c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u627e\u5230\u4e86\u4e00\u4e2a\u597d\u7684\u8fd1\u4f3c\u6a21\u578b\uff0c \u5219\u8be5\u6a21\u578b\u6bd4\u8f83\u51c6\u786e","title":"\u8ba8\u8bba\u7ebf\u6027\u56de\u5f52\u4e2dn\u548cd\u7684\u5173\u7cfb"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#least-squares-estimation","text":"We next show that \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) can be computed in closed form. Assume that \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) has full column rank. - This implies that the number or training examples, \\(n\\) , is larger than the number of model parameters, \\(d\\) . - features of \\(x\\) are linearly independent In the regime \\(n \\geq d\\) , if \\(\\mathbf{X}\\) does not have full column rank, then the features are linearly dependent and thus redundant. Proposition 1. Suppose that \\(\\mathbf{X}\\) has full column rank \\(d\\) . Then for any \\(\\mathbf{y}\\) , we have $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y} $$ There are at least two proofs of this proposition.","title":"Least squares estimation"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#analysis-of-the-least-squares-estimate-and-in-what-sense-co-linear-features-are-bad","text":"Thus far, we have not commented on the features of the regression model. Intuitively, if the features are very similar to each other, least squares estimation should be difficult. For example, suppose all the features (i.e., the entries of the \\(\\mathbf{x}_i\\) ) are linear functions of each other. Then the columns of \\(\\mathbf{X}\\) are linearly dependent and \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) is not unique. In statistics it is common to refer to features with this property as co-linear . A set of variables is said to be perfectly multi-colinear if there is one or more exact linear relationship among some of the variables \\(x_{i 1}, \\ldots, x_{i d}\\) ; for example the first two variables are co-linear if \\(x_{i 1}=-2 x_{i 2}\\) for all \\(i\\) . In practice, features are usually not co-linear but can be highly correlated, which leads to instable least squares estimates. Case1. \\(X\\) has full column rank and \\(y\\) is generated according to a linear model with additive Gaussian noise: $$ \\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}^*+\\mathbf{z} $$ Here, the entries of \\(\\mathbf{z}\\) are independent zero-mean Gaussian noise with variance \\(1 / n\\) , Then: the expected error is 0, or in statistical terms, the LSE is unbiased Case2. Features of \\(X\\) are strongly correlated where \\(\\sigma_i\\) are the singular values of \\(\\mathbf{X}\\) , and expectation is over the Gaussian noise \\(\\mathbf{z}\\) then \\(1 / \\sigma_{\\min }^2\\) is large and the bound becomes large. This is the case if some of the features are highly correlated, known as multicollinearity in the statistics literature. On the other hand, as expected, the error decays in \\(n\\) , the larger \\(n\\) , the smaller the error. proof of the error function:","title":"Analysis of the least squares estimate and in what sense co-linear features are \"bad\""}]}