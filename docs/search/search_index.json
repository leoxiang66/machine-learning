{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to My ML Notes","title":"Welcome to My ML Notes"},{"location":"#welcome-to-my-ml-notes","text":"","title":"Welcome to My ML Notes"},{"location":"Optimization/1%20Convexity/","text":"Convex Optimization Optimization Problems Many problems in science and engineering can be formulated as an optimization problem: $$ \\text { minimize } f(\\mathbf{x}) \\quad \\text { subject to } \\quad \\mathbf{x} \\in \\mathcal{C} \\text {, } $$ where \\(f\\) is a cost function and \\(\\mathcal{C}\\) is a set. Due to \\(\\max _{\\mathbf{x} \\in \\mathcal{C}} f(\\mathbf{x})=\\min _{\\mathbf{x} \\in \\mathcal{C}}-f(\\mathbf{x})\\) , this formulation includes the problem of maximizing a cost function. The function \\(f\\) can model goodness of fit in machine learning (we'll see many examples later), utility, or cost. The set \\(\\mathcal{C}\\) can incorporate constraints such as a limited budget \\((\\|\\mathbf{x}\\| \\leq B)\\) or priors ( \\(\\mathbf{x}\\) is non-negative). We say that \\(\\mathbf{x}^*\\) is a (global) solution to the optimization problem (1) if \\(\\mathbf{x}^* \\in \\mathcal{C}\\) , and \\(f\\left(\\mathbf{x}^*\\right) \\leq f(\\mathbf{x})\\) for all \\(\\mathrm{x} \\in \\mathcal{C}\\) . We say that \\(\\mathrm{x}^*\\) is a local solution to the optimization problem if in a neighborhood \\(\\mathcal{N}\\) around \\(\\mathbf{x}^*, f\\left(\\mathbf{x}^*\\right) \\leq f(\\mathbf{x})\\) for all \\(\\mathbf{x} \\in \\mathcal{C} \\cap \\mathcal{N}\\) . Optimality can be very hard to check, even if \\(f\\) is differentiable and \\(\\mathcal{C}=\\mathbb{R}^n\\) . Convexity An important class of optimization problems for which we can check optimality rather easily are convex optimization problems. A standard reference for convex optimization problems is the book [BV04]. Definition 1. A convex set \\(\\mathcal{C}\\) is any set such that for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathcal{C}\\) and all \\(\\theta \\in(0,1)\\) , $$ \\theta \\mathbf{x}+(1-\\theta) \\mathbf{y} \\in \\mathcal{C} $$ \u8fde\u63a5set\u5185\u4efb\u610f\u4e24\u4e2a\u70b9, \u8fd9\u4e24\u4e2a\u70b9\u7684\u8fde\u7ebf\u4e0a\u4efb\u610f\u4e00\u70b9\u90fd\u5728set\u5185 Figure 1 shows examples of convex and non-convex sets. Intersections of convex sets are convex. Important examples of convex sets are the following: Subspaces \\(\\left\\{\\mathbf{x}=\\mathbf{U a}: \\mathbf{a} \\in \\mathbb{R}^d\\right\\}\\) , Affines spaces \\(\\left\\{\\mathbf{x}=\\mathbf{U a}+\\mathbf{b}: \\mathbf{a} \\in \\mathbb{R}^d\\right\\}\\) Half-spaces \\(\\{\\mathbf{x}:\\langle\\mathbf{a}, \\mathbf{x}\\rangle \\leq b\\}\\) Definition 2. for a set of points \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^n\\) , a convex combination of them is defined as the set \\[ \\left\\{\\mathbf{x}=\\sum_{i=1}^n \\theta_i \\mathbf{x}_i: \\sum_{i=1}^n \\theta_i=1, \\theta_i \\geq 0\\right\\} \\] A set \\(\\mathcal{C}\\) is convex if and only if it contains all convex combinations of its elements. For any given set \\(\\mathcal{S}\\) , the convex hull of \\(\\mathcal{S}\\) is defined as the set of all convex combinations of points in \\(\\mathcal{S}\\) . Intuitively, it is the smallest convex set that contains \\(\\mathcal{S}\\) . As an example, consider the non-convex set \\(\\left\\{\\mathbf{x}:\\|\\mathbf{x}\\|_0 \\leq 1,\\|\\mathbf{x}\\|_1 \\leq 1\\right\\}\\) . Its convex hull is the \\(\\ell_1\\) -norm ball \\(\\left\\{\\mathbf{x}\\right.\\) : \\(\\left.\\|\\mathbf{x}\\|_1 \\leq 1\\right\\}\\) . We are now ready for one of the most important definitions in this class. Definition 3. A function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is convex if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) , and all \\(\\theta \\in(0,1)\\) , $$ \\theta f(\\mathbf{x})+(1-\\theta) f(\\mathbf{y}) \\geq f(\\theta \\mathbf{x}+(1-\\theta) \\mathbf{y}) . $$ A function is strictly convex, if the inequality above is strict whenever \\(\\mathbf{x} \\neq \\mathbf{y}\\) . A function \\(f\\) is concave if \\(-f\\) is convex. Common examples of convex functions are: Linear functions: \\(f(\\mathbf{x})=\\langle\\mathbf{a}, \\mathbf{x}\\rangle+b\\) , Quadratics: \\(f(\\mathbf{x})=\\frac{1}{2} \\mathbf{x}^T \\mathbf{Q} \\mathbf{x}+\\mathbf{b}^T \\mathbf{x}+c\\) , where \\(\\mathbf{Q}\\) is positive semidefinite. The function \\(f\\) is convex if and only if \\(\\mathbf{Q}\\) is positive semidefinite. Any norm \\(f(\\mathbf{x})=\\|\\mathbf{x}\\|\\) is convex. This follows from the triangle inequality and homogeneity/scalability. An important property of convex functions is that local minima are global. Proposition 1. Any local minimum of a convex function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is also a global minimum. We now turn to first order optimality conditions. To this end, we first state a proposition stating that a function is convex if and only if it is lower bounded by its first order Taylor expansion at any point. Proposition 2. A differentiable function \\(f\\) is convex if and only if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) , $$ f(\\mathbf{y}) \\geq f(\\mathbf{x})+\\langle\\mathbf{y}-\\mathbf{x}, \\nabla f(\\mathbf{x})\\rangle . $$ It is strictly convex if and only if the inequality holds strictly for all \\(\\mathbf{x} \\neq \\mathbf{y}\\) . An important consequence is the following optimality condition. Corollary 1. If a differentiable function \\(f\\) is convex and \\(\\nabla f\\left(\\mathbf{x}^*\\right)=0\\) , then \\(\\mathbf{x}^*\\) is a global minimizer of \\(f\\) . Computational aspects of optimization algorithms Suppose we want to minimize a differentiable function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) over \\(\\mathbb{R}^n\\) : $$ \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{\\operatorname{minimize}} f(\\mathbf{x}) . $$ For simplicity assume that \\(f(\\mathbf{x})=\\|\\mathbf{A} \\mathbf{x}-\\mathbf{y}\\|_2^2\\) , where \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) is a matrix with full column rank. So the optimization problem above is a standard least squares problem. If the columns are linearly independent, we can simply obtain a closed form solution as \\(\\mathbf{x}^*=\\left(\\mathbf{A}^T \\mathbf{A}\\right)^{-1} \\mathbf{A}^T \\mathbf{y}\\) . However, for many practically relevant functions \\(f\\) , we cannot compute a closed form solution. Even if we can, we might not want to because computing the closed form solution can be computationally expensive, e.g., for \\(\\mathbf{x}^*=\\left(\\mathbf{A}^T \\mathbf{A}\\right)^{-1} \\mathbf{A}^T \\mathbf{y}\\) we have to compute the inverse of a possibly large matrix. In the first part of this course we consider algorithms for finding approximate solutions of the minimization problem above. In practice we are essentially always content with an approximate solution (a computer can only give us a solution up to machine precision anyways). We assume that we have oracle access to \\(f\\) : A zeroth order oracle returns \\(f(\\mathbf{x})\\) for a given \\(\\mathbf{x}\\) . A first order oracle returns \\(\\{f(\\mathbf{x}), \\nabla f(\\mathbf{x})\\}\\) for a given \\(\\mathbf{x}\\) . A second order oracle returns \\(\\left\\{f(\\mathbf{x}), \\nabla f(\\mathbf{x}), \\nabla^2 f(\\mathbf{x})\\right\\}\\) for a given \\(\\mathbf{x}\\) . Ultimately, we are interested in understanding the computational complexity of convex optimization algorithms, i.e., the number of flops to obtain a \\(\\epsilon\\) -accurate solution (an \\(\\epsilon\\) -accurate solution can be a vector \\(\\mathbf{x}^k\\) obeying \\(\\left\\|\\mathbf{x}^k-\\mathbf{x}^*\\right\\| \\leq \\epsilon\\) , or \\(\\left.f\\left(\\mathbf{x}^k\\right)-f\\left(\\mathbf{x}^*\\right) \\leq \\epsilon\\right)\\) . To this end, we study the oracle complexity of iterative algorithms to obtain approximate solutions to convex optimization problems. The number of flops can then be obtained as the number of oracle queries required to obtain an \\(\\epsilon\\) -accurate solution times the complexity of the oracle. A word of caution: Other properties than the oracle complexity can affect the runtime of algorithms in practice. For example, standard gradient descent has a worse convergence rate than the accelerated gradient method, but might be more sensitive to noise. Takeaway Convex Set \u8fde\u63a5set\u5185\u4efb\u610f\u4e24\u4e2a\u70b9, \u8fd9\u4e24\u4e2a\u70b9\u7684\u8fde\u7ebf\u4e0a\u4efb\u610f\u4e00\u70b9\u90fd\u5728set\u5185 A set C is convex if and only if it contains all convex combinations of its elements. Convex Combination for a set of points \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^n\\) , a convex combination of them is defined as the set \\[ \\left\\{\\mathbf{x}=\\sum_{i=1}^n \\theta_i \\mathbf{x}_i: \\sum_{i=1}^n \\theta_i=1, \\theta_i \\geq 0\\right\\} \\] Convex Hull of S For any given set S, the convex hull of S is defined as the set of all convex combinations of points in S. Intuitively, it is the smallest convex set that contains S Differentiable Function A differentiable function \\(f\\) is convex if and only if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) , $$ f(\\mathbf{y}) \\geq f(\\mathbf{x})+\\langle\\mathbf{y}-\\mathbf{x}, \\nabla f(\\mathbf{x})\\rangle . $$","title":"Convex Optimization"},{"location":"Optimization/1%20Convexity/#convex-optimization","text":"","title":"Convex Optimization"},{"location":"Optimization/1%20Convexity/#optimization-problems","text":"Many problems in science and engineering can be formulated as an optimization problem: $$ \\text { minimize } f(\\mathbf{x}) \\quad \\text { subject to } \\quad \\mathbf{x} \\in \\mathcal{C} \\text {, } $$ where \\(f\\) is a cost function and \\(\\mathcal{C}\\) is a set. Due to \\(\\max _{\\mathbf{x} \\in \\mathcal{C}} f(\\mathbf{x})=\\min _{\\mathbf{x} \\in \\mathcal{C}}-f(\\mathbf{x})\\) , this formulation includes the problem of maximizing a cost function. The function \\(f\\) can model goodness of fit in machine learning (we'll see many examples later), utility, or cost. The set \\(\\mathcal{C}\\) can incorporate constraints such as a limited budget \\((\\|\\mathbf{x}\\| \\leq B)\\) or priors ( \\(\\mathbf{x}\\) is non-negative). We say that \\(\\mathbf{x}^*\\) is a (global) solution to the optimization problem (1) if \\(\\mathbf{x}^* \\in \\mathcal{C}\\) , and \\(f\\left(\\mathbf{x}^*\\right) \\leq f(\\mathbf{x})\\) for all \\(\\mathrm{x} \\in \\mathcal{C}\\) . We say that \\(\\mathrm{x}^*\\) is a local solution to the optimization problem if in a neighborhood \\(\\mathcal{N}\\) around \\(\\mathbf{x}^*, f\\left(\\mathbf{x}^*\\right) \\leq f(\\mathbf{x})\\) for all \\(\\mathbf{x} \\in \\mathcal{C} \\cap \\mathcal{N}\\) . Optimality can be very hard to check, even if \\(f\\) is differentiable and \\(\\mathcal{C}=\\mathbb{R}^n\\) .","title":"Optimization Problems"},{"location":"Optimization/1%20Convexity/#convexity","text":"An important class of optimization problems for which we can check optimality rather easily are convex optimization problems. A standard reference for convex optimization problems is the book [BV04]. Definition 1. A convex set \\(\\mathcal{C}\\) is any set such that for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathcal{C}\\) and all \\(\\theta \\in(0,1)\\) , $$ \\theta \\mathbf{x}+(1-\\theta) \\mathbf{y} \\in \\mathcal{C} $$ \u8fde\u63a5set\u5185\u4efb\u610f\u4e24\u4e2a\u70b9, \u8fd9\u4e24\u4e2a\u70b9\u7684\u8fde\u7ebf\u4e0a\u4efb\u610f\u4e00\u70b9\u90fd\u5728set\u5185 Figure 1 shows examples of convex and non-convex sets. Intersections of convex sets are convex. Important examples of convex sets are the following: Subspaces \\(\\left\\{\\mathbf{x}=\\mathbf{U a}: \\mathbf{a} \\in \\mathbb{R}^d\\right\\}\\) , Affines spaces \\(\\left\\{\\mathbf{x}=\\mathbf{U a}+\\mathbf{b}: \\mathbf{a} \\in \\mathbb{R}^d\\right\\}\\) Half-spaces \\(\\{\\mathbf{x}:\\langle\\mathbf{a}, \\mathbf{x}\\rangle \\leq b\\}\\) Definition 2. for a set of points \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^n\\) , a convex combination of them is defined as the set \\[ \\left\\{\\mathbf{x}=\\sum_{i=1}^n \\theta_i \\mathbf{x}_i: \\sum_{i=1}^n \\theta_i=1, \\theta_i \\geq 0\\right\\} \\] A set \\(\\mathcal{C}\\) is convex if and only if it contains all convex combinations of its elements. For any given set \\(\\mathcal{S}\\) , the convex hull of \\(\\mathcal{S}\\) is defined as the set of all convex combinations of points in \\(\\mathcal{S}\\) . Intuitively, it is the smallest convex set that contains \\(\\mathcal{S}\\) . As an example, consider the non-convex set \\(\\left\\{\\mathbf{x}:\\|\\mathbf{x}\\|_0 \\leq 1,\\|\\mathbf{x}\\|_1 \\leq 1\\right\\}\\) . Its convex hull is the \\(\\ell_1\\) -norm ball \\(\\left\\{\\mathbf{x}\\right.\\) : \\(\\left.\\|\\mathbf{x}\\|_1 \\leq 1\\right\\}\\) . We are now ready for one of the most important definitions in this class. Definition 3. A function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is convex if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) , and all \\(\\theta \\in(0,1)\\) , $$ \\theta f(\\mathbf{x})+(1-\\theta) f(\\mathbf{y}) \\geq f(\\theta \\mathbf{x}+(1-\\theta) \\mathbf{y}) . $$ A function is strictly convex, if the inequality above is strict whenever \\(\\mathbf{x} \\neq \\mathbf{y}\\) . A function \\(f\\) is concave if \\(-f\\) is convex. Common examples of convex functions are: Linear functions: \\(f(\\mathbf{x})=\\langle\\mathbf{a}, \\mathbf{x}\\rangle+b\\) , Quadratics: \\(f(\\mathbf{x})=\\frac{1}{2} \\mathbf{x}^T \\mathbf{Q} \\mathbf{x}+\\mathbf{b}^T \\mathbf{x}+c\\) , where \\(\\mathbf{Q}\\) is positive semidefinite. The function \\(f\\) is convex if and only if \\(\\mathbf{Q}\\) is positive semidefinite. Any norm \\(f(\\mathbf{x})=\\|\\mathbf{x}\\|\\) is convex. This follows from the triangle inequality and homogeneity/scalability. An important property of convex functions is that local minima are global. Proposition 1. Any local minimum of a convex function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is also a global minimum. We now turn to first order optimality conditions. To this end, we first state a proposition stating that a function is convex if and only if it is lower bounded by its first order Taylor expansion at any point. Proposition 2. A differentiable function \\(f\\) is convex if and only if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) , $$ f(\\mathbf{y}) \\geq f(\\mathbf{x})+\\langle\\mathbf{y}-\\mathbf{x}, \\nabla f(\\mathbf{x})\\rangle . $$ It is strictly convex if and only if the inequality holds strictly for all \\(\\mathbf{x} \\neq \\mathbf{y}\\) . An important consequence is the following optimality condition. Corollary 1. If a differentiable function \\(f\\) is convex and \\(\\nabla f\\left(\\mathbf{x}^*\\right)=0\\) , then \\(\\mathbf{x}^*\\) is a global minimizer of \\(f\\) .","title":"Convexity"},{"location":"Optimization/1%20Convexity/#computational-aspects-of-optimization-algorithms","text":"Suppose we want to minimize a differentiable function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) over \\(\\mathbb{R}^n\\) : $$ \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{\\operatorname{minimize}} f(\\mathbf{x}) . $$ For simplicity assume that \\(f(\\mathbf{x})=\\|\\mathbf{A} \\mathbf{x}-\\mathbf{y}\\|_2^2\\) , where \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) is a matrix with full column rank. So the optimization problem above is a standard least squares problem. If the columns are linearly independent, we can simply obtain a closed form solution as \\(\\mathbf{x}^*=\\left(\\mathbf{A}^T \\mathbf{A}\\right)^{-1} \\mathbf{A}^T \\mathbf{y}\\) . However, for many practically relevant functions \\(f\\) , we cannot compute a closed form solution. Even if we can, we might not want to because computing the closed form solution can be computationally expensive, e.g., for \\(\\mathbf{x}^*=\\left(\\mathbf{A}^T \\mathbf{A}\\right)^{-1} \\mathbf{A}^T \\mathbf{y}\\) we have to compute the inverse of a possibly large matrix. In the first part of this course we consider algorithms for finding approximate solutions of the minimization problem above. In practice we are essentially always content with an approximate solution (a computer can only give us a solution up to machine precision anyways). We assume that we have oracle access to \\(f\\) : A zeroth order oracle returns \\(f(\\mathbf{x})\\) for a given \\(\\mathbf{x}\\) . A first order oracle returns \\(\\{f(\\mathbf{x}), \\nabla f(\\mathbf{x})\\}\\) for a given \\(\\mathbf{x}\\) . A second order oracle returns \\(\\left\\{f(\\mathbf{x}), \\nabla f(\\mathbf{x}), \\nabla^2 f(\\mathbf{x})\\right\\}\\) for a given \\(\\mathbf{x}\\) . Ultimately, we are interested in understanding the computational complexity of convex optimization algorithms, i.e., the number of flops to obtain a \\(\\epsilon\\) -accurate solution (an \\(\\epsilon\\) -accurate solution can be a vector \\(\\mathbf{x}^k\\) obeying \\(\\left\\|\\mathbf{x}^k-\\mathbf{x}^*\\right\\| \\leq \\epsilon\\) , or \\(\\left.f\\left(\\mathbf{x}^k\\right)-f\\left(\\mathbf{x}^*\\right) \\leq \\epsilon\\right)\\) . To this end, we study the oracle complexity of iterative algorithms to obtain approximate solutions to convex optimization problems. The number of flops can then be obtained as the number of oracle queries required to obtain an \\(\\epsilon\\) -accurate solution times the complexity of the oracle. A word of caution: Other properties than the oracle complexity can affect the runtime of algorithms in practice. For example, standard gradient descent has a worse convergence rate than the accelerated gradient method, but might be more sensitive to noise.","title":"Computational aspects of optimization algorithms"},{"location":"Optimization/1%20Convexity/#takeaway","text":"Convex Set \u8fde\u63a5set\u5185\u4efb\u610f\u4e24\u4e2a\u70b9, \u8fd9\u4e24\u4e2a\u70b9\u7684\u8fde\u7ebf\u4e0a\u4efb\u610f\u4e00\u70b9\u90fd\u5728set\u5185 A set C is convex if and only if it contains all convex combinations of its elements. Convex Combination for a set of points \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^n\\) , a convex combination of them is defined as the set \\[ \\left\\{\\mathbf{x}=\\sum_{i=1}^n \\theta_i \\mathbf{x}_i: \\sum_{i=1}^n \\theta_i=1, \\theta_i \\geq 0\\right\\} \\] Convex Hull of S For any given set S, the convex hull of S is defined as the set of all convex combinations of points in S. Intuitively, it is the smallest convex set that contains S Differentiable Function A differentiable function \\(f\\) is convex if and only if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) , $$ f(\\mathbf{y}) \\geq f(\\mathbf{x})+\\langle\\mathbf{y}-\\mathbf{x}, \\nabla f(\\mathbf{x})\\rangle . $$","title":"Takeaway"},{"location":"Optimization/2%20Gradient%20Descent/","text":"Gradient Descent Gradient descent is a simple iterative algorithm for minimizing a differentiable function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) on \\(\\mathbb{R}^n\\) . Starting from an initial point \\(\\mathbf{x}_0\\) , gradient descent iterates: $$ \\mathbf{x}^{k+1}=\\mathbf{x}^k-\\alpha_k \\nabla f\\left(\\mathbf{x}^k\\right), $$ where \\(\\alpha_k\\) is a step size parameter. Gradient descent converges to a local minimum, and provided that \\(f\\) is convex, it converges to a global minimum. The idea behind this algorithm is to make small steps in the direction that minimizes the local first order approximation of \\(f\\) . Convergence for quadratic functions In order to understand the gradient method better, let us start with a simple class of functions, namely quadratic functions: $$ f(\\mathbf{x})=\\frac{1}{2} \\mathbf{x}^T \\mathbf{Q} \\mathbf{x}-\\mathbf{b}^T \\mathbf{x}, $$ where \\(\\mathbf{Q}\\) is symmetric and strictly positive definite. A closed form solution to the minimization problem minimize \\(\\mathbf{f}(\\mathbf{x})\\) is \\(\\mathbf{x}^*=\\mathbf{Q}^{-1} \\mathbf{b}\\) . We want to understand what gradient descent yields for this problem. We consider gradient descent with a fixed stepsize: $$ \\mathbf{x}^{k+1}=\\mathbf{x}^k-\\alpha \\nabla f\\left(\\mathbf{x}^k\\right) . $$ Using that the gradient is given by \\(\\nabla f(\\mathbf{x})=\\mathbf{Q} \\mathbf{x}-\\mathbf{b}\\) and that the optimal solution obeys \\(\\mathbf{Q x}^*=\\mathbf{b}\\) , the difference of the \\((k+1)\\) -st iteration to the optimum is It follows that \\[ \\left\\|\\mathbf{x}^{k+1}-\\mathbf{x}^*\\right\\|_2 \\leq\\|\\mathbf{I}-\\alpha \\mathbf{Q}\\|\\left\\|\\mathbf{x}^k-\\mathbf{x}^*\\right\\|_2 \\] Since \\(\\mathbf{I}-\\alpha \\mathbf{Q}\\) is symmetric (the first equality below can be checked by taking the singular value decomposition of the matrix) $$ |\\mathbf{I}-\\alpha \\mathbf{Q}|=\\max \\left(\\lambda_{\\max }(\\mathbf{I}-\\alpha \\mathbf{Q}),-\\lambda_{\\min }(\\mathbf{I}-\\alpha \\mathbf{Q})\\right)=\\max (\\alpha M-1,1-\\alpha m), $$ where \\(M\\) and \\(m\\) are the largest and smallest singular values of the matrix \\(\\mathbf{Q}\\) . For the second equality, we used that, the eigenvalues of \\(\\mathbf{I}-\\alpha \\mathbf{Q}\\) and \\(\\mathbf{Q}\\) are related as \\(\\lambda_i(\\mathbf{I}-\\alpha \\mathbf{Q})=1-\\alpha \\lambda_i(\\mathbf{Q})^1\\) . The right hand side above is minimized by \\(\\alpha=\\frac{2}{M+m}\\) . For this choice, \\(\\|\\mathbf{I}-\\alpha \\mathbf{Q}\\|=\\frac{1-1 / \\kappa}{1+1 / \\kappa}<1\\) , where \\(\\kappa=M / m\\) is the condition number of the matrix \\(\\mathbf{Q}\\) . To summarize, we have proven the following proposition: References [BV04] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.","title":"Gradient Descent"},{"location":"Optimization/2%20Gradient%20Descent/#gradient-descent","text":"Gradient descent is a simple iterative algorithm for minimizing a differentiable function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) on \\(\\mathbb{R}^n\\) . Starting from an initial point \\(\\mathbf{x}_0\\) , gradient descent iterates: $$ \\mathbf{x}^{k+1}=\\mathbf{x}^k-\\alpha_k \\nabla f\\left(\\mathbf{x}^k\\right), $$ where \\(\\alpha_k\\) is a step size parameter. Gradient descent converges to a local minimum, and provided that \\(f\\) is convex, it converges to a global minimum. The idea behind this algorithm is to make small steps in the direction that minimizes the local first order approximation of \\(f\\) .","title":"Gradient Descent"},{"location":"Optimization/2%20Gradient%20Descent/#convergence-for-quadratic-functions","text":"In order to understand the gradient method better, let us start with a simple class of functions, namely quadratic functions: $$ f(\\mathbf{x})=\\frac{1}{2} \\mathbf{x}^T \\mathbf{Q} \\mathbf{x}-\\mathbf{b}^T \\mathbf{x}, $$ where \\(\\mathbf{Q}\\) is symmetric and strictly positive definite. A closed form solution to the minimization problem minimize \\(\\mathbf{f}(\\mathbf{x})\\) is \\(\\mathbf{x}^*=\\mathbf{Q}^{-1} \\mathbf{b}\\) . We want to understand what gradient descent yields for this problem. We consider gradient descent with a fixed stepsize: $$ \\mathbf{x}^{k+1}=\\mathbf{x}^k-\\alpha \\nabla f\\left(\\mathbf{x}^k\\right) . $$ Using that the gradient is given by \\(\\nabla f(\\mathbf{x})=\\mathbf{Q} \\mathbf{x}-\\mathbf{b}\\) and that the optimal solution obeys \\(\\mathbf{Q x}^*=\\mathbf{b}\\) , the difference of the \\((k+1)\\) -st iteration to the optimum is It follows that \\[ \\left\\|\\mathbf{x}^{k+1}-\\mathbf{x}^*\\right\\|_2 \\leq\\|\\mathbf{I}-\\alpha \\mathbf{Q}\\|\\left\\|\\mathbf{x}^k-\\mathbf{x}^*\\right\\|_2 \\] Since \\(\\mathbf{I}-\\alpha \\mathbf{Q}\\) is symmetric (the first equality below can be checked by taking the singular value decomposition of the matrix) $$ |\\mathbf{I}-\\alpha \\mathbf{Q}|=\\max \\left(\\lambda_{\\max }(\\mathbf{I}-\\alpha \\mathbf{Q}),-\\lambda_{\\min }(\\mathbf{I}-\\alpha \\mathbf{Q})\\right)=\\max (\\alpha M-1,1-\\alpha m), $$ where \\(M\\) and \\(m\\) are the largest and smallest singular values of the matrix \\(\\mathbf{Q}\\) . For the second equality, we used that, the eigenvalues of \\(\\mathbf{I}-\\alpha \\mathbf{Q}\\) and \\(\\mathbf{Q}\\) are related as \\(\\lambda_i(\\mathbf{I}-\\alpha \\mathbf{Q})=1-\\alpha \\lambda_i(\\mathbf{Q})^1\\) . The right hand side above is minimized by \\(\\alpha=\\frac{2}{M+m}\\) . For this choice, \\(\\|\\mathbf{I}-\\alpha \\mathbf{Q}\\|=\\frac{1-1 / \\kappa}{1+1 / \\kappa}<1\\) , where \\(\\kappa=M / m\\) is the condition number of the matrix \\(\\mathbf{Q}\\) . To summarize, we have proven the following proposition:","title":"Convergence for quadratic functions"},{"location":"Optimization/2%20Gradient%20Descent/#references","text":"[BV04] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.","title":"References"},{"location":"Supervised%20Learning/0%20Intro/0.1%20Bias%20Variance%20Trade-off/","text":"Bias-Variance tradeoff Our primary goal is finding a model that has high prediction performance on previously unseen examples. In this section, we discuss the bias-variance tradeoff for predicting \\(y\\) based on \\(\\mathbf{x}\\) and our learned model. To start, we ask the question: How can we measure the effectiveness of a learning algorithm? To address this question in the context of learning, let us again consider the regression model $$ y=h(\\mathbf{x})+z, $$ but now we assume that both the feature vector \\(\\mathbf{x}\\) and the noise \\(z\\) are drawn from some distribution. Then \\(y\\) is a random variable and its distribution is fully determined by \\(h\\) and the distributions of \\(z\\) and \\(\\mathbf{x}\\) . Stated differently, \\(y\\) and \\(\\mathbf{x}\\) are drawn from a joint distribution that is determined by the feature and noise distributions as well as \\(h\\) . In this context, the learning problem is as follows: We obtain a dataset \\(\\mathcal{D}=\\left\\{\\left(y_1, \\mathbf{x}_1\\right), \\ldots,\\left(y_n, \\mathbf{x}_n\\right)\\right\\}\\) by drawing \\(n\\) examples from the joint distribution. The learning algorithm takes the dataset and returns an estimate of the regression function \\(\\hat{h}\\) . As a concrete example of learning a regression function, we can compute the ridge regression estimate \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) which then yields the following estimate of the regression function: $$ \\hat{h}(\\mathbf{x})=\\left\\langle\\mathbf{x}, \\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right\\rangle . $$ Note that the \\(\\hat{h}\\) is a function of the data, \\(\\mathcal{D}\\) . It is natural to measure performance of the learned estimate \\(\\hat{h}\\) by measuring its performance on an new example, drawn from the joint distribution, and then averaging over many of such examples. Thus, we consider the expectation: $$ R(\\hat{h})=\\mathbb{E}_{\\mathbf{x}, y}\\left[(\\hat{h}(\\mathbf{x})-y)^2\\right] . $$ Here, expectation is with respect to the newly drawn sample. Recall that the estimate \\(\\hat{h}\\) is a function of the dataset, to make this dependency explicit, we write \\(\\hat{h}=\\hat{h}_{\\mathcal{D}}\\) in what follows. Since we are interested in the performance of a learning algorithm that selects \\(\\hat{h}\\) based on the randomly chosen dataset \\(\\mathcal{D}\\) , we consider the expectation over the dataset: This error corresponds to the performance of the learner's prediction averaged over examples, and averaged over many data sets \\(\\mathcal{D}\\) . With the relation \\(y=h(\\mathbf{x})+z\\) , we can decompose the error as follows: where the last equality follows from \\(z\\) being independent of \\(\\mathbf{x}\\) and \\(\\mathcal{D}\\) and having zero mean. We next decompose the first term further. Towards this end, we condition on \\(\\mathbf{x}\\) (think about \\(\\mathbf{x}\\) as a fixed vector, not a random variable for the calculations below). Then the first term above becomes (all expectations below are with respect to the random dataset \\(\\mathcal{D}\\) ): where the middle cross term is equal to zero. Thus we have \\[ \\mathbb{E}_{\\mathcal{D}}[R(\\hat{h})]=\\mathbb{E}_{\\mathbf{x}}\\left[\\left(h(\\mathbf{x})-\\mathbb{E}_{\\mathcal{D}}\\left[\\hat{h}_{\\mathcal{D}}(\\mathbf{x})\\right]\\right)^2\\right]+\\mathbb{E}_{\\mathcal{D}, \\mathbf{x}}\\left[\\left(\\hat{h}_{\\mathcal{D}}(\\mathbf{x})-\\mathbb{E}\\left[\\hat{h}_{\\mathcal{D}}(\\mathbf{x})\\right]\\right)^2\\right]+\\operatorname{Var}[z] . \\] This is called a bias-variance decomposition . The first term is the bias of the method. It measures how well the average hypothesis can estimate the true underlying function \\(h\\) . A low bias means that \\(\\hat{h}\\) accurately estimates the underlying hypothesis \\(h\\) . The second term is the variance of the method. The variance of the method measures the variance of the hypothesis over the training sets. The third term is the variance of \\(z\\) , which is the irreducible error. The irreducible error term is the error that we cannot control or eliminate.","title":"Bias-Variance tradeoff"},{"location":"Supervised%20Learning/0%20Intro/0.1%20Bias%20Variance%20Trade-off/#bias-variance-tradeoff","text":"Our primary goal is finding a model that has high prediction performance on previously unseen examples. In this section, we discuss the bias-variance tradeoff for predicting \\(y\\) based on \\(\\mathbf{x}\\) and our learned model. To start, we ask the question: How can we measure the effectiveness of a learning algorithm? To address this question in the context of learning, let us again consider the regression model $$ y=h(\\mathbf{x})+z, $$ but now we assume that both the feature vector \\(\\mathbf{x}\\) and the noise \\(z\\) are drawn from some distribution. Then \\(y\\) is a random variable and its distribution is fully determined by \\(h\\) and the distributions of \\(z\\) and \\(\\mathbf{x}\\) . Stated differently, \\(y\\) and \\(\\mathbf{x}\\) are drawn from a joint distribution that is determined by the feature and noise distributions as well as \\(h\\) . In this context, the learning problem is as follows: We obtain a dataset \\(\\mathcal{D}=\\left\\{\\left(y_1, \\mathbf{x}_1\\right), \\ldots,\\left(y_n, \\mathbf{x}_n\\right)\\right\\}\\) by drawing \\(n\\) examples from the joint distribution. The learning algorithm takes the dataset and returns an estimate of the regression function \\(\\hat{h}\\) . As a concrete example of learning a regression function, we can compute the ridge regression estimate \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) which then yields the following estimate of the regression function: $$ \\hat{h}(\\mathbf{x})=\\left\\langle\\mathbf{x}, \\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right\\rangle . $$ Note that the \\(\\hat{h}\\) is a function of the data, \\(\\mathcal{D}\\) . It is natural to measure performance of the learned estimate \\(\\hat{h}\\) by measuring its performance on an new example, drawn from the joint distribution, and then averaging over many of such examples. Thus, we consider the expectation: $$ R(\\hat{h})=\\mathbb{E}_{\\mathbf{x}, y}\\left[(\\hat{h}(\\mathbf{x})-y)^2\\right] . $$ Here, expectation is with respect to the newly drawn sample. Recall that the estimate \\(\\hat{h}\\) is a function of the dataset, to make this dependency explicit, we write \\(\\hat{h}=\\hat{h}_{\\mathcal{D}}\\) in what follows. Since we are interested in the performance of a learning algorithm that selects \\(\\hat{h}\\) based on the randomly chosen dataset \\(\\mathcal{D}\\) , we consider the expectation over the dataset: This error corresponds to the performance of the learner's prediction averaged over examples, and averaged over many data sets \\(\\mathcal{D}\\) . With the relation \\(y=h(\\mathbf{x})+z\\) , we can decompose the error as follows: where the last equality follows from \\(z\\) being independent of \\(\\mathbf{x}\\) and \\(\\mathcal{D}\\) and having zero mean. We next decompose the first term further. Towards this end, we condition on \\(\\mathbf{x}\\) (think about \\(\\mathbf{x}\\) as a fixed vector, not a random variable for the calculations below). Then the first term above becomes (all expectations below are with respect to the random dataset \\(\\mathcal{D}\\) ): where the middle cross term is equal to zero. Thus we have \\[ \\mathbb{E}_{\\mathcal{D}}[R(\\hat{h})]=\\mathbb{E}_{\\mathbf{x}}\\left[\\left(h(\\mathbf{x})-\\mathbb{E}_{\\mathcal{D}}\\left[\\hat{h}_{\\mathcal{D}}(\\mathbf{x})\\right]\\right)^2\\right]+\\mathbb{E}_{\\mathcal{D}, \\mathbf{x}}\\left[\\left(\\hat{h}_{\\mathcal{D}}(\\mathbf{x})-\\mathbb{E}\\left[\\hat{h}_{\\mathcal{D}}(\\mathbf{x})\\right]\\right)^2\\right]+\\operatorname{Var}[z] . \\] This is called a bias-variance decomposition . The first term is the bias of the method. It measures how well the average hypothesis can estimate the true underlying function \\(h\\) . A low bias means that \\(\\hat{h}\\) accurately estimates the underlying hypothesis \\(h\\) . The second term is the variance of the method. The variance of the method measures the variance of the hypothesis over the training sets. The third term is the variance of \\(z\\) , which is the irreducible error. The irreducible error term is the error that we cannot control or eliminate.","title":"Bias-Variance tradeoff"},{"location":"Supervised%20Learning/0%20Intro/0.2%20Model%20Selection%20and%20Validation/","text":"Model selection and validation In a previous lecture, we discussed the ridge regression estimate \\[ \\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}=\\arg \\min _{\\boldsymbol{\\theta}}\\|\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\theta}\\|_2^2+\\lambda\\|\\boldsymbol{\\theta}\\|_2^2, \\] and found that the regularization parameter \\(\\lambda\\) enables trading off the bias and variance of the estimator. Note that the estimate \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) is parameterized by \\(\\lambda\\) , and when computing \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) , we do not optimize over \\(\\lambda\\) . For this reason, it is called a hyperparamete r. Almost all machine learning method have hyperparameters. Besides obvious hyperparameters like the regularization parameter in ridge regression, the following can also be regarded as hyperparameters that we wish to choose well: Which model to choose, for example a linear model. For a fixed model, their parameters. For example, the number of layers of a deep neural network, its activations functions etc. Which features to include or exclude. The performance of our method often critically depends on the choice of the hyperparameters. This raises the question: How can we choose the hyperparameters in a principled manner, and assess the performance of the resulting model? The statistical learning setup In order to fit a model, we typically minimize a quantity that captures how well the model predicts past data, but we actually care about how well the model predicts unseen data. In order to asses how well a model predicts unseen data it is useful to consider the statistical learning setup. Suppose that examples \\((\\mathbf{x}, y)\\) are drawn i.i.d. from some (unknown) distribution. For example, this distribution could be determined by \\(y=h^*(\\mathbf{x})+z\\) , where \\(h^*\\) is a fixed regression function (say \\(h^*(\\mathbf{x})=\\left\\langle\\mathbf{x}, \\boldsymbol{\\theta}^*\\right\\rangle\\) , and \\(\\mathbf{x}\\) and \\(z\\) are drawn i.i.d. from Gaussian distributions. Given a function \\(h\\) , we measure the error between the observation \\(y\\) and the estimate \\(\\hat{y}=h(\\mathbf{x})\\) with a loss function \\[ \\text { loss: } \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R} \\] For example, \\(\\operatorname{loss}(z, y)=(z-y)^2\\) is the quadratic loss popular in regression. Consider a class of functions \\(\\mathcal{H}\\) . The (true or population) risk of a function \\(h \\in \\mathcal{H}\\) in this class is defined as \\[ R(h)=\\mathbb{E}[\\operatorname{loss}(h(\\mathbf{x}), y)], \\] where the expectation is over the distribution of the data. Ideally, we would like to find the function in our hypothesis class that minimizes the risk: \\[ h^*=\\arg \\min _{h \\in \\mathcal{H}} R(h) . \\] Unfortunately, it is impossible to compute the expectation because we don't know the joint distribution of \\((\\mathbf{x}, y)\\) . Training, validation and test errors We do, however, have access to a set of examples \\(\\mathcal{D}=\\left\\{\\left(\\mathbf{x}_1, y_1\\right), \\ldots,\\left(\\mathbf{x}_n, y_n\\right)\\right\\}\\) . Training and testing: First, suppose our goal is to train a model that does not have hyperparameters and then predict how well it performs on new examples. Towards this goal, we first split our dataset into two non-overlapping sets \\(\\mathcal{D}_{\\text {train }}\\) and \\(\\mathcal{D}_{\\text {test }}\\) called the training set and the test set. The estimate is called the training error. Here, \\(\\left|\\mathcal{D}_{\\text {train }}\\right|\\) stands for the cardinality of the set \\(\\left|\\mathcal{D}_{\\text {train }}\\right|\\) , i.e., the number of examples in the training set. Almost exclusively, we fit a model by minimizing the training error, or a regularized version thereof. For example, linear regression fits a model by minimizing the training error and ridge regression fits a model by minimizing the training error regularized with the penalty \\(\\lambda\\|\\boldsymbol{\\theta}\\|_2^2\\) . Note that the training error \\(\\hat{R}\\left(h, \\mathcal{D}_{\\text {train }}\\right)\\) can be viewed as an estimate of the true risk computed based on the training set. Given a model trained on the training set, we cannot use the training error to estimate the population risk \\(R(\\hat{h})\\) of a function \\(\\hat{h}\\) trained on the training set. The reason is that \\(\\hat{h}\\) is a function of \\(\\mathcal{D}_{\\text {train }}\\) . We can, however, use the test error to estimate the expected risk: \\[ \\hat{R}\\left(h, \\mathcal{D}_{\\text {test }}\\right)=\\frac{1}{\\left|\\mathcal{D}_{\\text {test }}\\right|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{D}_{\\text {test }}} \\operatorname{loss}(h(\\mathbf{x}), y) \\] The test error \\(\\hat{R}\\left(\\hat{h}, \\mathcal{D}_{\\text {test }}\\right)\\) is an unbiased estimate of the population risk, because \\(\\hat{h}\\) is independent of the test set. The question that remains is: How large does the test set need to be? It needs to be sufficiently large so that the test error is a good estimate of the population risk. More formally, under mild assumptions on the risk and estimator (specifically, that it is bounded), we have by Hoeffding's inequality for a set \\(\\mathcal{D}\\) with \\(n\\) i.i.d. examples for any \\(\\delta>0\\) that \\[ \\mathrm{P}\\left[|\\hat{R}(h, \\mathcal{D})-R(h)| \\leq O\\left(\\sqrt{\\frac{\\log (1 / \\delta)}{n}}\\right)\\right] \\geq 1-\\delta . \\] In words, the larger our test set, the better our estimate of the true risk. Moreover, doubling the size of the test set improves the estimate by a factor of \\(\\sqrt{1 / 2}\\) . The parameter \\(\\delta\\) is the probability that the estimate is far from its mean, think about this as a very small number, say \\(10^{-8}\\) . Training, model selection, and testing: Next, suppose we want to train a method with a number of hyperparameter configurations \\(i=1, \\ldots, K\\) . For example, we wish to train a ridge regression estimate for \\(K\\) different choices of the hyperparameter \\(\\lambda\\) , determine which one performs the best, and then determine the performance we expect (i.e., estimate the risk). Towards this goal, we split the set of examples into three disjoint sets: a training set \\(\\mathcal{D}_{\\text {train }}\\) , a validation set \\(\\mathcal{D}_{\\text {val }}\\) , and a test set \\(\\mathcal{D}_{\\text {test }}\\) . We then perform the following procedure: For all choices of hyperparameter configurations \\(i=1, \\ldots, K\\) : Train model \\(\\hat{\\boldsymbol{\\theta}}_i\\) on \\(\\mathcal{D}_{\\text {train }}\\) Compute validation error for model \\(\\hat{\\boldsymbol{\\theta}}_i\\) on \\(\\mathcal{D}_{\\text {val }}\\) Select model \\(\\hat{\\boldsymbol{\\theta}}_i\\) with the smallest validation error \\(\\hat{R}\\left(\\hat{\\boldsymbol{\\theta}}_i, \\mathcal{D}_{\\text {val }}\\right)\\) as the estimate for the best performing model Report the final performance as \\(\\hat{R}\\left(\\mathcal{D}_{\\text {test }}, \\hat{\\boldsymbol{\\theta}}_i\\right)\\) With this procedure, we do have to be mindful of the fact that we are re-using the validation set many times. This has to be taken into account when choosing the size of the validation set. This procedure can go wrong: If the validation set is too small, then just by chance a hyperparameter configuration works well. If the number of hyperparameter configurations is extremely large, specifically exponential in the size of the validation set, this will happen and would be an issue. k-fold cross validation Cross validation is a method for estimating the risk of a model trained on a training set based on a finite and potentially small pool of examples \\(\\mathcal{D}\\) . The idea is to repeatedly split the set \\(\\mathcal{D}\\) into test and training sets. The most popular variant of this idea is \\(k\\) -fold cross validation, which consists of the following steps: Shuffle the data and partition it into \\(k\\) equally sized or near equally sized subsets \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_k\\) . If the data is drawn i.i.d., shuffling would not be necessary, but remember that assuming the data is drawn i.i.d. is just a modeling assumption not necessarily true in practice. For each subset \\(i\\) , train the model on the union of all subsets but \\(\\mathcal{D}_i\\) , i.e., on \\(\\mathcal{D} \\backslash \\mathcal{D}_i\\) , and evaluate the model using block \\(i\\) by computing the validation error. This gives the \\(i\\) -th estimate of the risk. \u6bcf\u6b21\u9009\u62e9 \\(D_{i}\\) \u4f5c\u4e3a\u9a8c\u8bc1\u96c6 Obtain a final estimate of the risk as the average of the so-obtained \\(k\\) estimates. Using this methodology, the model is always evaluated on a different set of points than it is trained out. So how do we choose \\(k\\) ? Larger values of \\(k\\) provide a better estimate of the true error that we expect when training on a training set with \\(n\\) points, but also requires more computation. To see this suppose we set \\(k=n\\) , then we train on \\(n-1\\) points, but also need to fit our model \\(n\\) many times, which can be very costly. How to choose \\(k\\) in practice depends on how one want to balance this tradeoff; a common value for \\(k\\) in practice is 10 . Let's look at an example following Hastie et al. [HTF09], section 7.10.2. Consider a classification or regression problem with a large number of features. Suppose we perform the following steps: Select a number of relevant features by correlating them with the observations, and select the features that are highly correlated. Use only this subset of features, and train a classification or regression model, for example ridge regression. Use cross-validation to estimate the unknown hyperparameters (e.g., the regularization parameter of ridge regression) and to estimate the prediction error of the final model. Is this a correct application of cross-validation, and if not, what can go wrong?\uff1f Suppose there are \\(n=50\\) examples and originally there are 5000 features that are independent of the labels. Assume both class labels are equally likely, then the true error rate of any classifier is at least \\(50 \\%\\) . Hastie et al. [HTF09] carried out the above steps and the average cross-validation error was 3\\%, a significant underestimation of the error rate. What happened here is that just by chance, features are highly correlated with the data. The right way to apply cross validation in the example above is the following: Divide the examples into \\(k\\) equally sized sets at random. For each set \\(i\\) , using all examples but those in set \\(i\\) , i) find a set of good predictor that are strongly correlated with the labels, ii) use only this set of predictors to build a model, and iii) use the classifier to predict the class labels in fold \\(i\\) . Accumulate the error estimates from each fold to produce the cross-validation estimate of the prediction error. Bootstrap Suppose we are interested in associating a confidence interval, standard error, or variance estimate with our learning algorithms. As a concrete example, suppose we are interested in estimating the variance of a learning algorithm \\(\\hat{h}\\) for a given example \\(\\mathbf{x}\\) : \\[ \\text{Var}[\\hat{h}(\\mathbf{x})]=\\mathbb{E}_{\\mathcal{D}}\\left[\\left(\\hat{h}_{\\mathcal{D}}-\\mathbb{E}\\left[\\hat{h}_{\\mathcal{D}}(\\mathbf{x})\\right]\\right)^2\\right] . \\] Suppose, as before that the examples \\((y, \\mathbf{x})\\) in \\(\\mathcal{D}\\) are drawn from some joint distribution. In order to estimate this quantity, we can sample datasets \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_b\\) , each containing \\(n\\) examples drawn uniformly at random from the joint distribution, and apply the learning algorithm to those datasets. That yields hypothesis \\(\\hat{h}_1, \\ldots, \\hat{h}_b\\) . Based on those, we can estimate the mean at a point \\(\\mathbf{x}\\) as and the variance as $$ \\text{Var}[\\hat{h}(\\mathbf{x})]=\\frac{1}{b} \\sum_{i=1}^b\\left(h_{\\mathcal{D}_i}(\\mathbf{x})-\\hat{\\mu}(\\mathbf{x})\\right)^2 . $$ While statistically sound, this method for estimating the variance of the estimator is not practically, we would need to sample a large number of examples. The bootstrap, proposed by Efron in 1979 offers a more practical approach. The basic idea of bootstrapping is that in absence of any other information about the unknown distribution, all the information about the distribution we have is contained in the observed sample \\(\\mathcal{D}\\) . Hence resampling from \\(\\mathcal{D}\\) is a good guide to see what can be expected if we would resample from the original distribution. For the concrete example above, we would sample a dataset \\(\\mathcal{D}\\) from the original, joint distribution, and then resample \\(b\\) many datasets \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_b\\) , all containing \\(n\\) samples, by drawing \\(n\\) examples each uniformly at random with replacement from \\(\\mathcal{D}\\) . Then, we would estimate the variance as above. Efron \u5728 1979 \u5e74\u63d0\u51fa\u7684 bootstrap \u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u65b9\u6cd5\u3002 \u81ea\u4e3e\u7684\u57fa\u672c\u601d\u60f3\u662f\uff0c\u5728\u6ca1\u6709\u5173\u4e8e\u672a\u77e5\u5206\u5e03\u7684\u4efb\u4f55\u5176\u4ed6\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u6240\u62e5\u6709\u7684\u5173\u4e8e\u5206\u5e03\u7684\u6240\u6709\u4fe1\u606f\u90fd\u5305\u542b\u5728\u89c2\u5bdf\u5230\u7684\u6837\u672c \\(\\mathcal{D}\\) \u4e2d\u3002 \u56e0\u6b64\uff0c\u4ece \\(\\mathcal{D}\\) \u91cd\u65b0\u91c7\u6837\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u6307\u5357\uff0c\u53ef\u4ee5\u5f88\u597d\u5730\u4e86\u89e3\u5982\u679c\u6211\u4eec\u4ece\u539f\u59cb\u5206\u5e03\u91cd\u65b0\u91c7\u6837\u4f1a\u53d1\u751f\u4ec0\u4e48\u3002 \u5bf9\u4e8e\u4e0a\u9762\u7684\u5177\u4f53\u793a\u4f8b\uff0c\u6211\u4eec\u5c06\u4ece\u539f\u59cb\u8054\u5408\u5206\u5e03\u4e2d\u91c7\u6837\u6570\u636e\u96c6 \\(\\mathcal{D}\\) \uff0c\u7136\u540e\u901a\u8fc7\u4ece \\(\\mathcal{D}\\) \u4e2d\u968f\u673a\u5747\u5300\u62bd\u53d6 \\(n\\) \u4e2a\u6837\u672c(\u82e5\u91c7\u96c6\u5230\u76f8\u540c\u7684\u6837\u672c\u5219\u66ff\u6362)\uff08\u91cd\u590d \\(b\\) \u6b21\uff09 \u91cd\u65b0\u91c7\u6837 \\(b\\) \u591a\u4e2a\u6570\u636e\u96c6 \\(\\mathcal{D}_1,\\ldots,\\mathcal{D}_b\\) \uff0c\u6bcf\u4e2a\u5305\u542b \\(n\\) \u4e2a\u6837\u672c\uff0c\u3002 \u7136\u540e\uff0c\u6211\u4eec\u5c06\u5982\u4e0a\u6240\u8ff0\u4f30\u8ba1\u65b9\u5dee\u3002 Reading: Chapter 7 in [HTF09]. References [HTF09] T. J. Hastie, R. J. Tibshirani, and J. J. H. Friedman. The elements of statistical learning. Springer, 2009.","title":"Model selection and validation"},{"location":"Supervised%20Learning/0%20Intro/0.2%20Model%20Selection%20and%20Validation/#model-selection-and-validation","text":"In a previous lecture, we discussed the ridge regression estimate \\[ \\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}=\\arg \\min _{\\boldsymbol{\\theta}}\\|\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\theta}\\|_2^2+\\lambda\\|\\boldsymbol{\\theta}\\|_2^2, \\] and found that the regularization parameter \\(\\lambda\\) enables trading off the bias and variance of the estimator. Note that the estimate \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) is parameterized by \\(\\lambda\\) , and when computing \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) , we do not optimize over \\(\\lambda\\) . For this reason, it is called a hyperparamete r. Almost all machine learning method have hyperparameters. Besides obvious hyperparameters like the regularization parameter in ridge regression, the following can also be regarded as hyperparameters that we wish to choose well: Which model to choose, for example a linear model. For a fixed model, their parameters. For example, the number of layers of a deep neural network, its activations functions etc. Which features to include or exclude. The performance of our method often critically depends on the choice of the hyperparameters. This raises the question: How can we choose the hyperparameters in a principled manner, and assess the performance of the resulting model?","title":"Model selection and validation"},{"location":"Supervised%20Learning/0%20Intro/0.2%20Model%20Selection%20and%20Validation/#the-statistical-learning-setup","text":"In order to fit a model, we typically minimize a quantity that captures how well the model predicts past data, but we actually care about how well the model predicts unseen data. In order to asses how well a model predicts unseen data it is useful to consider the statistical learning setup. Suppose that examples \\((\\mathbf{x}, y)\\) are drawn i.i.d. from some (unknown) distribution. For example, this distribution could be determined by \\(y=h^*(\\mathbf{x})+z\\) , where \\(h^*\\) is a fixed regression function (say \\(h^*(\\mathbf{x})=\\left\\langle\\mathbf{x}, \\boldsymbol{\\theta}^*\\right\\rangle\\) , and \\(\\mathbf{x}\\) and \\(z\\) are drawn i.i.d. from Gaussian distributions. Given a function \\(h\\) , we measure the error between the observation \\(y\\) and the estimate \\(\\hat{y}=h(\\mathbf{x})\\) with a loss function \\[ \\text { loss: } \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R} \\] For example, \\(\\operatorname{loss}(z, y)=(z-y)^2\\) is the quadratic loss popular in regression. Consider a class of functions \\(\\mathcal{H}\\) . The (true or population) risk of a function \\(h \\in \\mathcal{H}\\) in this class is defined as \\[ R(h)=\\mathbb{E}[\\operatorname{loss}(h(\\mathbf{x}), y)], \\] where the expectation is over the distribution of the data. Ideally, we would like to find the function in our hypothesis class that minimizes the risk: \\[ h^*=\\arg \\min _{h \\in \\mathcal{H}} R(h) . \\] Unfortunately, it is impossible to compute the expectation because we don't know the joint distribution of \\((\\mathbf{x}, y)\\) .","title":"The statistical learning setup"},{"location":"Supervised%20Learning/0%20Intro/0.2%20Model%20Selection%20and%20Validation/#training-validation-and-test-errors","text":"We do, however, have access to a set of examples \\(\\mathcal{D}=\\left\\{\\left(\\mathbf{x}_1, y_1\\right), \\ldots,\\left(\\mathbf{x}_n, y_n\\right)\\right\\}\\) . Training and testing: First, suppose our goal is to train a model that does not have hyperparameters and then predict how well it performs on new examples. Towards this goal, we first split our dataset into two non-overlapping sets \\(\\mathcal{D}_{\\text {train }}\\) and \\(\\mathcal{D}_{\\text {test }}\\) called the training set and the test set. The estimate is called the training error. Here, \\(\\left|\\mathcal{D}_{\\text {train }}\\right|\\) stands for the cardinality of the set \\(\\left|\\mathcal{D}_{\\text {train }}\\right|\\) , i.e., the number of examples in the training set. Almost exclusively, we fit a model by minimizing the training error, or a regularized version thereof. For example, linear regression fits a model by minimizing the training error and ridge regression fits a model by minimizing the training error regularized with the penalty \\(\\lambda\\|\\boldsymbol{\\theta}\\|_2^2\\) . Note that the training error \\(\\hat{R}\\left(h, \\mathcal{D}_{\\text {train }}\\right)\\) can be viewed as an estimate of the true risk computed based on the training set. Given a model trained on the training set, we cannot use the training error to estimate the population risk \\(R(\\hat{h})\\) of a function \\(\\hat{h}\\) trained on the training set. The reason is that \\(\\hat{h}\\) is a function of \\(\\mathcal{D}_{\\text {train }}\\) . We can, however, use the test error to estimate the expected risk: \\[ \\hat{R}\\left(h, \\mathcal{D}_{\\text {test }}\\right)=\\frac{1}{\\left|\\mathcal{D}_{\\text {test }}\\right|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{D}_{\\text {test }}} \\operatorname{loss}(h(\\mathbf{x}), y) \\] The test error \\(\\hat{R}\\left(\\hat{h}, \\mathcal{D}_{\\text {test }}\\right)\\) is an unbiased estimate of the population risk, because \\(\\hat{h}\\) is independent of the test set. The question that remains is: How large does the test set need to be? It needs to be sufficiently large so that the test error is a good estimate of the population risk. More formally, under mild assumptions on the risk and estimator (specifically, that it is bounded), we have by Hoeffding's inequality for a set \\(\\mathcal{D}\\) with \\(n\\) i.i.d. examples for any \\(\\delta>0\\) that \\[ \\mathrm{P}\\left[|\\hat{R}(h, \\mathcal{D})-R(h)| \\leq O\\left(\\sqrt{\\frac{\\log (1 / \\delta)}{n}}\\right)\\right] \\geq 1-\\delta . \\] In words, the larger our test set, the better our estimate of the true risk. Moreover, doubling the size of the test set improves the estimate by a factor of \\(\\sqrt{1 / 2}\\) . The parameter \\(\\delta\\) is the probability that the estimate is far from its mean, think about this as a very small number, say \\(10^{-8}\\) . Training, model selection, and testing: Next, suppose we want to train a method with a number of hyperparameter configurations \\(i=1, \\ldots, K\\) . For example, we wish to train a ridge regression estimate for \\(K\\) different choices of the hyperparameter \\(\\lambda\\) , determine which one performs the best, and then determine the performance we expect (i.e., estimate the risk). Towards this goal, we split the set of examples into three disjoint sets: a training set \\(\\mathcal{D}_{\\text {train }}\\) , a validation set \\(\\mathcal{D}_{\\text {val }}\\) , and a test set \\(\\mathcal{D}_{\\text {test }}\\) . We then perform the following procedure: For all choices of hyperparameter configurations \\(i=1, \\ldots, K\\) : Train model \\(\\hat{\\boldsymbol{\\theta}}_i\\) on \\(\\mathcal{D}_{\\text {train }}\\) Compute validation error for model \\(\\hat{\\boldsymbol{\\theta}}_i\\) on \\(\\mathcal{D}_{\\text {val }}\\) Select model \\(\\hat{\\boldsymbol{\\theta}}_i\\) with the smallest validation error \\(\\hat{R}\\left(\\hat{\\boldsymbol{\\theta}}_i, \\mathcal{D}_{\\text {val }}\\right)\\) as the estimate for the best performing model Report the final performance as \\(\\hat{R}\\left(\\mathcal{D}_{\\text {test }}, \\hat{\\boldsymbol{\\theta}}_i\\right)\\) With this procedure, we do have to be mindful of the fact that we are re-using the validation set many times. This has to be taken into account when choosing the size of the validation set. This procedure can go wrong: If the validation set is too small, then just by chance a hyperparameter configuration works well. If the number of hyperparameter configurations is extremely large, specifically exponential in the size of the validation set, this will happen and would be an issue.","title":"Training, validation and test errors"},{"location":"Supervised%20Learning/0%20Intro/0.2%20Model%20Selection%20and%20Validation/#k-fold-cross-validation","text":"Cross validation is a method for estimating the risk of a model trained on a training set based on a finite and potentially small pool of examples \\(\\mathcal{D}\\) . The idea is to repeatedly split the set \\(\\mathcal{D}\\) into test and training sets. The most popular variant of this idea is \\(k\\) -fold cross validation, which consists of the following steps: Shuffle the data and partition it into \\(k\\) equally sized or near equally sized subsets \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_k\\) . If the data is drawn i.i.d., shuffling would not be necessary, but remember that assuming the data is drawn i.i.d. is just a modeling assumption not necessarily true in practice. For each subset \\(i\\) , train the model on the union of all subsets but \\(\\mathcal{D}_i\\) , i.e., on \\(\\mathcal{D} \\backslash \\mathcal{D}_i\\) , and evaluate the model using block \\(i\\) by computing the validation error. This gives the \\(i\\) -th estimate of the risk. \u6bcf\u6b21\u9009\u62e9 \\(D_{i}\\) \u4f5c\u4e3a\u9a8c\u8bc1\u96c6 Obtain a final estimate of the risk as the average of the so-obtained \\(k\\) estimates. Using this methodology, the model is always evaluated on a different set of points than it is trained out. So how do we choose \\(k\\) ? Larger values of \\(k\\) provide a better estimate of the true error that we expect when training on a training set with \\(n\\) points, but also requires more computation. To see this suppose we set \\(k=n\\) , then we train on \\(n-1\\) points, but also need to fit our model \\(n\\) many times, which can be very costly. How to choose \\(k\\) in practice depends on how one want to balance this tradeoff; a common value for \\(k\\) in practice is 10 . Let's look at an example following Hastie et al. [HTF09], section 7.10.2. Consider a classification or regression problem with a large number of features. Suppose we perform the following steps: Select a number of relevant features by correlating them with the observations, and select the features that are highly correlated. Use only this subset of features, and train a classification or regression model, for example ridge regression. Use cross-validation to estimate the unknown hyperparameters (e.g., the regularization parameter of ridge regression) and to estimate the prediction error of the final model. Is this a correct application of cross-validation, and if not, what can go wrong?\uff1f Suppose there are \\(n=50\\) examples and originally there are 5000 features that are independent of the labels. Assume both class labels are equally likely, then the true error rate of any classifier is at least \\(50 \\%\\) . Hastie et al. [HTF09] carried out the above steps and the average cross-validation error was 3\\%, a significant underestimation of the error rate. What happened here is that just by chance, features are highly correlated with the data. The right way to apply cross validation in the example above is the following: Divide the examples into \\(k\\) equally sized sets at random. For each set \\(i\\) , using all examples but those in set \\(i\\) , i) find a set of good predictor that are strongly correlated with the labels, ii) use only this set of predictors to build a model, and iii) use the classifier to predict the class labels in fold \\(i\\) . Accumulate the error estimates from each fold to produce the cross-validation estimate of the prediction error.","title":"k-fold cross validation"},{"location":"Supervised%20Learning/0%20Intro/0.2%20Model%20Selection%20and%20Validation/#bootstrap","text":"Suppose we are interested in associating a confidence interval, standard error, or variance estimate with our learning algorithms. As a concrete example, suppose we are interested in estimating the variance of a learning algorithm \\(\\hat{h}\\) for a given example \\(\\mathbf{x}\\) : \\[ \\text{Var}[\\hat{h}(\\mathbf{x})]=\\mathbb{E}_{\\mathcal{D}}\\left[\\left(\\hat{h}_{\\mathcal{D}}-\\mathbb{E}\\left[\\hat{h}_{\\mathcal{D}}(\\mathbf{x})\\right]\\right)^2\\right] . \\] Suppose, as before that the examples \\((y, \\mathbf{x})\\) in \\(\\mathcal{D}\\) are drawn from some joint distribution. In order to estimate this quantity, we can sample datasets \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_b\\) , each containing \\(n\\) examples drawn uniformly at random from the joint distribution, and apply the learning algorithm to those datasets. That yields hypothesis \\(\\hat{h}_1, \\ldots, \\hat{h}_b\\) . Based on those, we can estimate the mean at a point \\(\\mathbf{x}\\) as and the variance as $$ \\text{Var}[\\hat{h}(\\mathbf{x})]=\\frac{1}{b} \\sum_{i=1}^b\\left(h_{\\mathcal{D}_i}(\\mathbf{x})-\\hat{\\mu}(\\mathbf{x})\\right)^2 . $$ While statistically sound, this method for estimating the variance of the estimator is not practically, we would need to sample a large number of examples. The bootstrap, proposed by Efron in 1979 offers a more practical approach. The basic idea of bootstrapping is that in absence of any other information about the unknown distribution, all the information about the distribution we have is contained in the observed sample \\(\\mathcal{D}\\) . Hence resampling from \\(\\mathcal{D}\\) is a good guide to see what can be expected if we would resample from the original distribution. For the concrete example above, we would sample a dataset \\(\\mathcal{D}\\) from the original, joint distribution, and then resample \\(b\\) many datasets \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_b\\) , all containing \\(n\\) samples, by drawing \\(n\\) examples each uniformly at random with replacement from \\(\\mathcal{D}\\) . Then, we would estimate the variance as above. Efron \u5728 1979 \u5e74\u63d0\u51fa\u7684 bootstrap \u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u65b9\u6cd5\u3002 \u81ea\u4e3e\u7684\u57fa\u672c\u601d\u60f3\u662f\uff0c\u5728\u6ca1\u6709\u5173\u4e8e\u672a\u77e5\u5206\u5e03\u7684\u4efb\u4f55\u5176\u4ed6\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u6240\u62e5\u6709\u7684\u5173\u4e8e\u5206\u5e03\u7684\u6240\u6709\u4fe1\u606f\u90fd\u5305\u542b\u5728\u89c2\u5bdf\u5230\u7684\u6837\u672c \\(\\mathcal{D}\\) \u4e2d\u3002 \u56e0\u6b64\uff0c\u4ece \\(\\mathcal{D}\\) \u91cd\u65b0\u91c7\u6837\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u6307\u5357\uff0c\u53ef\u4ee5\u5f88\u597d\u5730\u4e86\u89e3\u5982\u679c\u6211\u4eec\u4ece\u539f\u59cb\u5206\u5e03\u91cd\u65b0\u91c7\u6837\u4f1a\u53d1\u751f\u4ec0\u4e48\u3002 \u5bf9\u4e8e\u4e0a\u9762\u7684\u5177\u4f53\u793a\u4f8b\uff0c\u6211\u4eec\u5c06\u4ece\u539f\u59cb\u8054\u5408\u5206\u5e03\u4e2d\u91c7\u6837\u6570\u636e\u96c6 \\(\\mathcal{D}\\) \uff0c\u7136\u540e\u901a\u8fc7\u4ece \\(\\mathcal{D}\\) \u4e2d\u968f\u673a\u5747\u5300\u62bd\u53d6 \\(n\\) \u4e2a\u6837\u672c(\u82e5\u91c7\u96c6\u5230\u76f8\u540c\u7684\u6837\u672c\u5219\u66ff\u6362)\uff08\u91cd\u590d \\(b\\) \u6b21\uff09 \u91cd\u65b0\u91c7\u6837 \\(b\\) \u591a\u4e2a\u6570\u636e\u96c6 \\(\\mathcal{D}_1,\\ldots,\\mathcal{D}_b\\) \uff0c\u6bcf\u4e2a\u5305\u542b \\(n\\) \u4e2a\u6837\u672c\uff0c\u3002 \u7136\u540e\uff0c\u6211\u4eec\u5c06\u5982\u4e0a\u6240\u8ff0\u4f30\u8ba1\u65b9\u5dee\u3002 Reading: Chapter 7 in [HTF09].","title":"Bootstrap"},{"location":"Supervised%20Learning/0%20Intro/0.2%20Model%20Selection%20and%20Validation/#references","text":"[HTF09] T. J. Hastie, R. J. Tibshirani, and J. J. H. Friedman. The elements of statistical learning. Springer, 2009.","title":"References"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/","text":"Linear Regression Linear regression Intro Dataset: \\(\\mathcal{D}=\\left\\{\\left(y_1, \\mathbf{x}_1\\right), \\ldots,\\left(y_n, \\mathbf{x}_n\\right)\\right\\}\\) Assume \\(y=h(\\mathbf{x})+z\\) Our ultimate goal is to predict the value \\(y\\) corresponding to a feature vector \\(\\mathbf{x}\\) that is not in the training set. Towards this goal, we \"learn\" the function from the dataset, which yields an estimate \\(\\hat{h}\\) of the function \\(h^*\\) , and then predict \\(y=\\hat{h}(\\mathbf{x})\\) . Since learning an arbitrary function is intractable and not a well posed problem, we assume that the function \\(h\\) lies in some hypothesis class \\(\\mathcal{H}\\) of allowable functions. Typically, we assume that the set of functions \\(\\mathcal{H}\\) is parameterized, meaning that there is some (parameter or weight) vector \\(\\boldsymbol{\\theta}\\) which determines a function in the class \\(\\mathcal{H}\\) . A concrete example is the class of all linear function, which is given as the set: suppose the function \\(h\\) is fully determined by the vector \\(\\boldsymbol{\\theta}\\) ; we write \\(h_{\\boldsymbol{\\theta}}\\) to make this dependence explicit. With the class of functions parameterized by a vector \\(\\boldsymbol{\\theta}\\) , the problem of learning \\(h\\) reduces to the problem of estimating the parameter \\(\\boldsymbol{\\theta}\\) . Learn the parameter We choose a cost function loss: \\(\\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}\\) that measures how well (for a fixed \\(\\boldsymbol{\\theta}\\) ) the predictions \\(h_{\\boldsymbol{\\theta}}(\\mathbf{x})\\) describe the output \\(y\\) , then find the parameter that minimizes that loss: $$ \\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\sum_{i=1}^n \\operatorname{loss}\\left(h_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_i\\right), y_i\\right) $$ Here, and in the following we use the hat-notation \\(\\hat{\\boldsymbol{\\theta}}\\) to indicate that this variable is estimated based on data. Loss function The most popular metric in linear regression is the sum of the squares of the fitting error: ( least square cost function/MSE ) $$ \\hat{R}(\\boldsymbol{\\theta})=\\frac{1}{n} \\sum_{i=1}^n\\left(\\left\\langle\\mathbf{x}_i, \\boldsymbol{\\theta}\\right\\rangle-y_i\\right)^2 $$ $$ =\\frac{1}{n}|\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\theta}|_2^2 $$ The least squares cost function is convenient for a variety of reasons: - it can be numerically minimized efficiently as it is convex (convexity is defined below) - it has intuitive geometric and probabilistic interpretations, as we will see later. Least squares estimate: the parameter vector that minimizes this cost function: $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\hat{R}(\\boldsymbol{\\theta}) $$ \u8ba8\u8bba\u7ebf\u6027\u56de\u5f52\u4e2dn\u548cd\u7684\u5173\u7cfb Before discussing how \\(\\hat{\\theta}_{\\mathrm{LS}}\\) can be computed, let us comment on the relation of the number of examples, \\(n\\) , and the number of features, \\(d\\) . Suppose that at least \\(n\\) columns of \\(\\mathbf{X}\\) are linearly independent. Then we must have \\(n \\leq d\\) , and regardless of \\(\\mathbf{y}\\) , we can find a parameter vector \\(\\boldsymbol{\\theta}\\) (or many different ones) such that \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) , and thus describe the independent variable \\(y\\) exactly. - In the linear regression model, this is only possible if there are fewer examples than number of model parameters \\((n \\leq d)\\) . In that regime, even if \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) have nothing to do with each other, we achieve a perfect fit or zero loss. In this regime, we are likely overfitting: the model is too flexible given the data. Thus, fitting a linear model without additional assumptions only makes sense in the so called over-determined or under-parameterized regime, where the number of training examples is larger than the number of features \\((n \\geq d)\\) . If we find a good approximation in the \\((n \\geq d)\\) -regime, then this is an indication that the linear model is capturing structure of the problem. n<=d \u4e0d\u8bbax\u548cy\u7684\u5173\u7cfb\uff0c \u90fd\u80fd\u627e\u5230\u4e00\u4e2a \\(\\theta\\) , \u4f7f\u5f97 \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) overfitting not make senses n>=d: \u5982\u679c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u627e\u5230\u4e86\u4e00\u4e2a\u597d\u7684\u8fd1\u4f3c\u6a21\u578b\uff0c \u5219\u8be5\u6a21\u578b\u6bd4\u8f83\u51c6\u786e Least squares estimation We next show that \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) can be computed in closed form. Assume that \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) has full column rank. - This implies that the number or training examples, \\(n\\) , is larger than the number of model parameters, \\(d\\) . - features of \\(x\\) are linearly independent In the regime \\(n \\geq d\\) , if \\(\\mathbf{X}\\) does not have full column rank, then the features are linearly dependent and thus redundant. Proposition 1. Suppose that \\(\\mathbf{X}\\) has full column rank \\(d\\) . Then for any \\(\\mathbf{y}\\) , we have $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y} $$ There are at least two proofs of this proposition. Analysis of the least squares estimate and in what sense co-linear features are \"bad\" Thus far, we have not commented on the features of the regression model. Intuitively, if the features are very similar to each other, least squares estimation should be difficult. For example, suppose all the features (i.e., the entries of the \\(\\mathbf{x}_i\\) ) are linear functions of each other. Then the columns of \\(\\mathbf{X}\\) are linearly dependent and \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) is not unique. In statistics it is common to refer to features with this property as co-linear . A set of variables is said to be perfectly multi-colinear if there is one or more exact linear relationship among some of the variables \\(x_{i 1}, \\ldots, x_{i d}\\) ; for example the first two variables are co-linear if \\(x_{i 1}=-2 x_{i 2}\\) for all \\(i\\) . In practice, features are usually not co-linear but can be highly correlated, which leads to instable least squares estimates. Case1. \\(X\\) has full column rank and \\(y\\) is generated according to a linear model with additive Gaussian noise: $$ \\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}^*+\\mathbf{z} $$ Here, the entries of \\(\\mathbf{z}\\) are independent zero-mean Gaussian noise with variance \\(1 / n\\) , Then: the expected error is 0, or in statistical terms, the LSE is unbiased Case2. Features of \\(X\\) are strongly correlated where \\(\\sigma_i\\) are the singular values of \\(\\mathbf{X}\\) , and expectation is over the Gaussian noise \\(\\mathbf{z}\\) then \\(1 / \\sigma_{\\min }^2\\) is large and the bound becomes large. This is the case if some of the features are highly correlated, known as multicollinearity in the statistics literature. On the other hand, as expected, the error decays in \\(n\\) , the larger \\(n\\) , the smaller the error. proof of the error function:","title":"Linear Regression"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#linear-regression","text":"","title":"Linear Regression"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#linear-regression_1","text":"","title":"Linear regression"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#intro","text":"Dataset: \\(\\mathcal{D}=\\left\\{\\left(y_1, \\mathbf{x}_1\\right), \\ldots,\\left(y_n, \\mathbf{x}_n\\right)\\right\\}\\) Assume \\(y=h(\\mathbf{x})+z\\) Our ultimate goal is to predict the value \\(y\\) corresponding to a feature vector \\(\\mathbf{x}\\) that is not in the training set. Towards this goal, we \"learn\" the function from the dataset, which yields an estimate \\(\\hat{h}\\) of the function \\(h^*\\) , and then predict \\(y=\\hat{h}(\\mathbf{x})\\) . Since learning an arbitrary function is intractable and not a well posed problem, we assume that the function \\(h\\) lies in some hypothesis class \\(\\mathcal{H}\\) of allowable functions. Typically, we assume that the set of functions \\(\\mathcal{H}\\) is parameterized, meaning that there is some (parameter or weight) vector \\(\\boldsymbol{\\theta}\\) which determines a function in the class \\(\\mathcal{H}\\) . A concrete example is the class of all linear function, which is given as the set: suppose the function \\(h\\) is fully determined by the vector \\(\\boldsymbol{\\theta}\\) ; we write \\(h_{\\boldsymbol{\\theta}}\\) to make this dependence explicit. With the class of functions parameterized by a vector \\(\\boldsymbol{\\theta}\\) , the problem of learning \\(h\\) reduces to the problem of estimating the parameter \\(\\boldsymbol{\\theta}\\) .","title":"Intro"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#learn-the-parameter","text":"We choose a cost function loss: \\(\\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}\\) that measures how well (for a fixed \\(\\boldsymbol{\\theta}\\) ) the predictions \\(h_{\\boldsymbol{\\theta}}(\\mathbf{x})\\) describe the output \\(y\\) , then find the parameter that minimizes that loss: $$ \\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\sum_{i=1}^n \\operatorname{loss}\\left(h_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_i\\right), y_i\\right) $$ Here, and in the following we use the hat-notation \\(\\hat{\\boldsymbol{\\theta}}\\) to indicate that this variable is estimated based on data.","title":"Learn the parameter"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#loss-function","text":"The most popular metric in linear regression is the sum of the squares of the fitting error: ( least square cost function/MSE ) $$ \\hat{R}(\\boldsymbol{\\theta})=\\frac{1}{n} \\sum_{i=1}^n\\left(\\left\\langle\\mathbf{x}_i, \\boldsymbol{\\theta}\\right\\rangle-y_i\\right)^2 $$ $$ =\\frac{1}{n}|\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\theta}|_2^2 $$ The least squares cost function is convenient for a variety of reasons: - it can be numerically minimized efficiently as it is convex (convexity is defined below) - it has intuitive geometric and probabilistic interpretations, as we will see later. Least squares estimate: the parameter vector that minimizes this cost function: $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\hat{R}(\\boldsymbol{\\theta}) $$","title":"Loss function"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#nd","text":"Before discussing how \\(\\hat{\\theta}_{\\mathrm{LS}}\\) can be computed, let us comment on the relation of the number of examples, \\(n\\) , and the number of features, \\(d\\) . Suppose that at least \\(n\\) columns of \\(\\mathbf{X}\\) are linearly independent. Then we must have \\(n \\leq d\\) , and regardless of \\(\\mathbf{y}\\) , we can find a parameter vector \\(\\boldsymbol{\\theta}\\) (or many different ones) such that \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) , and thus describe the independent variable \\(y\\) exactly. - In the linear regression model, this is only possible if there are fewer examples than number of model parameters \\((n \\leq d)\\) . In that regime, even if \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) have nothing to do with each other, we achieve a perfect fit or zero loss. In this regime, we are likely overfitting: the model is too flexible given the data. Thus, fitting a linear model without additional assumptions only makes sense in the so called over-determined or under-parameterized regime, where the number of training examples is larger than the number of features \\((n \\geq d)\\) . If we find a good approximation in the \\((n \\geq d)\\) -regime, then this is an indication that the linear model is capturing structure of the problem. n<=d \u4e0d\u8bbax\u548cy\u7684\u5173\u7cfb\uff0c \u90fd\u80fd\u627e\u5230\u4e00\u4e2a \\(\\theta\\) , \u4f7f\u5f97 \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) overfitting not make senses n>=d: \u5982\u679c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u627e\u5230\u4e86\u4e00\u4e2a\u597d\u7684\u8fd1\u4f3c\u6a21\u578b\uff0c \u5219\u8be5\u6a21\u578b\u6bd4\u8f83\u51c6\u786e","title":"\u8ba8\u8bba\u7ebf\u6027\u56de\u5f52\u4e2dn\u548cd\u7684\u5173\u7cfb"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#least-squares-estimation","text":"We next show that \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) can be computed in closed form. Assume that \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) has full column rank. - This implies that the number or training examples, \\(n\\) , is larger than the number of model parameters, \\(d\\) . - features of \\(x\\) are linearly independent In the regime \\(n \\geq d\\) , if \\(\\mathbf{X}\\) does not have full column rank, then the features are linearly dependent and thus redundant. Proposition 1. Suppose that \\(\\mathbf{X}\\) has full column rank \\(d\\) . Then for any \\(\\mathbf{y}\\) , we have $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y} $$ There are at least two proofs of this proposition.","title":"Least squares estimation"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#analysis-of-the-least-squares-estimate-and-in-what-sense-co-linear-features-are-bad","text":"Thus far, we have not commented on the features of the regression model. Intuitively, if the features are very similar to each other, least squares estimation should be difficult. For example, suppose all the features (i.e., the entries of the \\(\\mathbf{x}_i\\) ) are linear functions of each other. Then the columns of \\(\\mathbf{X}\\) are linearly dependent and \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) is not unique. In statistics it is common to refer to features with this property as co-linear . A set of variables is said to be perfectly multi-colinear if there is one or more exact linear relationship among some of the variables \\(x_{i 1}, \\ldots, x_{i d}\\) ; for example the first two variables are co-linear if \\(x_{i 1}=-2 x_{i 2}\\) for all \\(i\\) . In practice, features are usually not co-linear but can be highly correlated, which leads to instable least squares estimates. Case1. \\(X\\) has full column rank and \\(y\\) is generated according to a linear model with additive Gaussian noise: $$ \\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}^*+\\mathbf{z} $$ Here, the entries of \\(\\mathbf{z}\\) are independent zero-mean Gaussian noise with variance \\(1 / n\\) , Then: the expected error is 0, or in statistical terms, the LSE is unbiased Case2. Features of \\(X\\) are strongly correlated where \\(\\sigma_i\\) are the singular values of \\(\\mathbf{X}\\) , and expectation is over the Gaussian noise \\(\\mathbf{z}\\) then \\(1 / \\sigma_{\\min }^2\\) is large and the bound becomes large. This is the case if some of the features are highly correlated, known as multicollinearity in the statistics literature. On the other hand, as expected, the error decays in \\(n\\) , the larger \\(n\\) , the smaller the error. proof of the error function:","title":"Analysis of the least squares estimate and in what sense co-linear features are \"bad\""},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/","text":"Ridge Regression \u5cad\u56de\u5f52 Motivation \u4e0a\u4e00\u8282\u8bb2\u7684\u7ebf\u6027\u56de\u5f52\u4e2d\uff0cexpected error: \u5982\u679c Features of \\(X\\) are (strongly) correlated specifically if its smallest singular value, \\(\\sigma_{\\min }^2(\\mathbf{X})\\) , is close to zero. In this case, \\(1 / \\sigma_{\\min }^2(\\mathbf{X})\\) is large, and therefore the expected error above is large. solution: ridge regression Ridge regression Ridge regression: \u6700\u5c0f\u4e8c\u4e58\u6cd5 + l2 norm regularization This additional term promotes solutions that yield a good fit while at the same time enforcing the coefficients of \\(\\theta\\) to be sufficiently small. Also known as: ridge regression (statistics) Tkhonov regularization/weight decay (machine learning) In order to avoid this, we can add a term penalizing the norm of the coefficient vector \\(\\boldsymbol{\\theta}\\) . This additional term promotes solutions that yield a good fit while at the same time enforcing the coefficients of \\(\\theta\\) to be sufficiently small. Incorporating assumptions about the solution-in this case that the coefficients should not be too large - is called regularization. A least squares penalty combined with \\(\\ell2\\) -norm regularization is called ridge regression in statistics and is also known as Tikhonov regularization and weight decay in machine learning. The ridge regression estimate is: Lambda parameter \\(\\lambda\\) is a fixed regularization parameter. different values of \\(\\lambda\\) give us different ridge regression estimates. Observe that for \\(\\lambda \\rightarrow 0\\) , the ridge regression estimate converges to the least squares estimate, and for \\(\\lambda \\rightarrow \\infty\\) , the ridge regression estimate converges to zero. Ridge regression estimate If \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) has full column rank \\(d\\) , the ridge regression estimate has a closed-form solution: Equation 5. proof: Recall that \\(\\mathbf{X} \\hat{\\theta}_{\\mathrm{LS}}\\) is the orthogonal projection of the data onto the column space of the feature matrix, i.e., \\(\\mathbf{X} \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\mathbf{U U}^T \\mathbf{y}\\) , where \\(\\mathbf{U} \\in \\mathbb{R}^{n \\times d}\\) is an orthogonal basis for the column space of \\(\\mathbf{X}\\) . The approximation \\(\\mathbf{X} \\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) can no longer be interpreted as the orthogonal projection of the data onto the column space of the feature matrix. However, it can be interpreted as a modified projection, where each component of the data in the direction of the left singular vector \\(\\mathbf{u}_i\\) of the feature matrix is shrunk by a factor of \\(\\sigma_i^2 /\\left(\\sigma_i^2+\\lambda\\right)\\) . Here, \\(\\sigma_i\\) is the singular value of the feature matrix corresponding to the singular vector \\(\\mathbf{u}_i\\) . In more detail, consider the singular value decomposition \\(\\mathbf{X}=\\mathbf{U} \\Sigma \\mathbf{V}^T\\) , and let \\(\\mathbf{u}_i\\) be the \\(i\\) -th column of \\(\\mathbf{U}\\) , i.e., \\(\\mathbf{U}=\\left[\\mathbf{u}_1, \\ldots, \\mathbf{u}_d\\right]\\) . Then we have equation 5 . where we used that for two square matrixes that are invertible, we have that \\((\\mathbf{A B})^{-1}=\\mathbf{B}^{-1} \\mathbf{A}^{-1}\\) . Next, note that \\(\\left(\\Sigma^2+\\lambda \\mathbf{I}\\right)\\) is a diagonal matrix with \\(\\sigma_i^2+\\lambda, i=1, \\ldots, d\\) on its diagonal, thus \\(\\left(\\Sigma^2+\\lambda \\mathbf{I}\\right)^{-1}\\) is a diagonal matrix with \\(1 /\\left(\\sigma_i^2+\\lambda\\right), i=1, \\ldots, d\\) on its diagonal. As a consequence, This concludes the proof of equation (5) Bias-variance tradeoff of the ridge regression estimate Assumption: the data indeed follows a linear model, i.e., \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}^*+\\mathbf{z}\\) the ride regression estimator can be decomposed into a signal and a noise term: Analysis suppose that the noise \\(\\mathbf{z}\\) is a zero-mean random variable. Then the expectation of \\(\\boldsymbol{\\theta}_{\\text {ridge }}\\) equals the first term \\(\\hat{\\theta}_{\\text {ridge }}^{\\text {signal }}\\) which is deterministic. The second term, \\(\\hat{\\theta}_{\\text {ridge }}^{\\text {noise }}\\) , is random and determines the variance of the ridge estimator. If \\(\\lambda=0\\) , the first term becomes \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}^{\\text {signal }}=\\boldsymbol{\\theta}^*\\) , otherwise the first term is not equal to \\(\\boldsymbol{\\theta}^*\\) . Thus, ridge regression is a biased estimator (for \\(\\lambda>0\\) ). Increasing \\(\\lambda\\) moves the mean of the estimate \\(\\hat{\\theta}_{\\text {ridge }}\\) farther from the true model parameter \\(\\theta^*\\) , and thus increases the bias of the estimate, which is defined as \\(\\mathbb{E}\\left[\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right]-\\boldsymbol{\\theta}^*=\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}^{\\text {signal }}-\\boldsymbol{\\theta}^*\\) \\(\\lambda\\) \u8d8a\u5927\uff0c \\(\\hat{\\theta}_{\\text{ridge}}\\) \u7684\u7edd\u5bf9\u503c\u8d8a\u5c0f --> 0 But in exchange, increasing \\(\\lambda\\) also shrinks the impact of the noise. Thus, choosing \\(\\lambda\\) appropriately enables us to adapt to the conditioning of the feature matrix and to the noise level for achieving a good tradeoff between the bias and the variance. \u56e0\u6b64\uff0c\u9009\u62e9\u9002\u5f53\u7684\u03bb\u4f7f\u6211\u4eec\u80fd\u591f\u9002\u5e94\u7279\u5f81\u77e9\u9635\u7684\u6761\u4ef6\u548c\u566a\u58f0\u6c34\u5e73\uff0c\u4ee5\u5728\u504f\u5dee\u548c\u65b9\u5dee\u4e4b\u95f4\u5b9e\u73b0\u826f\u597d\u7684\u6298\u8877\u3002 Note that the optimal choice depends on the noise which is unknown to us, the conditioning of the feature matrix, thus we cannot simply compute the optimal choice based on the data. Later in the course we will discuss methods that enable us to determine a good choice of \\(\\lambda\\) based on the data. Compared with linear regression where normalized MSE = \\(\\left\\|\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right\\|_2^2 /\\|\\boldsymbol{\\theta}\\|_2^2\\) \u53ef\u4ee5\u770b\u5230\u4f7f\u5f97loss\u6700\u5c0f\u7684 \\(\\lambda \\neq 0\\)","title":"Ridge Regression \u5cad\u56de\u5f52"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#ridge-regression","text":"","title":"Ridge Regression \u5cad\u56de\u5f52"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#motivation","text":"\u4e0a\u4e00\u8282\u8bb2\u7684\u7ebf\u6027\u56de\u5f52\u4e2d\uff0cexpected error: \u5982\u679c Features of \\(X\\) are (strongly) correlated specifically if its smallest singular value, \\(\\sigma_{\\min }^2(\\mathbf{X})\\) , is close to zero. In this case, \\(1 / \\sigma_{\\min }^2(\\mathbf{X})\\) is large, and therefore the expected error above is large. solution: ridge regression","title":"Motivation"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#ridge-regression_1","text":"Ridge regression: \u6700\u5c0f\u4e8c\u4e58\u6cd5 + l2 norm regularization This additional term promotes solutions that yield a good fit while at the same time enforcing the coefficients of \\(\\theta\\) to be sufficiently small. Also known as: ridge regression (statistics) Tkhonov regularization/weight decay (machine learning) In order to avoid this, we can add a term penalizing the norm of the coefficient vector \\(\\boldsymbol{\\theta}\\) . This additional term promotes solutions that yield a good fit while at the same time enforcing the coefficients of \\(\\theta\\) to be sufficiently small. Incorporating assumptions about the solution-in this case that the coefficients should not be too large - is called regularization. A least squares penalty combined with \\(\\ell2\\) -norm regularization is called ridge regression in statistics and is also known as Tikhonov regularization and weight decay in machine learning. The ridge regression estimate is:","title":"Ridge regression"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#lambda-parameter","text":"\\(\\lambda\\) is a fixed regularization parameter. different values of \\(\\lambda\\) give us different ridge regression estimates. Observe that for \\(\\lambda \\rightarrow 0\\) , the ridge regression estimate converges to the least squares estimate, and for \\(\\lambda \\rightarrow \\infty\\) , the ridge regression estimate converges to zero.","title":"Lambda parameter"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#ridge-regression-estimate","text":"If \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) has full column rank \\(d\\) , the ridge regression estimate has a closed-form solution: Equation 5. proof: Recall that \\(\\mathbf{X} \\hat{\\theta}_{\\mathrm{LS}}\\) is the orthogonal projection of the data onto the column space of the feature matrix, i.e., \\(\\mathbf{X} \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\mathbf{U U}^T \\mathbf{y}\\) , where \\(\\mathbf{U} \\in \\mathbb{R}^{n \\times d}\\) is an orthogonal basis for the column space of \\(\\mathbf{X}\\) . The approximation \\(\\mathbf{X} \\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) can no longer be interpreted as the orthogonal projection of the data onto the column space of the feature matrix. However, it can be interpreted as a modified projection, where each component of the data in the direction of the left singular vector \\(\\mathbf{u}_i\\) of the feature matrix is shrunk by a factor of \\(\\sigma_i^2 /\\left(\\sigma_i^2+\\lambda\\right)\\) . Here, \\(\\sigma_i\\) is the singular value of the feature matrix corresponding to the singular vector \\(\\mathbf{u}_i\\) . In more detail, consider the singular value decomposition \\(\\mathbf{X}=\\mathbf{U} \\Sigma \\mathbf{V}^T\\) , and let \\(\\mathbf{u}_i\\) be the \\(i\\) -th column of \\(\\mathbf{U}\\) , i.e., \\(\\mathbf{U}=\\left[\\mathbf{u}_1, \\ldots, \\mathbf{u}_d\\right]\\) . Then we have equation 5 . where we used that for two square matrixes that are invertible, we have that \\((\\mathbf{A B})^{-1}=\\mathbf{B}^{-1} \\mathbf{A}^{-1}\\) . Next, note that \\(\\left(\\Sigma^2+\\lambda \\mathbf{I}\\right)\\) is a diagonal matrix with \\(\\sigma_i^2+\\lambda, i=1, \\ldots, d\\) on its diagonal, thus \\(\\left(\\Sigma^2+\\lambda \\mathbf{I}\\right)^{-1}\\) is a diagonal matrix with \\(1 /\\left(\\sigma_i^2+\\lambda\\right), i=1, \\ldots, d\\) on its diagonal. As a consequence, This concludes the proof of equation (5)","title":"Ridge regression estimate"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#bias-variance-tradeoff-of-the-ridge-regression-estimate","text":"Assumption: the data indeed follows a linear model, i.e., \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}^*+\\mathbf{z}\\) the ride regression estimator can be decomposed into a signal and a noise term:","title":"Bias-variance tradeoff of the ridge regression estimate"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#analysis","text":"suppose that the noise \\(\\mathbf{z}\\) is a zero-mean random variable. Then the expectation of \\(\\boldsymbol{\\theta}_{\\text {ridge }}\\) equals the first term \\(\\hat{\\theta}_{\\text {ridge }}^{\\text {signal }}\\) which is deterministic. The second term, \\(\\hat{\\theta}_{\\text {ridge }}^{\\text {noise }}\\) , is random and determines the variance of the ridge estimator. If \\(\\lambda=0\\) , the first term becomes \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}^{\\text {signal }}=\\boldsymbol{\\theta}^*\\) , otherwise the first term is not equal to \\(\\boldsymbol{\\theta}^*\\) . Thus, ridge regression is a biased estimator (for \\(\\lambda>0\\) ). Increasing \\(\\lambda\\) moves the mean of the estimate \\(\\hat{\\theta}_{\\text {ridge }}\\) farther from the true model parameter \\(\\theta^*\\) , and thus increases the bias of the estimate, which is defined as \\(\\mathbb{E}\\left[\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right]-\\boldsymbol{\\theta}^*=\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}^{\\text {signal }}-\\boldsymbol{\\theta}^*\\) \\(\\lambda\\) \u8d8a\u5927\uff0c \\(\\hat{\\theta}_{\\text{ridge}}\\) \u7684\u7edd\u5bf9\u503c\u8d8a\u5c0f --> 0 But in exchange, increasing \\(\\lambda\\) also shrinks the impact of the noise. Thus, choosing \\(\\lambda\\) appropriately enables us to adapt to the conditioning of the feature matrix and to the noise level for achieving a good tradeoff between the bias and the variance. \u56e0\u6b64\uff0c\u9009\u62e9\u9002\u5f53\u7684\u03bb\u4f7f\u6211\u4eec\u80fd\u591f\u9002\u5e94\u7279\u5f81\u77e9\u9635\u7684\u6761\u4ef6\u548c\u566a\u58f0\u6c34\u5e73\uff0c\u4ee5\u5728\u504f\u5dee\u548c\u65b9\u5dee\u4e4b\u95f4\u5b9e\u73b0\u826f\u597d\u7684\u6298\u8877\u3002 Note that the optimal choice depends on the noise which is unknown to us, the conditioning of the feature matrix, thus we cannot simply compute the optimal choice based on the data. Later in the course we will discuss methods that enable us to determine a good choice of \\(\\lambda\\) based on the data.","title":"Analysis"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#compared-with-linear-regression","text":"where normalized MSE = \\(\\left\\|\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right\\|_2^2 /\\|\\boldsymbol{\\theta}\\|_2^2\\) \u53ef\u4ee5\u770b\u5230\u4f7f\u5f97loss\u6700\u5c0f\u7684 \\(\\lambda \\neq 0\\)","title":"Compared with linear regression"}]}