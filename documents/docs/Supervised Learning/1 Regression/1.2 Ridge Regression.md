# Ridge Regression 岭回归
## Motivation
上一节讲的线性回归中，expected error: ![](https://i.imgur.com/pL3abV4.png)

如果

- Features of $X$ are (strongly) correlated
    - specifically if its smallest singular value, $\sigma_{\min }^2(\mathbf{X})$, is close to zero. In this case, $1 / \sigma_{\min }^2(\mathbf{X})$ is large, and therefore the expected error above is large.

solution: ridge regression

## Ridge regression
Ridge regression: 最小二乘法 + l2 norm regularization
![](https://i.imgur.com/AoMwZrp.png)

- This additional term promotes solutions that yield a good fit
- while at the same time enforcing the coefficients of $\theta$ to be sufficiently small.

Also known as:

1. ridge regression (statistics)
2. Tkhonov regularization/weight decay (machine learning)


> *In order to avoid this, we can add a term penalizing the norm of the coefficient vector $\boldsymbol{\theta}$. This additional term promotes solutions that yield a good fit while at the same time enforcing the coefficients of $\theta$ to be sufficiently small. Incorporating assumptions about the solution-in this case that the coefficients should not be too large - is called regularization. A least squares penalty combined with $\ell2$-norm regularization is called ridge regression in statistics and is also known as Tikhonov regularization and weight decay in machine learning. The ridge regression estimate is:*

### Lambda parameter
$\lambda$ is a fixed regularization parameter.

- different values of $\lambda$ give us different ridge regression estimates.
- Observe that for $\lambda \rightarrow 0$, the ridge regression estimate converges to the least squares estimate, and for $\lambda \rightarrow \infty$, the ridge regression estimate converges to zero.



## Ridge regression estimate
If $\mathbf{X} \in \mathbb{R}^{n \times d}$ has full column rank $d$, the ridge regression estimate has a closed-form solution:
![](https://i.imgur.com/t4wIFHI.png)

**Equation 5.**
![](https://i.imgur.com/5I8VcFM.png)

> proof:
> Recall that $\mathbf{X} \hat{\theta}_{\mathrm{LS}}$ is the orthogonal projection of the data onto the column space of the feature matrix, i.e., $\mathbf{X} \hat{\boldsymbol{\theta}}_{\mathrm{LS}}=\mathbf{U U}^T \mathbf{y}$, where $\mathbf{U} \in \mathbb{R}^{n \times d}$ is an orthogonal basis for the column space of $\mathbf{X}$. The approximation $\mathbf{X} \hat{\boldsymbol{\theta}}_{\text {ridge }}$ can no longer be interpreted as the orthogonal projection of the data onto the column space of the feature matrix. However, it can be interpreted as a modified projection, where each component of the data in the direction of the left singular vector $\mathbf{u}_i$ of the feature matrix is shrunk by a factor of $\sigma_i^2 /\left(\sigma_i^2+\lambda\right)$. Here, $\sigma_i$ is the singular value of the feature matrix corresponding to the singular vector $\mathbf{u}_i$. In more detail, consider the singular value decomposition $\mathbf{X}=\mathbf{U} \Sigma \mathbf{V}^T$, and let $\mathbf{u}_i$ be the $i$-th column of $\mathbf{U}$, i.e., $\mathbf{U}=\left[\mathbf{u}_1, \ldots, \mathbf{u}_d\right]$. Then we have **equation 5**.
>
> ![](https://i.imgur.com/hAVrUYo.png)
> where we used that for two square matrixes that are invertible, we have that $(\mathbf{A B})^{-1}=\mathbf{B}^{-1} \mathbf{A}^{-1}$. Next, note that $\left(\Sigma^2+\lambda \mathbf{I}\right)$ is a diagonal matrix with $\sigma_i^2+\lambda, i=1, \ldots, d$ on its diagonal, thus $\left(\Sigma^2+\lambda \mathbf{I}\right)^{-1}$ is a diagonal matrix with $1 /\left(\sigma_i^2+\lambda\right), i=1, \ldots, d$ on its diagonal. As a consequence, ![](https://i.imgur.com/9e3MY1v.png)
> This concludes the proof of equation (5)

## Bias-variance tradeoff of the ridge regression estimate
Assumption: the data indeed follows a linear model, i.e., $\mathbf{y}=\mathbf{X} \boldsymbol{\theta}^*+\mathbf{z}$

the ride regression estimator can be decomposed into a signal and a noise term:
![](https://i.imgur.com/3emx8N5.png)

### Analysis
1. suppose that the noise $\mathbf{z}$ is a zero-mean random variable. Then the expectation of $\boldsymbol{\theta}_{\text {ridge }}$ equals the first term $\hat{\theta}_{\text {ridge }}^{\text {signal }}$ which is deterministic. The second term, $\hat{\theta}_{\text {ridge }}^{\text {noise }}$, is random and determines the variance of the ridge estimator.
2. If $\lambda=0$, the first term becomes $\hat{\boldsymbol{\theta}}_{\text {ridge }}^{\text {signal }}=\boldsymbol{\theta}^*$, otherwise the first term is not equal to $\boldsymbol{\theta}^*$. Thus, ridge regression is a biased estimator (for $\lambda>0$ ).
3. Increasing $\lambda$ moves the mean of the estimate $\hat{\theta}_{\text {ridge }}$ farther from the true model parameter $\theta^*$, and thus increases the bias of the estimate, which is defined as $\mathbb{E}\left[\hat{\boldsymbol{\theta}}_{\text {ridge }}\right]-\boldsymbol{\theta}^*=\hat{\boldsymbol{\theta}}_{\text {ridge }}^{\text {signal }}-\boldsymbol{\theta}^*$
    - $\lambda$越大， $\hat{\theta}_{\text{ridge}}$的绝对值越小 --> 0 ![](https://i.imgur.com/tZsxRv9.png)
4. But in exchange, increasing $\lambda$ also shrinks the impact of the noise.

Thus, choosing $\lambda$ appropriately enables us to adapt to the conditioning of the feature matrix and to the noise level for achieving a good tradeoff between the bias and the variance.
> 因此，选择适当的λ使我们能够适应特征矩阵的条件和噪声水平，以在偏差和方差之间实现良好的折衷。

Note that the optimal choice depends on

- the noise which is unknown to us,
- the conditioning of the feature matrix,

thus we cannot simply compute the optimal choice based on the data.

Later in the course we will discuss methods that enable us to determine a good choice of $\lambda$ based on the data.



### Compared with linear regression
![](https://i.imgur.com/ZiU507c.png)

where normalized MSE = $\left\|\boldsymbol{\theta}-\hat{\boldsymbol{\theta}}_{\text {ridge }}\right\|_2^2 /\|\boldsymbol{\theta}\|_2^2$

可以看到使得loss最小的$\lambda \neq 0$ 