{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to My ML Notes","title":"Welcome to My ML Notes"},{"location":"#welcome-to-my-ml-notes","text":"","title":"Welcome to My ML Notes"},{"location":"Optimization/1%20Convexity/","text":"Convex Optimization Optimization Problems Many problems in science and engineering can be formulated as an optimization problem: $$ \\text { minimize } f(\\mathbf{x}) \\quad \\text { subject to } \\quad \\mathbf{x} \\in \\mathcal{C} \\text {, } $$ where \\(f\\) is a cost function and \\(\\mathcal{C}\\) is a set. Due to \\(\\max _{\\mathbf{x} \\in \\mathcal{C}} f(\\mathbf{x})=\\min _{\\mathbf{x} \\in \\mathcal{C}}-f(\\mathbf{x})\\) , this formulation includes the problem of maximizing a cost function. The function \\(f\\) can model goodness of fit in machine learning (we'll see many examples later), utility, or cost. The set \\(\\mathcal{C}\\) can incorporate constraints such as a limited budget \\((\\|\\mathbf{x}\\| \\leq B)\\) or priors ( \\(\\mathbf{x}\\) is non-negative). We say that \\(\\mathbf{x}^*\\) is a (global) solution to the optimization problem (1) if \\(\\mathbf{x}^* \\in \\mathcal{C}\\) , and \\(f\\left(\\mathbf{x}^*\\right) \\leq f(\\mathbf{x})\\) for all \\(\\mathrm{x} \\in \\mathcal{C}\\) . We say that \\(\\mathrm{x}^*\\) is a local solution to the optimization problem if in a neighborhood \\(\\mathcal{N}\\) around \\(\\mathbf{x}^*, f\\left(\\mathbf{x}^*\\right) \\leq f(\\mathbf{x})\\) for all \\(\\mathbf{x} \\in \\mathcal{C} \\cap \\mathcal{N}\\) . Optimality can be very hard to check, even if \\(f\\) is differentiable and \\(\\mathcal{C}=\\mathbb{R}^n\\) . Convexity An important class of optimization problems for which we can check optimality rather easily are convex optimization problems. A standard reference for convex optimization problems is the book [BV04]. Definition 1. A convex set \\(\\mathcal{C}\\) is any set such that for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathcal{C}\\) and all \\(\\theta \\in(0,1)\\) , $$ \\theta \\mathbf{x}+(1-\\theta) \\mathbf{y} \\in \\mathcal{C} $$ \u8fde\u63a5set\u5185\u4efb\u610f\u4e24\u4e2a\u70b9, \u8fd9\u4e24\u4e2a\u70b9\u7684\u8fde\u7ebf\u4e0a\u4efb\u610f\u4e00\u70b9\u90fd\u5728set\u5185 Figure 1 shows examples of convex and non-convex sets. Intersections of convex sets are convex. Important examples of convex sets are the following: Subspaces \\(\\left\\{\\mathbf{x}=\\mathbf{U a}: \\mathbf{a} \\in \\mathbb{R}^d\\right\\}\\) , Affines spaces \\(\\left\\{\\mathbf{x}=\\mathbf{U a}+\\mathbf{b}: \\mathbf{a} \\in \\mathbb{R}^d\\right\\}\\) Half-spaces \\(\\{\\mathbf{x}:\\langle\\mathbf{a}, \\mathbf{x}\\rangle \\leq b\\}\\) Definition 2. for a set of points \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^n\\) , a convex combination of them is defined as the set \\[ \\left\\{\\mathbf{x}=\\sum_{i=1}^n \\theta_i \\mathbf{x}_i: \\sum_{i=1}^n \\theta_i=1, \\theta_i \\geq 0\\right\\} \\] A set \\(\\mathcal{C}\\) is convex if and only if it contains all convex combinations of its elements. For any given set \\(\\mathcal{S}\\) , the convex hull of \\(\\mathcal{S}\\) is defined as the set of all convex combinations of points in \\(\\mathcal{S}\\) . Intuitively, it is the smallest convex set that contains \\(\\mathcal{S}\\) . As an example, consider the non-convex set \\(\\left\\{\\mathbf{x}:\\|\\mathbf{x}\\|_0 \\leq 1,\\|\\mathbf{x}\\|_1 \\leq 1\\right\\}\\) . Its convex hull is the \\(\\ell_1\\) -norm ball \\(\\left\\{\\mathbf{x}\\right.\\) : \\(\\left.\\|\\mathbf{x}\\|_1 \\leq 1\\right\\}\\) . We are now ready for one of the most important definitions in this class. Definition 3. A function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is convex if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) , and all \\(\\theta \\in(0,1)\\) , $$ \\theta f(\\mathbf{x})+(1-\\theta) f(\\mathbf{y}) \\geq f(\\theta \\mathbf{x}+(1-\\theta) \\mathbf{y}) . $$ A function is strictly convex, if the inequality above is strict whenever \\(\\mathbf{x} \\neq \\mathbf{y}\\) . A function \\(f\\) is concave if \\(-f\\) is convex. Common examples of convex functions are: Linear functions: \\(f(\\mathbf{x})=\\langle\\mathbf{a}, \\mathbf{x}\\rangle+b\\) , Quadratics: \\(f(\\mathbf{x})=\\frac{1}{2} \\mathbf{x}^T \\mathbf{Q} \\mathbf{x}+\\mathbf{b}^T \\mathbf{x}+c\\) , where \\(\\mathbf{Q}\\) is positive semidefinite. The function \\(f\\) is convex if and only if \\(\\mathbf{Q}\\) is positive semidefinite. Any norm \\(f(\\mathbf{x})=\\|\\mathbf{x}\\|\\) is convex. This follows from the triangle inequality and homogeneity/scalability. An important property of convex functions is that local minima are global. Proposition 1. Any local minimum of a convex function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is also a global minimum. We now turn to first order optimality conditions. To this end, we first state a proposition stating that a function is convex if and only if it is lower bounded by its first order Taylor expansion at any point. Proposition 2. A differentiable function \\(f\\) is convex if and only if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) , $$ f(\\mathbf{y}) \\geq f(\\mathbf{x})+\\langle\\mathbf{y}-\\mathbf{x}, \\nabla f(\\mathbf{x})\\rangle . $$ It is strictly convex if and only if the inequality holds strictly for all \\(\\mathbf{x} \\neq \\mathbf{y}\\) . An important consequence is the following optimality condition. Corollary 1. If a differentiable function \\(f\\) is convex and \\(\\nabla f\\left(\\mathbf{x}^*\\right)=0\\) , then \\(\\mathbf{x}^*\\) is a global minimizer of \\(f\\) . Computational aspects of optimization algorithms Suppose we want to minimize a differentiable function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) over \\(\\mathbb{R}^n\\) : $$ \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{\\operatorname{minimize}} f(\\mathbf{x}) . $$ For simplicity assume that \\(f(\\mathbf{x})=\\|\\mathbf{A} \\mathbf{x}-\\mathbf{y}\\|_2^2\\) , where \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) is a matrix with full column rank. So the optimization problem above is a standard least squares problem. If the columns are linearly independent, we can simply obtain a closed form solution as \\(\\mathbf{x}^*=\\left(\\mathbf{A}^T \\mathbf{A}\\right)^{-1} \\mathbf{A}^T \\mathbf{y}\\) . However, for many practically relevant functions \\(f\\) , we cannot compute a closed form solution. Even if we can, we might not want to because computing the closed form solution can be computationally expensive, e.g., for \\(\\mathbf{x}^*=\\left(\\mathbf{A}^T \\mathbf{A}\\right)^{-1} \\mathbf{A}^T \\mathbf{y}\\) we have to compute the inverse of a possibly large matrix. In the first part of this course we consider algorithms for finding approximate solutions of the minimization problem above. In practice we are essentially always content with an approximate solution (a computer can only give us a solution up to machine precision anyways). We assume that we have oracle access to \\(f\\) : A zeroth order oracle returns \\(f(\\mathbf{x})\\) for a given \\(\\mathbf{x}\\) . A first order oracle returns \\(\\{f(\\mathbf{x}), \\nabla f(\\mathbf{x})\\}\\) for a given \\(\\mathbf{x}\\) . A second order oracle returns \\(\\left\\{f(\\mathbf{x}), \\nabla f(\\mathbf{x}), \\nabla^2 f(\\mathbf{x})\\right\\}\\) for a given \\(\\mathbf{x}\\) . Ultimately, we are interested in understanding the computational complexity of convex optimization algorithms, i.e., the number of flops to obtain a \\(\\epsilon\\) -accurate solution (an \\(\\epsilon\\) -accurate solution can be a vector \\(\\mathbf{x}^k\\) obeying \\(\\left\\|\\mathbf{x}^k-\\mathbf{x}^*\\right\\| \\leq \\epsilon\\) , or \\(\\left.f\\left(\\mathbf{x}^k\\right)-f\\left(\\mathbf{x}^*\\right) \\leq \\epsilon\\right)\\) . To this end, we study the oracle complexity of iterative algorithms to obtain approximate solutions to convex optimization problems. The number of flops can then be obtained as the number of oracle queries required to obtain an \\(\\epsilon\\) -accurate solution times the complexity of the oracle. A word of caution: Other properties than the oracle complexity can affect the runtime of algorithms in practice. For example, standard gradient descent has a worse convergence rate than the accelerated gradient method, but might be more sensitive to noise. Takeaway Convex Set \u8fde\u63a5set\u5185\u4efb\u610f\u4e24\u4e2a\u70b9, \u8fd9\u4e24\u4e2a\u70b9\u7684\u8fde\u7ebf\u4e0a\u4efb\u610f\u4e00\u70b9\u90fd\u5728set\u5185 A set C is convex if and only if it contains all convex combinations of its elements. Convex Combination for a set of points \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^n\\) , a convex combination of them is defined as the set \\[ \\left\\{\\mathbf{x}=\\sum_{i=1}^n \\theta_i \\mathbf{x}_i: \\sum_{i=1}^n \\theta_i=1, \\theta_i \\geq 0\\right\\} \\] Convex Hull of S For any given set S, the convex hull of S is defined as the set of all convex combinations of points in S. Intuitively, it is the smallest convex set that contains S Function A function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is convex if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) , and all \\(\\theta \\in(0,1)\\) , $$ \\theta f(\\mathbf{x})+(1-\\theta) f(\\mathbf{y}) \\geq f(\\theta \\mathbf{x}+(1-\\theta) \\mathbf{y}) . $$ A function is strictly convex, if the inequality above is strict whenever \\(\\mathbf{x} \\neq \\mathbf{y}\\) . A function \\(f\\) is concave if \\(-f\\) is convex. Differentiable Function A differentiable function \\(f\\) is convex if and only if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) , $$ f(\\mathbf{y}) \\geq f(\\mathbf{x})+\\langle\\mathbf{y}-\\mathbf{x}, \\nabla f(\\mathbf{x})\\rangle . $$","title":"Convex Optimization"},{"location":"Optimization/1%20Convexity/#convex-optimization","text":"","title":"Convex Optimization"},{"location":"Optimization/1%20Convexity/#optimization-problems","text":"Many problems in science and engineering can be formulated as an optimization problem: $$ \\text { minimize } f(\\mathbf{x}) \\quad \\text { subject to } \\quad \\mathbf{x} \\in \\mathcal{C} \\text {, } $$ where \\(f\\) is a cost function and \\(\\mathcal{C}\\) is a set. Due to \\(\\max _{\\mathbf{x} \\in \\mathcal{C}} f(\\mathbf{x})=\\min _{\\mathbf{x} \\in \\mathcal{C}}-f(\\mathbf{x})\\) , this formulation includes the problem of maximizing a cost function. The function \\(f\\) can model goodness of fit in machine learning (we'll see many examples later), utility, or cost. The set \\(\\mathcal{C}\\) can incorporate constraints such as a limited budget \\((\\|\\mathbf{x}\\| \\leq B)\\) or priors ( \\(\\mathbf{x}\\) is non-negative). We say that \\(\\mathbf{x}^*\\) is a (global) solution to the optimization problem (1) if \\(\\mathbf{x}^* \\in \\mathcal{C}\\) , and \\(f\\left(\\mathbf{x}^*\\right) \\leq f(\\mathbf{x})\\) for all \\(\\mathrm{x} \\in \\mathcal{C}\\) . We say that \\(\\mathrm{x}^*\\) is a local solution to the optimization problem if in a neighborhood \\(\\mathcal{N}\\) around \\(\\mathbf{x}^*, f\\left(\\mathbf{x}^*\\right) \\leq f(\\mathbf{x})\\) for all \\(\\mathbf{x} \\in \\mathcal{C} \\cap \\mathcal{N}\\) . Optimality can be very hard to check, even if \\(f\\) is differentiable and \\(\\mathcal{C}=\\mathbb{R}^n\\) .","title":"Optimization Problems"},{"location":"Optimization/1%20Convexity/#convexity","text":"An important class of optimization problems for which we can check optimality rather easily are convex optimization problems. A standard reference for convex optimization problems is the book [BV04]. Definition 1. A convex set \\(\\mathcal{C}\\) is any set such that for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathcal{C}\\) and all \\(\\theta \\in(0,1)\\) , $$ \\theta \\mathbf{x}+(1-\\theta) \\mathbf{y} \\in \\mathcal{C} $$ \u8fde\u63a5set\u5185\u4efb\u610f\u4e24\u4e2a\u70b9, \u8fd9\u4e24\u4e2a\u70b9\u7684\u8fde\u7ebf\u4e0a\u4efb\u610f\u4e00\u70b9\u90fd\u5728set\u5185 Figure 1 shows examples of convex and non-convex sets. Intersections of convex sets are convex. Important examples of convex sets are the following: Subspaces \\(\\left\\{\\mathbf{x}=\\mathbf{U a}: \\mathbf{a} \\in \\mathbb{R}^d\\right\\}\\) , Affines spaces \\(\\left\\{\\mathbf{x}=\\mathbf{U a}+\\mathbf{b}: \\mathbf{a} \\in \\mathbb{R}^d\\right\\}\\) Half-spaces \\(\\{\\mathbf{x}:\\langle\\mathbf{a}, \\mathbf{x}\\rangle \\leq b\\}\\) Definition 2. for a set of points \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^n\\) , a convex combination of them is defined as the set \\[ \\left\\{\\mathbf{x}=\\sum_{i=1}^n \\theta_i \\mathbf{x}_i: \\sum_{i=1}^n \\theta_i=1, \\theta_i \\geq 0\\right\\} \\] A set \\(\\mathcal{C}\\) is convex if and only if it contains all convex combinations of its elements. For any given set \\(\\mathcal{S}\\) , the convex hull of \\(\\mathcal{S}\\) is defined as the set of all convex combinations of points in \\(\\mathcal{S}\\) . Intuitively, it is the smallest convex set that contains \\(\\mathcal{S}\\) . As an example, consider the non-convex set \\(\\left\\{\\mathbf{x}:\\|\\mathbf{x}\\|_0 \\leq 1,\\|\\mathbf{x}\\|_1 \\leq 1\\right\\}\\) . Its convex hull is the \\(\\ell_1\\) -norm ball \\(\\left\\{\\mathbf{x}\\right.\\) : \\(\\left.\\|\\mathbf{x}\\|_1 \\leq 1\\right\\}\\) . We are now ready for one of the most important definitions in this class. Definition 3. A function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is convex if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) , and all \\(\\theta \\in(0,1)\\) , $$ \\theta f(\\mathbf{x})+(1-\\theta) f(\\mathbf{y}) \\geq f(\\theta \\mathbf{x}+(1-\\theta) \\mathbf{y}) . $$ A function is strictly convex, if the inequality above is strict whenever \\(\\mathbf{x} \\neq \\mathbf{y}\\) . A function \\(f\\) is concave if \\(-f\\) is convex. Common examples of convex functions are: Linear functions: \\(f(\\mathbf{x})=\\langle\\mathbf{a}, \\mathbf{x}\\rangle+b\\) , Quadratics: \\(f(\\mathbf{x})=\\frac{1}{2} \\mathbf{x}^T \\mathbf{Q} \\mathbf{x}+\\mathbf{b}^T \\mathbf{x}+c\\) , where \\(\\mathbf{Q}\\) is positive semidefinite. The function \\(f\\) is convex if and only if \\(\\mathbf{Q}\\) is positive semidefinite. Any norm \\(f(\\mathbf{x})=\\|\\mathbf{x}\\|\\) is convex. This follows from the triangle inequality and homogeneity/scalability. An important property of convex functions is that local minima are global. Proposition 1. Any local minimum of a convex function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is also a global minimum. We now turn to first order optimality conditions. To this end, we first state a proposition stating that a function is convex if and only if it is lower bounded by its first order Taylor expansion at any point. Proposition 2. A differentiable function \\(f\\) is convex if and only if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) , $$ f(\\mathbf{y}) \\geq f(\\mathbf{x})+\\langle\\mathbf{y}-\\mathbf{x}, \\nabla f(\\mathbf{x})\\rangle . $$ It is strictly convex if and only if the inequality holds strictly for all \\(\\mathbf{x} \\neq \\mathbf{y}\\) . An important consequence is the following optimality condition. Corollary 1. If a differentiable function \\(f\\) is convex and \\(\\nabla f\\left(\\mathbf{x}^*\\right)=0\\) , then \\(\\mathbf{x}^*\\) is a global minimizer of \\(f\\) .","title":"Convexity"},{"location":"Optimization/1%20Convexity/#computational-aspects-of-optimization-algorithms","text":"Suppose we want to minimize a differentiable function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) over \\(\\mathbb{R}^n\\) : $$ \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{\\operatorname{minimize}} f(\\mathbf{x}) . $$ For simplicity assume that \\(f(\\mathbf{x})=\\|\\mathbf{A} \\mathbf{x}-\\mathbf{y}\\|_2^2\\) , where \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) is a matrix with full column rank. So the optimization problem above is a standard least squares problem. If the columns are linearly independent, we can simply obtain a closed form solution as \\(\\mathbf{x}^*=\\left(\\mathbf{A}^T \\mathbf{A}\\right)^{-1} \\mathbf{A}^T \\mathbf{y}\\) . However, for many practically relevant functions \\(f\\) , we cannot compute a closed form solution. Even if we can, we might not want to because computing the closed form solution can be computationally expensive, e.g., for \\(\\mathbf{x}^*=\\left(\\mathbf{A}^T \\mathbf{A}\\right)^{-1} \\mathbf{A}^T \\mathbf{y}\\) we have to compute the inverse of a possibly large matrix. In the first part of this course we consider algorithms for finding approximate solutions of the minimization problem above. In practice we are essentially always content with an approximate solution (a computer can only give us a solution up to machine precision anyways). We assume that we have oracle access to \\(f\\) : A zeroth order oracle returns \\(f(\\mathbf{x})\\) for a given \\(\\mathbf{x}\\) . A first order oracle returns \\(\\{f(\\mathbf{x}), \\nabla f(\\mathbf{x})\\}\\) for a given \\(\\mathbf{x}\\) . A second order oracle returns \\(\\left\\{f(\\mathbf{x}), \\nabla f(\\mathbf{x}), \\nabla^2 f(\\mathbf{x})\\right\\}\\) for a given \\(\\mathbf{x}\\) . Ultimately, we are interested in understanding the computational complexity of convex optimization algorithms, i.e., the number of flops to obtain a \\(\\epsilon\\) -accurate solution (an \\(\\epsilon\\) -accurate solution can be a vector \\(\\mathbf{x}^k\\) obeying \\(\\left\\|\\mathbf{x}^k-\\mathbf{x}^*\\right\\| \\leq \\epsilon\\) , or \\(\\left.f\\left(\\mathbf{x}^k\\right)-f\\left(\\mathbf{x}^*\\right) \\leq \\epsilon\\right)\\) . To this end, we study the oracle complexity of iterative algorithms to obtain approximate solutions to convex optimization problems. The number of flops can then be obtained as the number of oracle queries required to obtain an \\(\\epsilon\\) -accurate solution times the complexity of the oracle. A word of caution: Other properties than the oracle complexity can affect the runtime of algorithms in practice. For example, standard gradient descent has a worse convergence rate than the accelerated gradient method, but might be more sensitive to noise.","title":"Computational aspects of optimization algorithms"},{"location":"Optimization/1%20Convexity/#takeaway","text":"Convex Set \u8fde\u63a5set\u5185\u4efb\u610f\u4e24\u4e2a\u70b9, \u8fd9\u4e24\u4e2a\u70b9\u7684\u8fde\u7ebf\u4e0a\u4efb\u610f\u4e00\u70b9\u90fd\u5728set\u5185 A set C is convex if and only if it contains all convex combinations of its elements. Convex Combination for a set of points \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n \\in \\mathbb{R}^n\\) , a convex combination of them is defined as the set \\[ \\left\\{\\mathbf{x}=\\sum_{i=1}^n \\theta_i \\mathbf{x}_i: \\sum_{i=1}^n \\theta_i=1, \\theta_i \\geq 0\\right\\} \\] Convex Hull of S For any given set S, the convex hull of S is defined as the set of all convex combinations of points in S. Intuitively, it is the smallest convex set that contains S Function A function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is convex if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) , and all \\(\\theta \\in(0,1)\\) , $$ \\theta f(\\mathbf{x})+(1-\\theta) f(\\mathbf{y}) \\geq f(\\theta \\mathbf{x}+(1-\\theta) \\mathbf{y}) . $$ A function is strictly convex, if the inequality above is strict whenever \\(\\mathbf{x} \\neq \\mathbf{y}\\) . A function \\(f\\) is concave if \\(-f\\) is convex. Differentiable Function A differentiable function \\(f\\) is convex if and only if for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) , $$ f(\\mathbf{y}) \\geq f(\\mathbf{x})+\\langle\\mathbf{y}-\\mathbf{x}, \\nabla f(\\mathbf{x})\\rangle . $$","title":"Takeaway"},{"location":"Optimization/2%20Gradient%20Descent/","text":"Gradient Descent Gradient descent is a simple iterative algorithm for minimizing a differentiable function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) on \\(\\mathbb{R}^n\\) . Starting from an initial point \\(\\mathbf{x}_0\\) , gradient descent iterates: $$ \\mathbf{x}^{k+1}=\\mathbf{x}^k-\\alpha_k \\nabla f\\left(\\mathbf{x}^k\\right), $$ where \\(\\alpha_k\\) is a step size parameter. Gradient descent converges to a local minimum, and provided that \\(f\\) is convex, it converges to a global minimum. The idea behind this algorithm is to make small steps in the direction that minimizes the local first order approximation of \\(f\\) . Convergence for quadratic functions In order to understand the gradient method better, let us start with a simple class of functions, namely quadratic functions: $$ f(\\mathbf{x})=\\frac{1}{2} \\mathbf{x}^T \\mathbf{Q} \\mathbf{x}-\\mathbf{b}^T \\mathbf{x}, $$ where \\(\\mathbf{Q}\\) is symmetric and strictly positive definite. A closed form solution to the minimization problem minimize \\(\\mathbf{f}(\\mathbf{x})\\) is \\(\\mathbf{x}^*=\\mathbf{Q}^{-1} \\mathbf{b}\\) . We want to understand what gradient descent yields for this problem. We consider gradient descent with a fixed stepsize: $$ \\mathbf{x}^{k+1}=\\mathbf{x}^k-\\alpha \\nabla f\\left(\\mathbf{x}^k\\right) . $$ Using that the gradient is given by \\(\\nabla f(\\mathbf{x})=\\mathbf{Q} \\mathbf{x}-\\mathbf{b}\\) and that the optimal solution obeys \\(\\mathbf{Q x}^*=\\mathbf{b}\\) , the difference of the \\((k+1)\\) -st iteration to the optimum is It follows that \\[ \\left\\|\\mathbf{x}^{k+1}-\\mathbf{x}^*\\right\\|_2 \\leq\\|\\mathbf{I}-\\alpha \\mathbf{Q}\\|\\left\\|\\mathbf{x}^k-\\mathbf{x}^*\\right\\|_2 \\] Since \\(\\mathbf{I}-\\alpha \\mathbf{Q}\\) is symmetric (the first equality below can be checked by taking the singular value decomposition of the matrix) $$ |\\mathbf{I}-\\alpha \\mathbf{Q}|=\\max \\left(\\lambda_{\\max }(\\mathbf{I}-\\alpha \\mathbf{Q}),-\\lambda_{\\min }(\\mathbf{I}-\\alpha \\mathbf{Q})\\right)=\\max (\\alpha M-1,1-\\alpha m), $$ where \\(M\\) and \\(m\\) are the largest and smallest singular values of the matrix \\(\\mathbf{Q}\\) . For the second equality, we used that, the eigenvalues of \\(\\mathbf{I}-\\alpha \\mathbf{Q}\\) and \\(\\mathbf{Q}\\) are related as \\(\\lambda_i(\\mathbf{I}-\\alpha \\mathbf{Q})=1-\\alpha \\lambda_i(\\mathbf{Q})^1\\) . The right hand side above is minimized by \\(\\alpha=\\frac{2}{M+m}\\) . For this choice, \\(\\|\\mathbf{I}-\\alpha \\mathbf{Q}\\|=\\frac{1-1 / \\kappa}{1+1 / \\kappa}<1\\) , where \\(\\kappa=M / m\\) is the condition number of the matrix \\(\\mathbf{Q}\\) . To summarize, we have proven the following proposition: References [BV04] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.","title":"Gradient Descent"},{"location":"Optimization/2%20Gradient%20Descent/#gradient-descent","text":"Gradient descent is a simple iterative algorithm for minimizing a differentiable function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) on \\(\\mathbb{R}^n\\) . Starting from an initial point \\(\\mathbf{x}_0\\) , gradient descent iterates: $$ \\mathbf{x}^{k+1}=\\mathbf{x}^k-\\alpha_k \\nabla f\\left(\\mathbf{x}^k\\right), $$ where \\(\\alpha_k\\) is a step size parameter. Gradient descent converges to a local minimum, and provided that \\(f\\) is convex, it converges to a global minimum. The idea behind this algorithm is to make small steps in the direction that minimizes the local first order approximation of \\(f\\) .","title":"Gradient Descent"},{"location":"Optimization/2%20Gradient%20Descent/#convergence-for-quadratic-functions","text":"In order to understand the gradient method better, let us start with a simple class of functions, namely quadratic functions: $$ f(\\mathbf{x})=\\frac{1}{2} \\mathbf{x}^T \\mathbf{Q} \\mathbf{x}-\\mathbf{b}^T \\mathbf{x}, $$ where \\(\\mathbf{Q}\\) is symmetric and strictly positive definite. A closed form solution to the minimization problem minimize \\(\\mathbf{f}(\\mathbf{x})\\) is \\(\\mathbf{x}^*=\\mathbf{Q}^{-1} \\mathbf{b}\\) . We want to understand what gradient descent yields for this problem. We consider gradient descent with a fixed stepsize: $$ \\mathbf{x}^{k+1}=\\mathbf{x}^k-\\alpha \\nabla f\\left(\\mathbf{x}^k\\right) . $$ Using that the gradient is given by \\(\\nabla f(\\mathbf{x})=\\mathbf{Q} \\mathbf{x}-\\mathbf{b}\\) and that the optimal solution obeys \\(\\mathbf{Q x}^*=\\mathbf{b}\\) , the difference of the \\((k+1)\\) -st iteration to the optimum is It follows that \\[ \\left\\|\\mathbf{x}^{k+1}-\\mathbf{x}^*\\right\\|_2 \\leq\\|\\mathbf{I}-\\alpha \\mathbf{Q}\\|\\left\\|\\mathbf{x}^k-\\mathbf{x}^*\\right\\|_2 \\] Since \\(\\mathbf{I}-\\alpha \\mathbf{Q}\\) is symmetric (the first equality below can be checked by taking the singular value decomposition of the matrix) $$ |\\mathbf{I}-\\alpha \\mathbf{Q}|=\\max \\left(\\lambda_{\\max }(\\mathbf{I}-\\alpha \\mathbf{Q}),-\\lambda_{\\min }(\\mathbf{I}-\\alpha \\mathbf{Q})\\right)=\\max (\\alpha M-1,1-\\alpha m), $$ where \\(M\\) and \\(m\\) are the largest and smallest singular values of the matrix \\(\\mathbf{Q}\\) . For the second equality, we used that, the eigenvalues of \\(\\mathbf{I}-\\alpha \\mathbf{Q}\\) and \\(\\mathbf{Q}\\) are related as \\(\\lambda_i(\\mathbf{I}-\\alpha \\mathbf{Q})=1-\\alpha \\lambda_i(\\mathbf{Q})^1\\) . The right hand side above is minimized by \\(\\alpha=\\frac{2}{M+m}\\) . For this choice, \\(\\|\\mathbf{I}-\\alpha \\mathbf{Q}\\|=\\frac{1-1 / \\kappa}{1+1 / \\kappa}<1\\) , where \\(\\kappa=M / m\\) is the condition number of the matrix \\(\\mathbf{Q}\\) . To summarize, we have proven the following proposition:","title":"Convergence for quadratic functions"},{"location":"Optimization/2%20Gradient%20Descent/#references","text":"[BV04] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.","title":"References"},{"location":"Supervised%20Learning/0%20Intro/0.1%20Bias%20Variance%20Trade-off/","text":"Bias-Variance tradeoff Our primary goal is finding a model that has high prediction performance on previously unseen examples. In this section, we discuss the bias-variance tradeoff for predicting \\(y\\) based on \\(\\mathbf{x}\\) and our learned model. To start, we ask the question: How can we measure the effectiveness of a learning algorithm? To address this question in the context of learning, let us again consider the regression model $$ y=h(\\mathbf{x})+z, $$ but now we assume that both the feature vector \\(\\mathbf{x}\\) and the noise \\(z\\) are drawn from some distribution. Then \\(y\\) is a random variable and its distribution is fully determined by \\(h\\) and the distributions of \\(z\\) and \\(\\mathbf{x}\\) . Stated differently, \\(y\\) and \\(\\mathbf{x}\\) are drawn from a joint distribution that is determined by the feature and noise distributions as well as \\(h\\) . In this context, the learning problem is as follows: We obtain a dataset \\(\\mathcal{D}=\\left\\{\\left(y_1, \\mathbf{x}_1\\right), \\ldots,\\left(y_n, \\mathbf{x}_n\\right)\\right\\}\\) by drawing \\(n\\) examples from the joint distribution. The learning algorithm takes the dataset and returns an estimate of the regression function \\(\\hat{h}\\) . As a concrete example of learning a regression function, we can compute the ridge regression estimate \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) which then yields the following estimate of the regression function: $$ \\hat{h}(\\mathbf{x})=\\left\\langle\\mathbf{x}, \\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right\\rangle . $$ Note that the \\(\\hat{h}\\) is a function of the data, \\(\\mathcal{D}\\) . It is natural to measure performance of the learned estimate \\(\\hat{h}\\) by measuring its performance on an new example, drawn from the joint distribution, and then averaging over many of such examples. Thus, we consider the expectation: $$ R(\\hat{h})=\\mathbb{E}_{\\mathbf{x}, y}\\left[(\\hat{h}(\\mathbf{x})-y)^2\\right] . $$ Here, expectation is with respect to the newly drawn sample. Recall that the estimate \\(\\hat{h}\\) is a function of the dataset, to make this dependency explicit, we write \\(\\hat{h}=\\hat{h}_{\\mathcal{D}}\\) in what follows. Since we are interested in the performance of a learning algorithm that selects \\(\\hat{h}\\) based on the randomly chosen dataset \\(\\mathcal{D}\\) , we consider the expectation over the dataset: This error corresponds to the performance of the learner's prediction averaged over examples, and averaged over many data sets \\(\\mathcal{D}\\) . With the relation \\(y=h(\\mathbf{x})+z\\) , we can decompose the error as follows: where the last equality follows from \\(z\\) being independent of \\(\\mathbf{x}\\) and \\(\\mathcal{D}\\) and having zero mean. We next decompose the first term further. Towards this end, we condition on \\(\\mathbf{x}\\) (think about \\(\\mathbf{x}\\) as a fixed vector, not a random variable for the calculations below). Then the first term above becomes (all expectations below are with respect to the random dataset \\(\\mathcal{D}\\) ): where the middle cross term is equal to zero. Thus we have \\[ \\mathbb{E}_{\\mathcal{D}}[R(\\hat{h})]=\\mathbb{E}_{\\mathbf{x}}\\left[\\left(h(\\mathbf{x})-\\mathbb{E}_{\\mathcal{D}}\\left[\\hat{h}_{\\mathcal{D}}(\\mathbf{x})\\right]\\right)^2\\right]+\\mathbb{E}_{\\mathcal{D}, \\mathbf{x}}\\left[\\left(\\hat{h}_{\\mathcal{D}}(\\mathbf{x})-\\mathbb{E}\\left[\\hat{h}_{\\mathcal{D}}(\\mathbf{x})\\right]\\right)^2\\right]+\\operatorname{Var}[z] . \\] This is called a bias-variance decomposition . The first term is the bias of the method. It measures how well the average hypothesis can estimate the true underlying function \\(h\\) . A low bias means that \\(\\hat{h}\\) accurately estimates the underlying hypothesis \\(h\\) . The second term is the variance of the method. The variance of the method measures the variance of the hypothesis over the training sets. The third term is the variance of \\(z\\) , which is the irreducible error. The irreducible error term is the error that we cannot control or eliminate.","title":"Bias-Variance tradeoff"},{"location":"Supervised%20Learning/0%20Intro/0.1%20Bias%20Variance%20Trade-off/#bias-variance-tradeoff","text":"Our primary goal is finding a model that has high prediction performance on previously unseen examples. In this section, we discuss the bias-variance tradeoff for predicting \\(y\\) based on \\(\\mathbf{x}\\) and our learned model. To start, we ask the question: How can we measure the effectiveness of a learning algorithm? To address this question in the context of learning, let us again consider the regression model $$ y=h(\\mathbf{x})+z, $$ but now we assume that both the feature vector \\(\\mathbf{x}\\) and the noise \\(z\\) are drawn from some distribution. Then \\(y\\) is a random variable and its distribution is fully determined by \\(h\\) and the distributions of \\(z\\) and \\(\\mathbf{x}\\) . Stated differently, \\(y\\) and \\(\\mathbf{x}\\) are drawn from a joint distribution that is determined by the feature and noise distributions as well as \\(h\\) . In this context, the learning problem is as follows: We obtain a dataset \\(\\mathcal{D}=\\left\\{\\left(y_1, \\mathbf{x}_1\\right), \\ldots,\\left(y_n, \\mathbf{x}_n\\right)\\right\\}\\) by drawing \\(n\\) examples from the joint distribution. The learning algorithm takes the dataset and returns an estimate of the regression function \\(\\hat{h}\\) . As a concrete example of learning a regression function, we can compute the ridge regression estimate \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) which then yields the following estimate of the regression function: $$ \\hat{h}(\\mathbf{x})=\\left\\langle\\mathbf{x}, \\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right\\rangle . $$ Note that the \\(\\hat{h}\\) is a function of the data, \\(\\mathcal{D}\\) . It is natural to measure performance of the learned estimate \\(\\hat{h}\\) by measuring its performance on an new example, drawn from the joint distribution, and then averaging over many of such examples. Thus, we consider the expectation: $$ R(\\hat{h})=\\mathbb{E}_{\\mathbf{x}, y}\\left[(\\hat{h}(\\mathbf{x})-y)^2\\right] . $$ Here, expectation is with respect to the newly drawn sample. Recall that the estimate \\(\\hat{h}\\) is a function of the dataset, to make this dependency explicit, we write \\(\\hat{h}=\\hat{h}_{\\mathcal{D}}\\) in what follows. Since we are interested in the performance of a learning algorithm that selects \\(\\hat{h}\\) based on the randomly chosen dataset \\(\\mathcal{D}\\) , we consider the expectation over the dataset: This error corresponds to the performance of the learner's prediction averaged over examples, and averaged over many data sets \\(\\mathcal{D}\\) . With the relation \\(y=h(\\mathbf{x})+z\\) , we can decompose the error as follows: where the last equality follows from \\(z\\) being independent of \\(\\mathbf{x}\\) and \\(\\mathcal{D}\\) and having zero mean. We next decompose the first term further. Towards this end, we condition on \\(\\mathbf{x}\\) (think about \\(\\mathbf{x}\\) as a fixed vector, not a random variable for the calculations below). Then the first term above becomes (all expectations below are with respect to the random dataset \\(\\mathcal{D}\\) ): where the middle cross term is equal to zero. Thus we have \\[ \\mathbb{E}_{\\mathcal{D}}[R(\\hat{h})]=\\mathbb{E}_{\\mathbf{x}}\\left[\\left(h(\\mathbf{x})-\\mathbb{E}_{\\mathcal{D}}\\left[\\hat{h}_{\\mathcal{D}}(\\mathbf{x})\\right]\\right)^2\\right]+\\mathbb{E}_{\\mathcal{D}, \\mathbf{x}}\\left[\\left(\\hat{h}_{\\mathcal{D}}(\\mathbf{x})-\\mathbb{E}\\left[\\hat{h}_{\\mathcal{D}}(\\mathbf{x})\\right]\\right)^2\\right]+\\operatorname{Var}[z] . \\] This is called a bias-variance decomposition . The first term is the bias of the method. It measures how well the average hypothesis can estimate the true underlying function \\(h\\) . A low bias means that \\(\\hat{h}\\) accurately estimates the underlying hypothesis \\(h\\) . The second term is the variance of the method. The variance of the method measures the variance of the hypothesis over the training sets. The third term is the variance of \\(z\\) , which is the irreducible error. The irreducible error term is the error that we cannot control or eliminate.","title":"Bias-Variance tradeoff"},{"location":"Supervised%20Learning/0%20Intro/0.2%20Model%20Selection%20and%20Validation/","text":"Model selection and validation In a previous lecture, we discussed the ridge regression estimate \\[ \\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}=\\arg \\min _{\\boldsymbol{\\theta}}\\|\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\theta}\\|_2^2+\\lambda\\|\\boldsymbol{\\theta}\\|_2^2, \\] and found that the regularization parameter \\(\\lambda\\) enables trading off the bias and variance of the estimator. Note that the estimate \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) is parameterized by \\(\\lambda\\) , and when computing \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) , we do not optimize over \\(\\lambda\\) . For this reason, it is called a hyperparamete r. Almost all machine learning method have hyperparameters. Besides obvious hyperparameters like the regularization parameter in ridge regression, the following can also be regarded as hyperparameters that we wish to choose well: Which model to choose, for example a linear model. For a fixed model, their parameters. For example, the number of layers of a deep neural network, its activations functions etc. Which features to include or exclude. The performance of our method often critically depends on the choice of the hyperparameters. This raises the question: How can we choose the hyperparameters in a principled manner, and assess the performance of the resulting model? The statistical learning setup In order to fit a model, we typically minimize a quantity that captures how well the model predicts past data, but we actually care about how well the model predicts unseen data. In order to asses how well a model predicts unseen data it is useful to consider the statistical learning setup. Suppose that examples \\((\\mathbf{x}, y)\\) are drawn i.i.d. from some (unknown) distribution. For example, this distribution could be determined by \\(y=h^*(\\mathbf{x})+z\\) , where \\(h^*\\) is a fixed regression function (say \\(h^*(\\mathbf{x})=\\left\\langle\\mathbf{x}, \\boldsymbol{\\theta}^*\\right\\rangle\\) , and \\(\\mathbf{x}\\) and \\(z\\) are drawn i.i.d. from Gaussian distributions. Given a function \\(h\\) , we measure the error between the observation \\(y\\) and the estimate \\(\\hat{y}=h(\\mathbf{x})\\) with a loss function \\[ \\text { loss: } \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R} \\] For example, \\(\\operatorname{loss}(z, y)=(z-y)^2\\) is the quadratic loss popular in regression. Consider a class of functions \\(\\mathcal{H}\\) . The (true or population) risk of a function \\(h \\in \\mathcal{H}\\) in this class is defined as \\[ R(h)=\\mathbb{E}[\\operatorname{loss}(h(\\mathbf{x}), y)], \\] where the expectation is over the distribution of the data. Ideally, we would like to find the function in our hypothesis class that minimizes the risk: \\[ h^*=\\arg \\min _{h \\in \\mathcal{H}} R(h) . \\] Unfortunately, it is impossible to compute the expectation because we don't know the joint distribution of \\((\\mathbf{x}, y)\\) . Training, validation and test errors We do, however, have access to a set of examples \\(\\mathcal{D}=\\left\\{\\left(\\mathbf{x}_1, y_1\\right), \\ldots,\\left(\\mathbf{x}_n, y_n\\right)\\right\\}\\) . Training and testing: First, suppose our goal is to train a model that does not have hyperparameters and then predict how well it performs on new examples. Towards this goal, we first split our dataset into two non-overlapping sets \\(\\mathcal{D}_{\\text {train }}\\) and \\(\\mathcal{D}_{\\text {test }}\\) called the training set and the test set. The estimate is called the training error. Here, \\(\\left|\\mathcal{D}_{\\text {train }}\\right|\\) stands for the cardinality of the set \\(\\left|\\mathcal{D}_{\\text {train }}\\right|\\) , i.e., the number of examples in the training set. Almost exclusively, we fit a model by minimizing the training error, or a regularized version thereof. For example, linear regression fits a model by minimizing the training error and ridge regression fits a model by minimizing the training error regularized with the penalty \\(\\lambda\\|\\boldsymbol{\\theta}\\|_2^2\\) . Note that the training error \\(\\hat{R}\\left(h, \\mathcal{D}_{\\text {train }}\\right)\\) can be viewed as an estimate of the true risk computed based on the training set. Given a model trained on the training set, we cannot use the training error to estimate the population risk \\(R(\\hat{h})\\) of a function \\(\\hat{h}\\) trained on the training set. The reason is that \\(\\hat{h}\\) is a function of \\(\\mathcal{D}_{\\text {train }}\\) . We can, however, use the test error to estimate the expected risk: \\[ \\hat{R}\\left(h, \\mathcal{D}_{\\text {test }}\\right)=\\frac{1}{\\left|\\mathcal{D}_{\\text {test }}\\right|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{D}_{\\text {test }}} \\operatorname{loss}(h(\\mathbf{x}), y) \\] The test error \\(\\hat{R}\\left(\\hat{h}, \\mathcal{D}_{\\text {test }}\\right)\\) is an unbiased estimate of the population risk, because \\(\\hat{h}\\) is independent of the test set. The question that remains is: How large does the test set need to be? It needs to be sufficiently large so that the test error is a good estimate of the population risk. More formally, under mild assumptions on the risk and estimator (specifically, that it is bounded), we have by Hoeffding's inequality for a set \\(\\mathcal{D}\\) with \\(n\\) i.i.d. examples for any \\(\\delta>0\\) that \\[ \\mathrm{P}\\left[|\\hat{R}(h, \\mathcal{D})-R(h)| \\leq O\\left(\\sqrt{\\frac{\\log (1 / \\delta)}{n}}\\right)\\right] \\geq 1-\\delta . \\] In words, the larger our test set, the better our estimate of the true risk. Moreover, doubling the size of the test set improves the estimate by a factor of \\(\\sqrt{1 / 2}\\) . The parameter \\(\\delta\\) is the probability that the estimate is far from its mean, think about this as a very small number, say \\(10^{-8}\\) . Training, model selection, and testing: Next, suppose we want to train a method with a number of hyperparameter configurations \\(i=1, \\ldots, K\\) . For example, we wish to train a ridge regression estimate for \\(K\\) different choices of the hyperparameter \\(\\lambda\\) , determine which one performs the best, and then determine the performance we expect (i.e., estimate the risk). Towards this goal, we split the set of examples into three disjoint sets: a training set \\(\\mathcal{D}_{\\text {train }}\\) , a validation set \\(\\mathcal{D}_{\\text {val }}\\) , and a test set \\(\\mathcal{D}_{\\text {test }}\\) . We then perform the following procedure: For all choices of hyperparameter configurations \\(i=1, \\ldots, K\\) : Train model \\(\\hat{\\boldsymbol{\\theta}}_i\\) on \\(\\mathcal{D}_{\\text {train }}\\) Compute validation error for model \\(\\hat{\\boldsymbol{\\theta}}_i\\) on \\(\\mathcal{D}_{\\text {val }}\\) Select model \\(\\hat{\\boldsymbol{\\theta}}_i\\) with the smallest validation error \\(\\hat{R}\\left(\\hat{\\boldsymbol{\\theta}}_i, \\mathcal{D}_{\\text {val }}\\right)\\) as the estimate for the best performing model Report the final performance as \\(\\hat{R}\\left(\\mathcal{D}_{\\text {test }}, \\hat{\\boldsymbol{\\theta}}_i\\right)\\) With this procedure, we do have to be mindful of the fact that we are re-using the validation set many times. This has to be taken into account when choosing the size of the validation set. This procedure can go wrong: If the validation set is too small, then just by chance a hyperparameter configuration works well. If the number of hyperparameter configurations is extremely large, specifically exponential in the size of the validation set, this will happen and would be an issue. k-fold cross validation Cross validation is a method for estimating the risk of a model trained on a training set based on a finite and potentially small pool of examples \\(\\mathcal{D}\\) . The idea is to repeatedly split the set \\(\\mathcal{D}\\) into test and training sets. The most popular variant of this idea is \\(k\\) -fold cross validation, which consists of the following steps: Shuffle the data and partition it into \\(k\\) equally sized or near equally sized subsets \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_k\\) . If the data is drawn i.i.d., shuffling would not be necessary, but remember that assuming the data is drawn i.i.d. is just a modeling assumption not necessarily true in practice. For each subset \\(i\\) , train the model on the union of all subsets but \\(\\mathcal{D}_i\\) , i.e., on \\(\\mathcal{D} \\backslash \\mathcal{D}_i\\) , and evaluate the model using block \\(i\\) by computing the validation error. This gives the \\(i\\) -th estimate of the risk. \u6bcf\u6b21\u9009\u62e9 \\(D_{i}\\) \u4f5c\u4e3a\u9a8c\u8bc1\u96c6 Obtain a final estimate of the risk as the average of the so-obtained \\(k\\) estimates. Using this methodology, the model is always evaluated on a different set of points than it is trained out. So how do we choose \\(k\\) ? Larger values of \\(k\\) provide a better estimate of the true error that we expect when training on a training set with \\(n\\) points, but also requires more computation. To see this suppose we set \\(k=n\\) , then we train on \\(n-1\\) points, but also need to fit our model \\(n\\) many times, which can be very costly. How to choose \\(k\\) in practice depends on how one want to balance this tradeoff; a common value for \\(k\\) in practice is 10 . Let's look at an example following Hastie et al. [HTF09], section 7.10.2. Consider a classification or regression problem with a large number of features. Suppose we perform the following steps: Select a number of relevant features by correlating them with the observations, and select the features that are highly correlated. Use only this subset of features, and train a classification or regression model, for example ridge regression. Use cross-validation to estimate the unknown hyperparameters (e.g., the regularization parameter of ridge regression) and to estimate the prediction error of the final model. Is this a correct application of cross-validation, and if not, what can go wrong?\uff1f Suppose there are \\(n=50\\) examples and originally there are 5000 features that are independent of the labels. Assume both class labels are equally likely, then the true error rate of any classifier is at least \\(50 \\%\\) . Hastie et al. [HTF09] carried out the above steps and the average cross-validation error was 3\\%, a significant underestimation of the error rate. What happened here is that just by chance, features are highly correlated with the data. The right way to apply cross validation in the example above is the following: Divide the examples into \\(k\\) equally sized sets at random. For each set \\(i\\) , using all examples but those in set \\(i\\) , i) find a set of good predictor that are strongly correlated with the labels, ii) use only this set of predictors to build a model, and iii) use the classifier to predict the class labels in fold \\(i\\) . Accumulate the error estimates from each fold to produce the cross-validation estimate of the prediction error. Bootstrap Suppose we are interested in associating a confidence interval, standard error, or variance estimate with our learning algorithms. As a concrete example, suppose we are interested in estimating the variance of a learning algorithm \\(\\hat{h}\\) for a given example \\(\\mathbf{x}\\) : \\[ \\text{Var}[\\hat{h}(\\mathbf{x})]=\\mathbb{E}_{\\mathcal{D}}\\left[\\left(\\hat{h}_{\\mathcal{D}}-\\mathbb{E}\\left[\\hat{h}_{\\mathcal{D}}(\\mathbf{x})\\right]\\right)^2\\right] . \\] Suppose, as before that the examples \\((y, \\mathbf{x})\\) in \\(\\mathcal{D}\\) are drawn from some joint distribution. In order to estimate this quantity, we can sample datasets \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_b\\) , each containing \\(n\\) examples drawn uniformly at random from the joint distribution, and apply the learning algorithm to those datasets. That yields hypothesis \\(\\hat{h}_1, \\ldots, \\hat{h}_b\\) . Based on those, we can estimate the mean at a point \\(\\mathbf{x}\\) as and the variance as $$ \\text{Var}[\\hat{h}(\\mathbf{x})]=\\frac{1}{b} \\sum_{i=1}^b\\left(h_{\\mathcal{D}_i}(\\mathbf{x})-\\hat{\\mu}(\\mathbf{x})\\right)^2 . $$ While statistically sound, this method for estimating the variance of the estimator is not practically, we would need to sample a large number of examples. The bootstrap, proposed by Efron in 1979 offers a more practical approach. The basic idea of bootstrapping is that in absence of any other information about the unknown distribution, all the information about the distribution we have is contained in the observed sample \\(\\mathcal{D}\\) . Hence resampling from \\(\\mathcal{D}\\) is a good guide to see what can be expected if we would resample from the original distribution. For the concrete example above, we would sample a dataset \\(\\mathcal{D}\\) from the original, joint distribution, and then resample \\(b\\) many datasets \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_b\\) , all containing \\(n\\) samples, by drawing \\(n\\) examples each uniformly at random with replacement from \\(\\mathcal{D}\\) . Then, we would estimate the variance as above. Efron \u5728 1979 \u5e74\u63d0\u51fa\u7684 bootstrap \u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u65b9\u6cd5\u3002 \u81ea\u4e3e\u7684\u57fa\u672c\u601d\u60f3\u662f\uff0c\u5728\u6ca1\u6709\u5173\u4e8e\u672a\u77e5\u5206\u5e03\u7684\u4efb\u4f55\u5176\u4ed6\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u6240\u62e5\u6709\u7684\u5173\u4e8e\u5206\u5e03\u7684\u6240\u6709\u4fe1\u606f\u90fd\u5305\u542b\u5728\u89c2\u5bdf\u5230\u7684\u6837\u672c \\(\\mathcal{D}\\) \u4e2d\u3002 \u56e0\u6b64\uff0c\u4ece \\(\\mathcal{D}\\) \u91cd\u65b0\u91c7\u6837\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u6307\u5357\uff0c\u53ef\u4ee5\u5f88\u597d\u5730\u4e86\u89e3\u5982\u679c\u6211\u4eec\u4ece\u539f\u59cb\u5206\u5e03\u91cd\u65b0\u91c7\u6837\u4f1a\u53d1\u751f\u4ec0\u4e48\u3002 \u5bf9\u4e8e\u4e0a\u9762\u7684\u5177\u4f53\u793a\u4f8b\uff0c\u6211\u4eec\u5c06\u4ece\u539f\u59cb\u8054\u5408\u5206\u5e03\u4e2d\u91c7\u6837\u6570\u636e\u96c6 \\(\\mathcal{D}\\) \uff0c\u7136\u540e\u901a\u8fc7\u4ece \\(\\mathcal{D}\\) \u4e2d\u968f\u673a\u5747\u5300\u62bd\u53d6 \\(n\\) \u4e2a\u6837\u672c(\u82e5\u91c7\u96c6\u5230\u76f8\u540c\u7684\u6837\u672c\u5219\u66ff\u6362)\uff08\u91cd\u590d \\(b\\) \u6b21\uff09 \u91cd\u65b0\u91c7\u6837 \\(b\\) \u591a\u4e2a\u6570\u636e\u96c6 \\(\\mathcal{D}_1,\\ldots,\\mathcal{D}_b\\) \uff0c\u6bcf\u4e2a\u5305\u542b \\(n\\) \u4e2a\u6837\u672c\uff0c\u3002 \u7136\u540e\uff0c\u6211\u4eec\u5c06\u5982\u4e0a\u6240\u8ff0\u4f30\u8ba1\u65b9\u5dee\u3002 Reading: Chapter 7 in [HTF09]. References [HTF09] T. J. Hastie, R. J. Tibshirani, and J. J. H. Friedman. The elements of statistical learning. Springer, 2009.","title":"Model selection and validation"},{"location":"Supervised%20Learning/0%20Intro/0.2%20Model%20Selection%20and%20Validation/#model-selection-and-validation","text":"In a previous lecture, we discussed the ridge regression estimate \\[ \\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}=\\arg \\min _{\\boldsymbol{\\theta}}\\|\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\theta}\\|_2^2+\\lambda\\|\\boldsymbol{\\theta}\\|_2^2, \\] and found that the regularization parameter \\(\\lambda\\) enables trading off the bias and variance of the estimator. Note that the estimate \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) is parameterized by \\(\\lambda\\) , and when computing \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) , we do not optimize over \\(\\lambda\\) . For this reason, it is called a hyperparamete r. Almost all machine learning method have hyperparameters. Besides obvious hyperparameters like the regularization parameter in ridge regression, the following can also be regarded as hyperparameters that we wish to choose well: Which model to choose, for example a linear model. For a fixed model, their parameters. For example, the number of layers of a deep neural network, its activations functions etc. Which features to include or exclude. The performance of our method often critically depends on the choice of the hyperparameters. This raises the question: How can we choose the hyperparameters in a principled manner, and assess the performance of the resulting model?","title":"Model selection and validation"},{"location":"Supervised%20Learning/0%20Intro/0.2%20Model%20Selection%20and%20Validation/#the-statistical-learning-setup","text":"In order to fit a model, we typically minimize a quantity that captures how well the model predicts past data, but we actually care about how well the model predicts unseen data. In order to asses how well a model predicts unseen data it is useful to consider the statistical learning setup. Suppose that examples \\((\\mathbf{x}, y)\\) are drawn i.i.d. from some (unknown) distribution. For example, this distribution could be determined by \\(y=h^*(\\mathbf{x})+z\\) , where \\(h^*\\) is a fixed regression function (say \\(h^*(\\mathbf{x})=\\left\\langle\\mathbf{x}, \\boldsymbol{\\theta}^*\\right\\rangle\\) , and \\(\\mathbf{x}\\) and \\(z\\) are drawn i.i.d. from Gaussian distributions. Given a function \\(h\\) , we measure the error between the observation \\(y\\) and the estimate \\(\\hat{y}=h(\\mathbf{x})\\) with a loss function \\[ \\text { loss: } \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R} \\] For example, \\(\\operatorname{loss}(z, y)=(z-y)^2\\) is the quadratic loss popular in regression. Consider a class of functions \\(\\mathcal{H}\\) . The (true or population) risk of a function \\(h \\in \\mathcal{H}\\) in this class is defined as \\[ R(h)=\\mathbb{E}[\\operatorname{loss}(h(\\mathbf{x}), y)], \\] where the expectation is over the distribution of the data. Ideally, we would like to find the function in our hypothesis class that minimizes the risk: \\[ h^*=\\arg \\min _{h \\in \\mathcal{H}} R(h) . \\] Unfortunately, it is impossible to compute the expectation because we don't know the joint distribution of \\((\\mathbf{x}, y)\\) .","title":"The statistical learning setup"},{"location":"Supervised%20Learning/0%20Intro/0.2%20Model%20Selection%20and%20Validation/#training-validation-and-test-errors","text":"We do, however, have access to a set of examples \\(\\mathcal{D}=\\left\\{\\left(\\mathbf{x}_1, y_1\\right), \\ldots,\\left(\\mathbf{x}_n, y_n\\right)\\right\\}\\) . Training and testing: First, suppose our goal is to train a model that does not have hyperparameters and then predict how well it performs on new examples. Towards this goal, we first split our dataset into two non-overlapping sets \\(\\mathcal{D}_{\\text {train }}\\) and \\(\\mathcal{D}_{\\text {test }}\\) called the training set and the test set. The estimate is called the training error. Here, \\(\\left|\\mathcal{D}_{\\text {train }}\\right|\\) stands for the cardinality of the set \\(\\left|\\mathcal{D}_{\\text {train }}\\right|\\) , i.e., the number of examples in the training set. Almost exclusively, we fit a model by minimizing the training error, or a regularized version thereof. For example, linear regression fits a model by minimizing the training error and ridge regression fits a model by minimizing the training error regularized with the penalty \\(\\lambda\\|\\boldsymbol{\\theta}\\|_2^2\\) . Note that the training error \\(\\hat{R}\\left(h, \\mathcal{D}_{\\text {train }}\\right)\\) can be viewed as an estimate of the true risk computed based on the training set. Given a model trained on the training set, we cannot use the training error to estimate the population risk \\(R(\\hat{h})\\) of a function \\(\\hat{h}\\) trained on the training set. The reason is that \\(\\hat{h}\\) is a function of \\(\\mathcal{D}_{\\text {train }}\\) . We can, however, use the test error to estimate the expected risk: \\[ \\hat{R}\\left(h, \\mathcal{D}_{\\text {test }}\\right)=\\frac{1}{\\left|\\mathcal{D}_{\\text {test }}\\right|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{D}_{\\text {test }}} \\operatorname{loss}(h(\\mathbf{x}), y) \\] The test error \\(\\hat{R}\\left(\\hat{h}, \\mathcal{D}_{\\text {test }}\\right)\\) is an unbiased estimate of the population risk, because \\(\\hat{h}\\) is independent of the test set. The question that remains is: How large does the test set need to be? It needs to be sufficiently large so that the test error is a good estimate of the population risk. More formally, under mild assumptions on the risk and estimator (specifically, that it is bounded), we have by Hoeffding's inequality for a set \\(\\mathcal{D}\\) with \\(n\\) i.i.d. examples for any \\(\\delta>0\\) that \\[ \\mathrm{P}\\left[|\\hat{R}(h, \\mathcal{D})-R(h)| \\leq O\\left(\\sqrt{\\frac{\\log (1 / \\delta)}{n}}\\right)\\right] \\geq 1-\\delta . \\] In words, the larger our test set, the better our estimate of the true risk. Moreover, doubling the size of the test set improves the estimate by a factor of \\(\\sqrt{1 / 2}\\) . The parameter \\(\\delta\\) is the probability that the estimate is far from its mean, think about this as a very small number, say \\(10^{-8}\\) . Training, model selection, and testing: Next, suppose we want to train a method with a number of hyperparameter configurations \\(i=1, \\ldots, K\\) . For example, we wish to train a ridge regression estimate for \\(K\\) different choices of the hyperparameter \\(\\lambda\\) , determine which one performs the best, and then determine the performance we expect (i.e., estimate the risk). Towards this goal, we split the set of examples into three disjoint sets: a training set \\(\\mathcal{D}_{\\text {train }}\\) , a validation set \\(\\mathcal{D}_{\\text {val }}\\) , and a test set \\(\\mathcal{D}_{\\text {test }}\\) . We then perform the following procedure: For all choices of hyperparameter configurations \\(i=1, \\ldots, K\\) : Train model \\(\\hat{\\boldsymbol{\\theta}}_i\\) on \\(\\mathcal{D}_{\\text {train }}\\) Compute validation error for model \\(\\hat{\\boldsymbol{\\theta}}_i\\) on \\(\\mathcal{D}_{\\text {val }}\\) Select model \\(\\hat{\\boldsymbol{\\theta}}_i\\) with the smallest validation error \\(\\hat{R}\\left(\\hat{\\boldsymbol{\\theta}}_i, \\mathcal{D}_{\\text {val }}\\right)\\) as the estimate for the best performing model Report the final performance as \\(\\hat{R}\\left(\\mathcal{D}_{\\text {test }}, \\hat{\\boldsymbol{\\theta}}_i\\right)\\) With this procedure, we do have to be mindful of the fact that we are re-using the validation set many times. This has to be taken into account when choosing the size of the validation set. This procedure can go wrong: If the validation set is too small, then just by chance a hyperparameter configuration works well. If the number of hyperparameter configurations is extremely large, specifically exponential in the size of the validation set, this will happen and would be an issue.","title":"Training, validation and test errors"},{"location":"Supervised%20Learning/0%20Intro/0.2%20Model%20Selection%20and%20Validation/#k-fold-cross-validation","text":"Cross validation is a method for estimating the risk of a model trained on a training set based on a finite and potentially small pool of examples \\(\\mathcal{D}\\) . The idea is to repeatedly split the set \\(\\mathcal{D}\\) into test and training sets. The most popular variant of this idea is \\(k\\) -fold cross validation, which consists of the following steps: Shuffle the data and partition it into \\(k\\) equally sized or near equally sized subsets \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_k\\) . If the data is drawn i.i.d., shuffling would not be necessary, but remember that assuming the data is drawn i.i.d. is just a modeling assumption not necessarily true in practice. For each subset \\(i\\) , train the model on the union of all subsets but \\(\\mathcal{D}_i\\) , i.e., on \\(\\mathcal{D} \\backslash \\mathcal{D}_i\\) , and evaluate the model using block \\(i\\) by computing the validation error. This gives the \\(i\\) -th estimate of the risk. \u6bcf\u6b21\u9009\u62e9 \\(D_{i}\\) \u4f5c\u4e3a\u9a8c\u8bc1\u96c6 Obtain a final estimate of the risk as the average of the so-obtained \\(k\\) estimates. Using this methodology, the model is always evaluated on a different set of points than it is trained out. So how do we choose \\(k\\) ? Larger values of \\(k\\) provide a better estimate of the true error that we expect when training on a training set with \\(n\\) points, but also requires more computation. To see this suppose we set \\(k=n\\) , then we train on \\(n-1\\) points, but also need to fit our model \\(n\\) many times, which can be very costly. How to choose \\(k\\) in practice depends on how one want to balance this tradeoff; a common value for \\(k\\) in practice is 10 . Let's look at an example following Hastie et al. [HTF09], section 7.10.2. Consider a classification or regression problem with a large number of features. Suppose we perform the following steps: Select a number of relevant features by correlating them with the observations, and select the features that are highly correlated. Use only this subset of features, and train a classification or regression model, for example ridge regression. Use cross-validation to estimate the unknown hyperparameters (e.g., the regularization parameter of ridge regression) and to estimate the prediction error of the final model. Is this a correct application of cross-validation, and if not, what can go wrong?\uff1f Suppose there are \\(n=50\\) examples and originally there are 5000 features that are independent of the labels. Assume both class labels are equally likely, then the true error rate of any classifier is at least \\(50 \\%\\) . Hastie et al. [HTF09] carried out the above steps and the average cross-validation error was 3\\%, a significant underestimation of the error rate. What happened here is that just by chance, features are highly correlated with the data. The right way to apply cross validation in the example above is the following: Divide the examples into \\(k\\) equally sized sets at random. For each set \\(i\\) , using all examples but those in set \\(i\\) , i) find a set of good predictor that are strongly correlated with the labels, ii) use only this set of predictors to build a model, and iii) use the classifier to predict the class labels in fold \\(i\\) . Accumulate the error estimates from each fold to produce the cross-validation estimate of the prediction error.","title":"k-fold cross validation"},{"location":"Supervised%20Learning/0%20Intro/0.2%20Model%20Selection%20and%20Validation/#bootstrap","text":"Suppose we are interested in associating a confidence interval, standard error, or variance estimate with our learning algorithms. As a concrete example, suppose we are interested in estimating the variance of a learning algorithm \\(\\hat{h}\\) for a given example \\(\\mathbf{x}\\) : \\[ \\text{Var}[\\hat{h}(\\mathbf{x})]=\\mathbb{E}_{\\mathcal{D}}\\left[\\left(\\hat{h}_{\\mathcal{D}}-\\mathbb{E}\\left[\\hat{h}_{\\mathcal{D}}(\\mathbf{x})\\right]\\right)^2\\right] . \\] Suppose, as before that the examples \\((y, \\mathbf{x})\\) in \\(\\mathcal{D}\\) are drawn from some joint distribution. In order to estimate this quantity, we can sample datasets \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_b\\) , each containing \\(n\\) examples drawn uniformly at random from the joint distribution, and apply the learning algorithm to those datasets. That yields hypothesis \\(\\hat{h}_1, \\ldots, \\hat{h}_b\\) . Based on those, we can estimate the mean at a point \\(\\mathbf{x}\\) as and the variance as $$ \\text{Var}[\\hat{h}(\\mathbf{x})]=\\frac{1}{b} \\sum_{i=1}^b\\left(h_{\\mathcal{D}_i}(\\mathbf{x})-\\hat{\\mu}(\\mathbf{x})\\right)^2 . $$ While statistically sound, this method for estimating the variance of the estimator is not practically, we would need to sample a large number of examples. The bootstrap, proposed by Efron in 1979 offers a more practical approach. The basic idea of bootstrapping is that in absence of any other information about the unknown distribution, all the information about the distribution we have is contained in the observed sample \\(\\mathcal{D}\\) . Hence resampling from \\(\\mathcal{D}\\) is a good guide to see what can be expected if we would resample from the original distribution. For the concrete example above, we would sample a dataset \\(\\mathcal{D}\\) from the original, joint distribution, and then resample \\(b\\) many datasets \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_b\\) , all containing \\(n\\) samples, by drawing \\(n\\) examples each uniformly at random with replacement from \\(\\mathcal{D}\\) . Then, we would estimate the variance as above. Efron \u5728 1979 \u5e74\u63d0\u51fa\u7684 bootstrap \u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u65b9\u6cd5\u3002 \u81ea\u4e3e\u7684\u57fa\u672c\u601d\u60f3\u662f\uff0c\u5728\u6ca1\u6709\u5173\u4e8e\u672a\u77e5\u5206\u5e03\u7684\u4efb\u4f55\u5176\u4ed6\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u6240\u62e5\u6709\u7684\u5173\u4e8e\u5206\u5e03\u7684\u6240\u6709\u4fe1\u606f\u90fd\u5305\u542b\u5728\u89c2\u5bdf\u5230\u7684\u6837\u672c \\(\\mathcal{D}\\) \u4e2d\u3002 \u56e0\u6b64\uff0c\u4ece \\(\\mathcal{D}\\) \u91cd\u65b0\u91c7\u6837\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u6307\u5357\uff0c\u53ef\u4ee5\u5f88\u597d\u5730\u4e86\u89e3\u5982\u679c\u6211\u4eec\u4ece\u539f\u59cb\u5206\u5e03\u91cd\u65b0\u91c7\u6837\u4f1a\u53d1\u751f\u4ec0\u4e48\u3002 \u5bf9\u4e8e\u4e0a\u9762\u7684\u5177\u4f53\u793a\u4f8b\uff0c\u6211\u4eec\u5c06\u4ece\u539f\u59cb\u8054\u5408\u5206\u5e03\u4e2d\u91c7\u6837\u6570\u636e\u96c6 \\(\\mathcal{D}\\) \uff0c\u7136\u540e\u901a\u8fc7\u4ece \\(\\mathcal{D}\\) \u4e2d\u968f\u673a\u5747\u5300\u62bd\u53d6 \\(n\\) \u4e2a\u6837\u672c(\u82e5\u91c7\u96c6\u5230\u76f8\u540c\u7684\u6837\u672c\u5219\u66ff\u6362)\uff08\u91cd\u590d \\(b\\) \u6b21\uff09 \u91cd\u65b0\u91c7\u6837 \\(b\\) \u591a\u4e2a\u6570\u636e\u96c6 \\(\\mathcal{D}_1,\\ldots,\\mathcal{D}_b\\) \uff0c\u6bcf\u4e2a\u5305\u542b \\(n\\) \u4e2a\u6837\u672c\uff0c\u3002 \u7136\u540e\uff0c\u6211\u4eec\u5c06\u5982\u4e0a\u6240\u8ff0\u4f30\u8ba1\u65b9\u5dee\u3002 Reading: Chapter 7 in [HTF09].","title":"Bootstrap"},{"location":"Supervised%20Learning/0%20Intro/0.2%20Model%20Selection%20and%20Validation/#references","text":"[HTF09] T. J. Hastie, R. J. Tibshirani, and J. J. H. Friedman. The elements of statistical learning. Springer, 2009.","title":"References"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/","text":"Linear Regression Linear regression Intro Dataset: \\(\\mathcal{D}=\\left\\{\\left(y_1, \\mathbf{x}_1\\right), \\ldots,\\left(y_n, \\mathbf{x}_n\\right)\\right\\}\\) Assume \\(y=h(\\mathbf{x})+z\\) Our ultimate goal is to predict the value \\(y\\) corresponding to a feature vector \\(\\mathbf{x}\\) that is not in the training set. Towards this goal, we \"learn\" the function from the dataset, which yields an estimate \\(\\hat{h}\\) of the function \\(h^*\\) , and then predict \\(y=\\hat{h}(\\mathbf{x})\\) . Since learning an arbitrary function is intractable and not a well posed problem, we assume that the function \\(h\\) lies in some hypothesis class \\(\\mathcal{H}\\) of allowable functions. Typically, we assume that the set of functions \\(\\mathcal{H}\\) is parameterized, meaning that there is some (parameter or weight) vector \\(\\boldsymbol{\\theta}\\) which determines a function in the class \\(\\mathcal{H}\\) . A concrete example is the class of all linear function, which is given as the set: suppose the function \\(h\\) is fully determined by the vector \\(\\boldsymbol{\\theta}\\) ; we write \\(h_{\\boldsymbol{\\theta}}\\) to make this dependence explicit. With the class of functions parameterized by a vector \\(\\boldsymbol{\\theta}\\) , the problem of learning \\(h\\) reduces to the problem of estimating the parameter \\(\\boldsymbol{\\theta}\\) . Learn the parameter We choose a cost function loss: \\(\\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}\\) that measures how well (for a fixed \\(\\boldsymbol{\\theta}\\) ) the predictions \\(h_{\\boldsymbol{\\theta}}(\\mathbf{x})\\) describe the output \\(y\\) , then find the parameter that minimizes that loss: $$ \\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\sum_{i=1}^n \\operatorname{loss}\\left(h_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_i\\right), y_i\\right) $$ Here, and in the following we use the hat-notation \\(\\hat{\\boldsymbol{\\theta}}\\) to indicate that this variable is estimated based on data. Loss function The most popular metric in linear regression is the sum of the squares of the fitting error: ( least square cost function/MSE ) $$ \\hat{R}(\\boldsymbol{\\theta})=\\frac{1}{n} \\sum_{i=1}^n\\left(\\left\\langle\\mathbf{x}_i, \\boldsymbol{\\theta}\\right\\rangle-y_i\\right)^2 $$ $$ =\\frac{1}{n}|\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\theta}|_2^2 $$ The least squares cost function is convenient for a variety of reasons: - it can be numerically minimized efficiently as it is convex (convexity is defined below) - it has intuitive geometric and probabilistic interpretations, as we will see later. Least squares estimate: the parameter vector that minimizes this cost function: $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\hat{R}(\\boldsymbol{\\theta}) $$ \u8ba8\u8bba\u7ebf\u6027\u56de\u5f52\u4e2dn\u548cd\u7684\u5173\u7cfb Before discussing how \\(\\hat{\\theta}_{\\mathrm{LS}}\\) can be computed, let us comment on the relation of the number of examples, \\(n\\) , and the number of features, \\(d\\) . Suppose that at least \\(n\\) columns of \\(\\mathbf{X}\\) are linearly independent. Then we must have \\(n \\leq d\\) , and regardless of \\(\\mathbf{y}\\) , we can find a parameter vector \\(\\boldsymbol{\\theta}\\) (or many different ones) such that \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) , and thus describe the independent variable \\(y\\) exactly. - In the linear regression model, this is only possible if there are fewer examples than number of model parameters \\((n \\leq d)\\) . In that regime, even if \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) have nothing to do with each other, we achieve a perfect fit or zero loss. In this regime, we are likely overfitting: the model is too flexible given the data. Thus, fitting a linear model without additional assumptions only makes sense in the so called over-determined or under-parameterized regime, where the number of training examples is larger than the number of features \\((n \\geq d)\\) . If we find a good approximation in the \\((n \\geq d)\\) -regime, then this is an indication that the linear model is capturing structure of the problem. n<=d \u4e0d\u8bbax\u548cy\u7684\u5173\u7cfb\uff0c \u90fd\u80fd\u627e\u5230\u4e00\u4e2a \\(\\theta\\) , \u4f7f\u5f97 \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) overfitting not make senses n>=d: \u5982\u679c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u627e\u5230\u4e86\u4e00\u4e2a\u597d\u7684\u8fd1\u4f3c\u6a21\u578b\uff0c \u5219\u8be5\u6a21\u578b\u6bd4\u8f83\u51c6\u786e Least squares estimation We next show that \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) can be computed in closed form. Assume that \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) has full column rank. - This implies that the number or training examples, \\(n\\) , is larger than the number of model parameters, \\(d\\) . - features of \\(x\\) are linearly independent In the regime \\(n \\geq d\\) , if \\(\\mathbf{X}\\) does not have full column rank, then the features are linearly dependent and thus redundant. Proposition 1. Suppose that \\(\\mathbf{X}\\) has full column rank \\(d\\) . Then for any \\(\\mathbf{y}\\) , we have $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y} $$ There are at least two proofs of this proposition. Analysis of the least squares estimate and in what sense co-linear features are \"bad\" Thus far, we have not commented on the features of the regression model. Intuitively, if the features are very similar to each other, least squares estimation should be difficult. For example, suppose all the features (i.e., the entries of the \\(\\mathbf{x}_i\\) ) are linear functions of each other. Then the columns of \\(\\mathbf{X}\\) are linearly dependent and \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) is not unique. In statistics it is common to refer to features with this property as co-linear . A set of variables is said to be perfectly multi-colinear if there is one or more exact linear relationship among some of the variables \\(x_{i 1}, \\ldots, x_{i d}\\) ; for example the first two variables are co-linear if \\(x_{i 1}=-2 x_{i 2}\\) for all \\(i\\) . In practice, features are usually not co-linear but can be highly correlated, which leads to instable least squares estimates. Case1. \\(X\\) has full column rank and \\(y\\) is generated according to a linear model with additive Gaussian noise: $$ \\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}^*+\\mathbf{z} $$ Here, the entries of \\(\\mathbf{z}\\) are independent zero-mean Gaussian noise with variance \\(1 / n\\) , Then: the expected error is 0, or in statistical terms, the LSE is unbiased Case2. Features of \\(X\\) are strongly correlated where \\(\\sigma_i\\) are the singular values of \\(\\mathbf{X}\\) , and expectation is over the Gaussian noise \\(\\mathbf{z}\\) then \\(1 / \\sigma_{\\min }^2\\) is large and the bound becomes large. This is the case if some of the features are highly correlated, known as multicollinearity in the statistics literature. On the other hand, as expected, the error decays in \\(n\\) , the larger \\(n\\) , the smaller the error. proof of the error function:","title":"Linear Regression"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#linear-regression","text":"","title":"Linear Regression"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#linear-regression_1","text":"","title":"Linear regression"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#intro","text":"Dataset: \\(\\mathcal{D}=\\left\\{\\left(y_1, \\mathbf{x}_1\\right), \\ldots,\\left(y_n, \\mathbf{x}_n\\right)\\right\\}\\) Assume \\(y=h(\\mathbf{x})+z\\) Our ultimate goal is to predict the value \\(y\\) corresponding to a feature vector \\(\\mathbf{x}\\) that is not in the training set. Towards this goal, we \"learn\" the function from the dataset, which yields an estimate \\(\\hat{h}\\) of the function \\(h^*\\) , and then predict \\(y=\\hat{h}(\\mathbf{x})\\) . Since learning an arbitrary function is intractable and not a well posed problem, we assume that the function \\(h\\) lies in some hypothesis class \\(\\mathcal{H}\\) of allowable functions. Typically, we assume that the set of functions \\(\\mathcal{H}\\) is parameterized, meaning that there is some (parameter or weight) vector \\(\\boldsymbol{\\theta}\\) which determines a function in the class \\(\\mathcal{H}\\) . A concrete example is the class of all linear function, which is given as the set: suppose the function \\(h\\) is fully determined by the vector \\(\\boldsymbol{\\theta}\\) ; we write \\(h_{\\boldsymbol{\\theta}}\\) to make this dependence explicit. With the class of functions parameterized by a vector \\(\\boldsymbol{\\theta}\\) , the problem of learning \\(h\\) reduces to the problem of estimating the parameter \\(\\boldsymbol{\\theta}\\) .","title":"Intro"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#learn-the-parameter","text":"We choose a cost function loss: \\(\\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}\\) that measures how well (for a fixed \\(\\boldsymbol{\\theta}\\) ) the predictions \\(h_{\\boldsymbol{\\theta}}(\\mathbf{x})\\) describe the output \\(y\\) , then find the parameter that minimizes that loss: $$ \\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\sum_{i=1}^n \\operatorname{loss}\\left(h_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_i\\right), y_i\\right) $$ Here, and in the following we use the hat-notation \\(\\hat{\\boldsymbol{\\theta}}\\) to indicate that this variable is estimated based on data.","title":"Learn the parameter"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#loss-function","text":"The most popular metric in linear regression is the sum of the squares of the fitting error: ( least square cost function/MSE ) $$ \\hat{R}(\\boldsymbol{\\theta})=\\frac{1}{n} \\sum_{i=1}^n\\left(\\left\\langle\\mathbf{x}_i, \\boldsymbol{\\theta}\\right\\rangle-y_i\\right)^2 $$ $$ =\\frac{1}{n}|\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\theta}|_2^2 $$ The least squares cost function is convenient for a variety of reasons: - it can be numerically minimized efficiently as it is convex (convexity is defined below) - it has intuitive geometric and probabilistic interpretations, as we will see later. Least squares estimate: the parameter vector that minimizes this cost function: $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\hat{R}(\\boldsymbol{\\theta}) $$","title":"Loss function"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#nd","text":"Before discussing how \\(\\hat{\\theta}_{\\mathrm{LS}}\\) can be computed, let us comment on the relation of the number of examples, \\(n\\) , and the number of features, \\(d\\) . Suppose that at least \\(n\\) columns of \\(\\mathbf{X}\\) are linearly independent. Then we must have \\(n \\leq d\\) , and regardless of \\(\\mathbf{y}\\) , we can find a parameter vector \\(\\boldsymbol{\\theta}\\) (or many different ones) such that \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) , and thus describe the independent variable \\(y\\) exactly. - In the linear regression model, this is only possible if there are fewer examples than number of model parameters \\((n \\leq d)\\) . In that regime, even if \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) have nothing to do with each other, we achieve a perfect fit or zero loss. In this regime, we are likely overfitting: the model is too flexible given the data. Thus, fitting a linear model without additional assumptions only makes sense in the so called over-determined or under-parameterized regime, where the number of training examples is larger than the number of features \\((n \\geq d)\\) . If we find a good approximation in the \\((n \\geq d)\\) -regime, then this is an indication that the linear model is capturing structure of the problem. n<=d \u4e0d\u8bbax\u548cy\u7684\u5173\u7cfb\uff0c \u90fd\u80fd\u627e\u5230\u4e00\u4e2a \\(\\theta\\) , \u4f7f\u5f97 \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) overfitting not make senses n>=d: \u5982\u679c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u627e\u5230\u4e86\u4e00\u4e2a\u597d\u7684\u8fd1\u4f3c\u6a21\u578b\uff0c \u5219\u8be5\u6a21\u578b\u6bd4\u8f83\u51c6\u786e","title":"\u8ba8\u8bba\u7ebf\u6027\u56de\u5f52\u4e2dn\u548cd\u7684\u5173\u7cfb"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#least-squares-estimation","text":"We next show that \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) can be computed in closed form. Assume that \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) has full column rank. - This implies that the number or training examples, \\(n\\) , is larger than the number of model parameters, \\(d\\) . - features of \\(x\\) are linearly independent In the regime \\(n \\geq d\\) , if \\(\\mathbf{X}\\) does not have full column rank, then the features are linearly dependent and thus redundant. Proposition 1. Suppose that \\(\\mathbf{X}\\) has full column rank \\(d\\) . Then for any \\(\\mathbf{y}\\) , we have $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y} $$ There are at least two proofs of this proposition.","title":"Least squares estimation"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#analysis-of-the-least-squares-estimate-and-in-what-sense-co-linear-features-are-bad","text":"Thus far, we have not commented on the features of the regression model. Intuitively, if the features are very similar to each other, least squares estimation should be difficult. For example, suppose all the features (i.e., the entries of the \\(\\mathbf{x}_i\\) ) are linear functions of each other. Then the columns of \\(\\mathbf{X}\\) are linearly dependent and \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) is not unique. In statistics it is common to refer to features with this property as co-linear . A set of variables is said to be perfectly multi-colinear if there is one or more exact linear relationship among some of the variables \\(x_{i 1}, \\ldots, x_{i d}\\) ; for example the first two variables are co-linear if \\(x_{i 1}=-2 x_{i 2}\\) for all \\(i\\) . In practice, features are usually not co-linear but can be highly correlated, which leads to instable least squares estimates. Case1. \\(X\\) has full column rank and \\(y\\) is generated according to a linear model with additive Gaussian noise: $$ \\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}^*+\\mathbf{z} $$ Here, the entries of \\(\\mathbf{z}\\) are independent zero-mean Gaussian noise with variance \\(1 / n\\) , Then: the expected error is 0, or in statistical terms, the LSE is unbiased Case2. Features of \\(X\\) are strongly correlated where \\(\\sigma_i\\) are the singular values of \\(\\mathbf{X}\\) , and expectation is over the Gaussian noise \\(\\mathbf{z}\\) then \\(1 / \\sigma_{\\min }^2\\) is large and the bound becomes large. This is the case if some of the features are highly correlated, known as multicollinearity in the statistics literature. On the other hand, as expected, the error decays in \\(n\\) , the larger \\(n\\) , the smaller the error. proof of the error function:","title":"Analysis of the least squares estimate and in what sense co-linear features are \"bad\""},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/","text":"Ridge Regression \u5cad\u56de\u5f52 Motivation \u4e0a\u4e00\u8282\u8bb2\u7684\u7ebf\u6027\u56de\u5f52\u4e2d\uff0cexpected error: \u5982\u679c Features of \\(X\\) are (strongly) correlated specifically if its smallest singular value, \\(\\sigma_{\\min }^2(\\mathbf{X})\\) , is close to zero. In this case, \\(1 / \\sigma_{\\min }^2(\\mathbf{X})\\) is large, and therefore the expected error above is large. solution: ridge regression Ridge regression Ridge regression: \u6700\u5c0f\u4e8c\u4e58\u6cd5 + l2 norm regularization This additional term promotes solutions that yield a good fit while at the same time enforcing the coefficients of \\(\\theta\\) to be sufficiently small. Also known as: ridge regression (statistics) Tkhonov regularization/weight decay (machine learning) In order to avoid this, we can add a term penalizing the norm of the coefficient vector \\(\\boldsymbol{\\theta}\\) . This additional term promotes solutions that yield a good fit while at the same time enforcing the coefficients of \\(\\theta\\) to be sufficiently small. Incorporating assumptions about the solution-in this case that the coefficients should not be too large - is called regularization. A least squares penalty combined with \\(\\ell2\\) -norm regularization is called ridge regression in statistics and is also known as Tikhonov regularization and weight decay in machine learning. The ridge regression estimate is: Lambda parameter \\(\\lambda\\) is a fixed regularization parameter. different values of \\(\\lambda\\) give us different ridge regression estimates. Observe that for \\(\\lambda \\rightarrow 0\\) , the ridge regression estimate converges to the least squares estimate, and for \\(\\lambda \\rightarrow \\infty\\) , the ridge regression estimate converges to zero. Ridge regression estimate If \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) has full column rank \\(d\\) , the ridge regression estimate has a closed-form solution: Equation 5. proof: Recall that \\(\\mathbf{X} \\hat{\\theta}_{\\mathrm{LS}}\\) is the orthogonal projection of the data onto the column space of the feature matrix, i.e., \\(\\mathbf{X} \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\mathbf{U U}^T \\mathbf{y}\\) , where \\(\\mathbf{U} \\in \\mathbb{R}^{n \\times d}\\) is an orthogonal basis for the column space of \\(\\mathbf{X}\\) . The approximation \\(\\mathbf{X} \\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) can no longer be interpreted as the orthogonal projection of the data onto the column space of the feature matrix. However, it can be interpreted as a modified projection, where each component of the data in the direction of the left singular vector \\(\\mathbf{u}_i\\) of the feature matrix is shrunk by a factor of \\(\\sigma_i^2 /\\left(\\sigma_i^2+\\lambda\\right)\\) . Here, \\(\\sigma_i\\) is the singular value of the feature matrix corresponding to the singular vector \\(\\mathbf{u}_i\\) . In more detail, consider the singular value decomposition \\(\\mathbf{X}=\\mathbf{U} \\Sigma \\mathbf{V}^T\\) , and let \\(\\mathbf{u}_i\\) be the \\(i\\) -th column of \\(\\mathbf{U}\\) , i.e., \\(\\mathbf{U}=\\left[\\mathbf{u}_1, \\ldots, \\mathbf{u}_d\\right]\\) . Then we have equation 5 . where we used that for two square matrixes that are invertible, we have that \\((\\mathbf{A B})^{-1}=\\mathbf{B}^{-1} \\mathbf{A}^{-1}\\) . Next, note that \\(\\left(\\Sigma^2+\\lambda \\mathbf{I}\\right)\\) is a diagonal matrix with \\(\\sigma_i^2+\\lambda, i=1, \\ldots, d\\) on its diagonal, thus \\(\\left(\\Sigma^2+\\lambda \\mathbf{I}\\right)^{-1}\\) is a diagonal matrix with \\(1 /\\left(\\sigma_i^2+\\lambda\\right), i=1, \\ldots, d\\) on its diagonal. As a consequence, This concludes the proof of equation (5) Bias-variance tradeoff of the ridge regression estimate Assumption: the data indeed follows a linear model, i.e., \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}^*+\\mathbf{z}\\) the ride regression estimator can be decomposed into a signal and a noise term: Analysis suppose that the noise \\(\\mathbf{z}\\) is a zero-mean random variable. Then the expectation of \\(\\boldsymbol{\\theta}_{\\text {ridge }}\\) equals the first term \\(\\hat{\\theta}_{\\text {ridge }}^{\\text {signal }}\\) which is deterministic. The second term, \\(\\hat{\\theta}_{\\text {ridge }}^{\\text {noise }}\\) , is random and determines the variance of the ridge estimator. If \\(\\lambda=0\\) , the first term becomes \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}^{\\text {signal }}=\\boldsymbol{\\theta}^*\\) , otherwise the first term is not equal to \\(\\boldsymbol{\\theta}^*\\) . Thus, ridge regression is a biased estimator (for \\(\\lambda>0\\) ). Increasing \\(\\lambda\\) moves the mean of the estimate \\(\\hat{\\theta}_{\\text {ridge }}\\) farther from the true model parameter \\(\\theta^*\\) , and thus increases the bias of the estimate, which is defined as \\(\\mathbb{E}\\left[\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right]-\\boldsymbol{\\theta}^*=\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}^{\\text {signal }}-\\boldsymbol{\\theta}^*\\) \\(\\lambda\\) \u8d8a\u5927\uff0c \\(\\hat{\\theta}_{\\text{ridge}}\\) \u7684\u7edd\u5bf9\u503c\u8d8a\u5c0f --> 0 But in exchange, increasing \\(\\lambda\\) also shrinks the impact of the noise. Thus, choosing \\(\\lambda\\) appropriately enables us to adapt to the conditioning of the feature matrix and to the noise level for achieving a good tradeoff between the bias and the variance. \u56e0\u6b64\uff0c\u9009\u62e9\u9002\u5f53\u7684\u03bb\u4f7f\u6211\u4eec\u80fd\u591f\u9002\u5e94\u7279\u5f81\u77e9\u9635\u7684\u6761\u4ef6\u548c\u566a\u58f0\u6c34\u5e73\uff0c\u4ee5\u5728\u504f\u5dee\u548c\u65b9\u5dee\u4e4b\u95f4\u5b9e\u73b0\u826f\u597d\u7684\u6298\u8877\u3002 Note that the optimal choice depends on the noise which is unknown to us, the conditioning of the feature matrix, thus we cannot simply compute the optimal choice based on the data. Later in the course we will discuss methods that enable us to determine a good choice of \\(\\lambda\\) based on the data. Compared with linear regression where normalized MSE = \\(\\left\\|\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right\\|_2^2 /\\|\\boldsymbol{\\theta}\\|_2^2\\) \u53ef\u4ee5\u770b\u5230\u4f7f\u5f97loss\u6700\u5c0f\u7684 \\(\\lambda \\neq 0\\)","title":"Ridge Regression \u5cad\u56de\u5f52"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#ridge-regression","text":"","title":"Ridge Regression \u5cad\u56de\u5f52"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#motivation","text":"\u4e0a\u4e00\u8282\u8bb2\u7684\u7ebf\u6027\u56de\u5f52\u4e2d\uff0cexpected error: \u5982\u679c Features of \\(X\\) are (strongly) correlated specifically if its smallest singular value, \\(\\sigma_{\\min }^2(\\mathbf{X})\\) , is close to zero. In this case, \\(1 / \\sigma_{\\min }^2(\\mathbf{X})\\) is large, and therefore the expected error above is large. solution: ridge regression","title":"Motivation"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#ridge-regression_1","text":"Ridge regression: \u6700\u5c0f\u4e8c\u4e58\u6cd5 + l2 norm regularization This additional term promotes solutions that yield a good fit while at the same time enforcing the coefficients of \\(\\theta\\) to be sufficiently small. Also known as: ridge regression (statistics) Tkhonov regularization/weight decay (machine learning) In order to avoid this, we can add a term penalizing the norm of the coefficient vector \\(\\boldsymbol{\\theta}\\) . This additional term promotes solutions that yield a good fit while at the same time enforcing the coefficients of \\(\\theta\\) to be sufficiently small. Incorporating assumptions about the solution-in this case that the coefficients should not be too large - is called regularization. A least squares penalty combined with \\(\\ell2\\) -norm regularization is called ridge regression in statistics and is also known as Tikhonov regularization and weight decay in machine learning. The ridge regression estimate is:","title":"Ridge regression"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#lambda-parameter","text":"\\(\\lambda\\) is a fixed regularization parameter. different values of \\(\\lambda\\) give us different ridge regression estimates. Observe that for \\(\\lambda \\rightarrow 0\\) , the ridge regression estimate converges to the least squares estimate, and for \\(\\lambda \\rightarrow \\infty\\) , the ridge regression estimate converges to zero.","title":"Lambda parameter"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#ridge-regression-estimate","text":"If \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) has full column rank \\(d\\) , the ridge regression estimate has a closed-form solution: Equation 5. proof: Recall that \\(\\mathbf{X} \\hat{\\theta}_{\\mathrm{LS}}\\) is the orthogonal projection of the data onto the column space of the feature matrix, i.e., \\(\\mathbf{X} \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\mathbf{U U}^T \\mathbf{y}\\) , where \\(\\mathbf{U} \\in \\mathbb{R}^{n \\times d}\\) is an orthogonal basis for the column space of \\(\\mathbf{X}\\) . The approximation \\(\\mathbf{X} \\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) can no longer be interpreted as the orthogonal projection of the data onto the column space of the feature matrix. However, it can be interpreted as a modified projection, where each component of the data in the direction of the left singular vector \\(\\mathbf{u}_i\\) of the feature matrix is shrunk by a factor of \\(\\sigma_i^2 /\\left(\\sigma_i^2+\\lambda\\right)\\) . Here, \\(\\sigma_i\\) is the singular value of the feature matrix corresponding to the singular vector \\(\\mathbf{u}_i\\) . In more detail, consider the singular value decomposition \\(\\mathbf{X}=\\mathbf{U} \\Sigma \\mathbf{V}^T\\) , and let \\(\\mathbf{u}_i\\) be the \\(i\\) -th column of \\(\\mathbf{U}\\) , i.e., \\(\\mathbf{U}=\\left[\\mathbf{u}_1, \\ldots, \\mathbf{u}_d\\right]\\) . Then we have equation 5 . where we used that for two square matrixes that are invertible, we have that \\((\\mathbf{A B})^{-1}=\\mathbf{B}^{-1} \\mathbf{A}^{-1}\\) . Next, note that \\(\\left(\\Sigma^2+\\lambda \\mathbf{I}\\right)\\) is a diagonal matrix with \\(\\sigma_i^2+\\lambda, i=1, \\ldots, d\\) on its diagonal, thus \\(\\left(\\Sigma^2+\\lambda \\mathbf{I}\\right)^{-1}\\) is a diagonal matrix with \\(1 /\\left(\\sigma_i^2+\\lambda\\right), i=1, \\ldots, d\\) on its diagonal. As a consequence, This concludes the proof of equation (5)","title":"Ridge regression estimate"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#bias-variance-tradeoff-of-the-ridge-regression-estimate","text":"Assumption: the data indeed follows a linear model, i.e., \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}^*+\\mathbf{z}\\) the ride regression estimator can be decomposed into a signal and a noise term:","title":"Bias-variance tradeoff of the ridge regression estimate"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#analysis","text":"suppose that the noise \\(\\mathbf{z}\\) is a zero-mean random variable. Then the expectation of \\(\\boldsymbol{\\theta}_{\\text {ridge }}\\) equals the first term \\(\\hat{\\theta}_{\\text {ridge }}^{\\text {signal }}\\) which is deterministic. The second term, \\(\\hat{\\theta}_{\\text {ridge }}^{\\text {noise }}\\) , is random and determines the variance of the ridge estimator. If \\(\\lambda=0\\) , the first term becomes \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}^{\\text {signal }}=\\boldsymbol{\\theta}^*\\) , otherwise the first term is not equal to \\(\\boldsymbol{\\theta}^*\\) . Thus, ridge regression is a biased estimator (for \\(\\lambda>0\\) ). Increasing \\(\\lambda\\) moves the mean of the estimate \\(\\hat{\\theta}_{\\text {ridge }}\\) farther from the true model parameter \\(\\theta^*\\) , and thus increases the bias of the estimate, which is defined as \\(\\mathbb{E}\\left[\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right]-\\boldsymbol{\\theta}^*=\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}^{\\text {signal }}-\\boldsymbol{\\theta}^*\\) \\(\\lambda\\) \u8d8a\u5927\uff0c \\(\\hat{\\theta}_{\\text{ridge}}\\) \u7684\u7edd\u5bf9\u503c\u8d8a\u5c0f --> 0 But in exchange, increasing \\(\\lambda\\) also shrinks the impact of the noise. Thus, choosing \\(\\lambda\\) appropriately enables us to adapt to the conditioning of the feature matrix and to the noise level for achieving a good tradeoff between the bias and the variance. \u56e0\u6b64\uff0c\u9009\u62e9\u9002\u5f53\u7684\u03bb\u4f7f\u6211\u4eec\u80fd\u591f\u9002\u5e94\u7279\u5f81\u77e9\u9635\u7684\u6761\u4ef6\u548c\u566a\u58f0\u6c34\u5e73\uff0c\u4ee5\u5728\u504f\u5dee\u548c\u65b9\u5dee\u4e4b\u95f4\u5b9e\u73b0\u826f\u597d\u7684\u6298\u8877\u3002 Note that the optimal choice depends on the noise which is unknown to us, the conditioning of the feature matrix, thus we cannot simply compute the optimal choice based on the data. Later in the course we will discuss methods that enable us to determine a good choice of \\(\\lambda\\) based on the data.","title":"Analysis"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#compared-with-linear-regression","text":"where normalized MSE = \\(\\left\\|\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right\\|_2^2 /\\|\\boldsymbol{\\theta}\\|_2^2\\) \u53ef\u4ee5\u770b\u5230\u4f7f\u5f97loss\u6700\u5c0f\u7684 \\(\\lambda \\neq 0\\)","title":"Compared with linear regression"},{"location":"Supervised%20Learning/2%20Classification/2.1%20Logistic%20Regression/","text":"Logistic Regression In this lecture, we start discussing classification problems. In a classification problem, our goal is, given a feature vector \\(\\mathbf{x}\\) , to predict its label \\(y\\) , which can only take on a small set of discrete values. A special case is binary classification, where the label \\(y\\) only takes on two values, 0 and 1 for example. Most applications of machine learning are classification problems. A concrete classical example of a classification problem is spam filtering, where the vector \\(\\mathbf{x}\\) contains features of an email, and \\(y=1\\) stands for spam and \\(y=0\\) stands for not spam. Formally, the classification problem is as follows: Given a training set \\(\\left.\\left\\{\\left(\\mathbf{x}_1, y_1\\right), \\ldots, \\mathbf{x}_n, y_n\\right)\\right\\}\\) , where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(y_i \\in\\{0,1, \\ldots, K-1\\}\\) , our goal is to find a classifier \\(h\\) such that given an arbitrary feature vector \\(\\mathbf{x}\\) , the classifier \\(h\\) predicts the corresponding class \\(\\{0,1, \\ldots, K-1\\}\\) . Models for classification problems can be distinguished as being generative or discriminative . A discriminative model describes the distribution \\(\\mathrm{P}[y \\mid \\mathbf{x}]\\) directly and thus directly discriminates between the target value \\(y\\) for any given feature vector \\(\\mathbf{x}\\) . In contrast, a generative model learns \\(\\mathrm{P}[\\mathbf{x} \\mid y]\\) and \\(\\mathrm{P}[y]\\) directly, we will discuss an example of a generative model/classifier in the next lecture. Logistic Regression Model For simplicity, we start with a binary classification problem with \\(y \\in\\{0,1\\}\\) . We could approach the classification problem as a continuous-valued regression problem by ignoring the fact that \\(y\\) is discretely valued, but that usually performs very poorly. Instead, we can change the parametric form of our classifier, to ensure that it only outputs values in the interval \\([0,1]\\) . The output of the classifier can then be interpreted as the probability that the feature belongs to one class or the other. In the logistic regression model, the classifier takes the following form: $$ h_\\theta(\\mathbf{x})=g(\\langle\\theta, \\mathbf{x}\\rangle), $$ where $$ g(z)=\\left(1+e^{-z}\\right)^{-1} $$ is the logistic function , also called the sigmoid function . Figure 1 contains a plot. The logistic function smoothly varies between 0 and 1 . We can also choose other functions instead of the logistic function, but we will see later that the logistic function has some advantages. Let us interpret \\(h_\\theta(\\mathbf{x})\\) as a probability: $$ \\mathrm{P}[y=1 \\mid \\mathbf{x}]=h_\\theta(\\mathbf{x}) \\text { and } \\mathrm{P}[y=0 \\mid \\mathbf{x}]=1-h_\\theta(\\mathbf{x}) . $$ We then classify an example as belonging to class 1 if \\(\\mathrm{P}[y=1 \\mid \\mathbf{x}]>1 / 2\\) which is equivalent to \\(\\langle\\theta, \\mathbf{x}\\rangle>0\\) . Logistic regression is called a discriminative classifier, since it directly estimates the parameters of the distribution \\(\\mathrm{P}[y \\mid \\mathbf{x}]\\) . \u903b\u8f91\u56de\u5f52\u6a21\u578bP(y|x) - \u4e3a\u4f2f\u52aa\u5229\u5206\u5e03 - discriminative Fitting a Model We next discuss methods to learn the feature vector \\(\\theta\\) based on training data. In the lecture on linear regression, we saw that we can learn parameters based on modeling assumptions on how the training data was generated. Let us assume that we are given a training set \\(\\left.\\mathcal{D}=\\left\\{\\left(\\mathbf{x}_1, y_1\\right), \\ldots, \\mathbf{x}_n, y_n\\right)\\right\\}\\) , where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(y_i \\in\\{0,1\\}\\) . We furthermore assume that the feature vectors \\(\\mathbf{x}_i\\) are deterministic, but the labels are generated at random and independently across \\(i\\) and follow the distribution \\[ \\mathrm{P}\\left[y_i=1 \\mid \\mathbf{x}_i, \\theta\\right]=h_\\theta\\left(\\mathbf{x}_i\\right) \\text { and } \\mathrm{P}\\left[y_i=0 \\mid \\mathbf{x}_i, \\theta\\right]=1-h_\\theta\\left(\\mathbf{x}_i\\right) . \\] As before, we use the maximum likelihood estimate for the parameter \\(\\theta\\) , i.e., the estimate that maximizes the probability that the data \\(\\mathcal{D}\\) is from the model \\(h_\\theta\\) . This amounts to finding the parameter \\(\\theta\\) that maximizes the likelihood function, defined as The second equality follows by our assumption that each label \\(y_i\\) is generated independently, and the third equality follows from our assumption that the data is generated by the model \\(h_\\theta\\) . As before, we maximize the log-likelihood instead (which we can do since log is strictly monotonic) \\[ \\log \\mathcal{L}(\\theta, \\mathcal{D})=\\sum_{i=1}^n y_i \\log \\left(h\\left(\\mathbf{x}_i\\right)\\right)+\\left(1-y_i\\right) \\log \\left(1-h_\\theta\\left(\\mathbf{x}_i\\right)\\right) . \\] Cross Entropy \u5c31\u662f\u901a\u8fc7\u5bf9likelihood (MLE) \u53d6log \u5316\u7b80\u5f97\u5230\u7684 In conclusion, the logistic regression estimate of the parameter \\(\\theta\\) is given by: \\[ \\hat{\\theta}=\\arg \\min _\\theta \\sum_{i=1}^n y_i \\log \\left(1 / h_\\theta\\left(\\mathbf{x}_i\\right)\\right)+\\left(1-y_i\\right) \\log 1 /\\left(1-h_\\theta\\left(\\mathbf{x}_i\\right)\\right) . \\] Before discussing how to solve this optimization problem, we note that in regression, we started our discussion by fitting a model by minimizing the loss between the data predicted by the model and the training data, and we choose the quadratic loss function \\(\\operatorname{loss}(y, z)=(y-z)^2\\) to measure the loss. The theoretical justification was that learning with the quadratic loss corresponds to maximum likelihood estimation under Gaussian noise. We could approach logistic regression in the same manner. Specifically, we can fit a model by solving \\[ \\hat{\\theta}=\\arg \\min _\\theta \\frac{1}{n} \\sum_{i=1}^n \\operatorname{loss}\\left(h_\\theta\\left(\\mathbf{x}_i\\right), y_i\\right), \\] where loss is some loss function. Logistic regression does exactly that, and chooses as the loss function the log-loss or the negative cross entropy defined as \\[ \\operatorname{loss}(y, z)=-y \\log (z)-(1-y) \\log (1-z)=y \\log (1 / z)+(1-y) \\log (1 /(1-z)) \\] Computing the logistic regression estimate with gradient descent The negative log-likelihood \\[ -\\log \\mathcal{L}(\\theta, \\mathcal{D})=\\sum_{i=1}^n \\operatorname{loss}\\left(g\\left(\\left\\langle\\theta, \\mathbf{x}_i\\right\\rangle\\right), y_i\\right), \\] with \\[ \\operatorname{loss}\\left(g\\left(\\left\\langle\\theta, \\mathbf{x}_i\\right\\rangle\\right), y_i\\right)=-y_i \\log \\left(g\\left(\\left\\langle\\theta, \\mathbf{x}_i\\right\\rangle\\right)\\right)-\\left(1-y_i\\right) \\log \\left(1-g\\left(\\left\\langle\\theta, \\mathbf{x}_i\\right\\rangle\\right)\\right) \\] is convex since \\(\\operatorname{loss}\\left(g\\left(\\left\\langle\\theta, \\mathbf{x}_i\\right\\rangle\\right), y_i\\right)\\) is convex, as we show later. Thus, we can approximate the logistic regression estimate \\(\\hat{\\theta}\\) with gradient descent: \\[ \\begin{aligned} \\theta^{k+1} &=\\theta^k-\\alpha \\nabla(-\\log \\mathcal{L}(\\theta, \\mathcal{D})) \\\\ &=\\theta^k-\\alpha \\sum_{i=1}^n \\nabla \\operatorname{loss}\\left(g\\left(\\left\\langle\\theta, \\mathbf{x}_i\\right\\rangle\\right), y_i\\right) \\end{aligned} \\] In practice we would actually often use stochastic gradient descent to minimize the function above, because of computational reasons. The stochastic gradient method is similar to gradient descent, and will be discussed in detail later in this course. We next compute the gradient of the loss above. Towards this goal, we first note that a convenient property of the logistic function is that its derivative can be written as: \\[ g^{\\prime}(z)=\\frac{-1}{\\left(1+e^{-z}\\right)^2} e^{-z}(-1)=\\frac{1}{\\left(1+e^{-z}\\right)}\\left(1-\\frac{1}{\\left(1+e^{-z}\\right)}\\right)=g(z)(1-g(z)), \\] where the first equality is by the chain rule. With this relation, the gradients can be computed as where the second inequality follows from the convenient relation \\(g^{\\prime}(z)=g(z)(1-g(z))\\) , shown above. Next, we show that the loss above is indeed convex in \\(\\theta\\) , as claimed. We can prove this as follows. Suppose that \\(d=1\\) . Then the likelihood is a function \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) . A differentiable function \\(f\\) is convex if and only if its second derivative obeys \\(f^{\\prime \\prime}(\\mathbf{x}) \\geq 0\\) for all \\(\\mathbf{x}\\) . For example \\(x^2\\) is convex and it's second derivative is 1 . This generalizes to multivariate functions as follows. Define the Hessian of a multivariate function \\(f: \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) as the \\(d \\times d\\) matrix \\(\\nabla^2 f(\\mathbf{x})\\) with entries \\[ \\left[\\nabla^2 f(\\mathbf{x})\\right]_{i j}=\\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j} f(\\mathbf{x}) . \\] A multivariate function is convex if and only if \\(\\nabla^2 f(\\mathbf{x})\\) is positive semidefinite (recall that a matrix \\(\\mathbf{A}\\) is positive semidefinite if \\(\\mathbf{z}^T \\mathbf{A} \\mathbf{z} \\geq 0\\) for all \\(\\mathbf{z}\\) ). Thus, all we need to do is to show that the Hessian of the loss if positive semidefinite. Towards this goal, we compute Thus, the Hessian can be written as \\[ \\nabla^2 \\operatorname{loss}\\left(y, h_\\theta(\\mathbf{x})\\right)=g(\\langle\\theta, \\mathbf{x}\\rangle)(1-g(\\langle\\theta, \\mathbf{x}\\rangle)) \\mathbf{x x}^T . \\] Since \\(g(\\langle\\theta, \\mathbf{x}\\rangle)(1-g(\\langle\\theta, \\mathbf{x}\\rangle))\\) is non-negative, and \\(\\mathbf{x x}^T\\) is positive semidefinite, the Hessian above is positive semidefinite, and therefore the loss is convex in \\(\\theta\\) . Since the negative log-likelihood \\(-\\log \\mathcal{L}(\\theta, \\mathcal{D})\\) is s a sum of convex functions, it is also convex. As a consequence, gradient descent will converge to a minimizer. Multiclass logistic regression Let us next generalize logistic regression to the case with \\(K>2\\) classes instead of only two classes. This is called multiclass logistic regression or softmax regression . A key idea is to use what is called a one-hot-encoding \\(\\mathbf{y} \\in\\{0,1\\}^K\\) for representing the labels. The one-hot encoding \\(\\mathbf{y}\\) associated with a label \\(y=k\\) is equal to 1 only at the position \\(k\\) \\[ [\\mathbf{y}]_k=1 \\text {, if } y=k, \\quad \\text { and } \\quad[\\mathbf{y}]_j=0 \\text { for all } j \\neq k \\text {. } \\] We associate a parameter vector \\(\\theta_k \\in \\mathbb{R}^d\\) with each class. For a given feature vector, we can associate a score with each class \\(z_k=\\left\\langle\\theta_k, \\mathbf{x}\\right\\rangle\\) . Computing this score for each \\(k\\) yields the vector \\(\\mathbf{z}=\\left[z_1, \\ldots, z_K\\right]\\) . The generalization of the logistic function is the softmax function \\(\\sigma: \\mathbb{R}^d \\rightarrow[0,1]^d\\) , and is defined as: \\[ [\\sigma(\\mathbf{z})]_j=\\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}, \\quad \\text { for } k=1, \\ldots, K \\] Note that the components of the sofmax function \\(\\sigma(\\mathbf{z})\\) applied to any vector \\(\\mathbf{z}\\) sum to one and lie in the range \\([0,1]\\) . Thus, they can be interpreted as a probability distribution ( Categorical Distribution ). Given a feature vector \\(\\mathbf{x}\\) , the model then predict its label as the index of the largest component of \\(\\sigma(\\mathbf{z})\\) . We can estimate the parameters \\(\\theta\\) based on training data in a similar manner as before. Specifically, by summarizing the parameter vectors with the matrix \\(\\Theta=\\left[\\theta_1, \\ldots, \\theta_K\\right]\\) , we estimate the parameters by maximizing the likelihood or equivalently, minimizing the log-likeliehood: \\[ \\begin{aligned} \\hat{\\Theta} &=\\arg \\max _{\\Theta} \\mathcal{L}(\\Theta, \\mathcal{D}) \\\\ &=\\arg \\max _{\\Theta} \\prod_{i=1}^n \\mathrm{P}\\left[y_i \\mid \\mathbf{x}_i, \\Theta\\right] \\\\ &=\\arg \\max _{\\Theta} \\prod_{i=1}^n \\prod_{k=1}^k \\mathrm{P}\\left[y_i=k \\mid \\mathbf{x}_i, \\theta_k\\right]^{\\mathbb{1}\\left\\{y_i=k\\right\\}} \\\\ &=\\arg \\min _{\\Theta}-\\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{1}\\left\\{y_i=k\\right\\} \\log \\mathrm{P}\\left[y_i=k \\mid \\mathbf{x}_i, \\theta_k\\right] \\\\ &=\\arg \\min _{\\Theta}-\\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{1}\\left\\{y_i=k\\right\\} \\log \\left(\\frac{e^{\\left\\langle\\theta_k, \\mathbf{x}_i\\right\\rangle}}{\\sum_{j=1}^K e^{\\left\\langle\\theta_j, \\mathbf{x}_i\\right\\rangle}}\\right) . \\end{aligned} \\] CE \u5c31\u662f\u901a\u8fc7\u5bf9Likelihood (MLE) \u53d6log\u5316\u7b80\u5f97\u5230\u7684 As for the binary regression case, this is a convex optimization problem and can be solved with gradient descent.","title":"Logistic Regression"},{"location":"Supervised%20Learning/2%20Classification/2.1%20Logistic%20Regression/#logistic-regression","text":"In this lecture, we start discussing classification problems. In a classification problem, our goal is, given a feature vector \\(\\mathbf{x}\\) , to predict its label \\(y\\) , which can only take on a small set of discrete values. A special case is binary classification, where the label \\(y\\) only takes on two values, 0 and 1 for example. Most applications of machine learning are classification problems. A concrete classical example of a classification problem is spam filtering, where the vector \\(\\mathbf{x}\\) contains features of an email, and \\(y=1\\) stands for spam and \\(y=0\\) stands for not spam. Formally, the classification problem is as follows: Given a training set \\(\\left.\\left\\{\\left(\\mathbf{x}_1, y_1\\right), \\ldots, \\mathbf{x}_n, y_n\\right)\\right\\}\\) , where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(y_i \\in\\{0,1, \\ldots, K-1\\}\\) , our goal is to find a classifier \\(h\\) such that given an arbitrary feature vector \\(\\mathbf{x}\\) , the classifier \\(h\\) predicts the corresponding class \\(\\{0,1, \\ldots, K-1\\}\\) . Models for classification problems can be distinguished as being generative or discriminative . A discriminative model describes the distribution \\(\\mathrm{P}[y \\mid \\mathbf{x}]\\) directly and thus directly discriminates between the target value \\(y\\) for any given feature vector \\(\\mathbf{x}\\) . In contrast, a generative model learns \\(\\mathrm{P}[\\mathbf{x} \\mid y]\\) and \\(\\mathrm{P}[y]\\) directly, we will discuss an example of a generative model/classifier in the next lecture.","title":"Logistic Regression"},{"location":"Supervised%20Learning/2%20Classification/2.1%20Logistic%20Regression/#logistic-regression-model","text":"For simplicity, we start with a binary classification problem with \\(y \\in\\{0,1\\}\\) . We could approach the classification problem as a continuous-valued regression problem by ignoring the fact that \\(y\\) is discretely valued, but that usually performs very poorly. Instead, we can change the parametric form of our classifier, to ensure that it only outputs values in the interval \\([0,1]\\) . The output of the classifier can then be interpreted as the probability that the feature belongs to one class or the other. In the logistic regression model, the classifier takes the following form: $$ h_\\theta(\\mathbf{x})=g(\\langle\\theta, \\mathbf{x}\\rangle), $$ where $$ g(z)=\\left(1+e^{-z}\\right)^{-1} $$ is the logistic function , also called the sigmoid function . Figure 1 contains a plot. The logistic function smoothly varies between 0 and 1 . We can also choose other functions instead of the logistic function, but we will see later that the logistic function has some advantages. Let us interpret \\(h_\\theta(\\mathbf{x})\\) as a probability: $$ \\mathrm{P}[y=1 \\mid \\mathbf{x}]=h_\\theta(\\mathbf{x}) \\text { and } \\mathrm{P}[y=0 \\mid \\mathbf{x}]=1-h_\\theta(\\mathbf{x}) . $$ We then classify an example as belonging to class 1 if \\(\\mathrm{P}[y=1 \\mid \\mathbf{x}]>1 / 2\\) which is equivalent to \\(\\langle\\theta, \\mathbf{x}\\rangle>0\\) . Logistic regression is called a discriminative classifier, since it directly estimates the parameters of the distribution \\(\\mathrm{P}[y \\mid \\mathbf{x}]\\) . \u903b\u8f91\u56de\u5f52\u6a21\u578bP(y|x) - \u4e3a\u4f2f\u52aa\u5229\u5206\u5e03 - discriminative","title":"Logistic Regression Model"},{"location":"Supervised%20Learning/2%20Classification/2.1%20Logistic%20Regression/#fitting-a-model","text":"We next discuss methods to learn the feature vector \\(\\theta\\) based on training data. In the lecture on linear regression, we saw that we can learn parameters based on modeling assumptions on how the training data was generated. Let us assume that we are given a training set \\(\\left.\\mathcal{D}=\\left\\{\\left(\\mathbf{x}_1, y_1\\right), \\ldots, \\mathbf{x}_n, y_n\\right)\\right\\}\\) , where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(y_i \\in\\{0,1\\}\\) . We furthermore assume that the feature vectors \\(\\mathbf{x}_i\\) are deterministic, but the labels are generated at random and independently across \\(i\\) and follow the distribution \\[ \\mathrm{P}\\left[y_i=1 \\mid \\mathbf{x}_i, \\theta\\right]=h_\\theta\\left(\\mathbf{x}_i\\right) \\text { and } \\mathrm{P}\\left[y_i=0 \\mid \\mathbf{x}_i, \\theta\\right]=1-h_\\theta\\left(\\mathbf{x}_i\\right) . \\] As before, we use the maximum likelihood estimate for the parameter \\(\\theta\\) , i.e., the estimate that maximizes the probability that the data \\(\\mathcal{D}\\) is from the model \\(h_\\theta\\) . This amounts to finding the parameter \\(\\theta\\) that maximizes the likelihood function, defined as The second equality follows by our assumption that each label \\(y_i\\) is generated independently, and the third equality follows from our assumption that the data is generated by the model \\(h_\\theta\\) . As before, we maximize the log-likelihood instead (which we can do since log is strictly monotonic) \\[ \\log \\mathcal{L}(\\theta, \\mathcal{D})=\\sum_{i=1}^n y_i \\log \\left(h\\left(\\mathbf{x}_i\\right)\\right)+\\left(1-y_i\\right) \\log \\left(1-h_\\theta\\left(\\mathbf{x}_i\\right)\\right) . \\] Cross Entropy \u5c31\u662f\u901a\u8fc7\u5bf9likelihood (MLE) \u53d6log \u5316\u7b80\u5f97\u5230\u7684 In conclusion, the logistic regression estimate of the parameter \\(\\theta\\) is given by: \\[ \\hat{\\theta}=\\arg \\min _\\theta \\sum_{i=1}^n y_i \\log \\left(1 / h_\\theta\\left(\\mathbf{x}_i\\right)\\right)+\\left(1-y_i\\right) \\log 1 /\\left(1-h_\\theta\\left(\\mathbf{x}_i\\right)\\right) . \\] Before discussing how to solve this optimization problem, we note that in regression, we started our discussion by fitting a model by minimizing the loss between the data predicted by the model and the training data, and we choose the quadratic loss function \\(\\operatorname{loss}(y, z)=(y-z)^2\\) to measure the loss. The theoretical justification was that learning with the quadratic loss corresponds to maximum likelihood estimation under Gaussian noise. We could approach logistic regression in the same manner. Specifically, we can fit a model by solving \\[ \\hat{\\theta}=\\arg \\min _\\theta \\frac{1}{n} \\sum_{i=1}^n \\operatorname{loss}\\left(h_\\theta\\left(\\mathbf{x}_i\\right), y_i\\right), \\] where loss is some loss function. Logistic regression does exactly that, and chooses as the loss function the log-loss or the negative cross entropy defined as \\[ \\operatorname{loss}(y, z)=-y \\log (z)-(1-y) \\log (1-z)=y \\log (1 / z)+(1-y) \\log (1 /(1-z)) \\]","title":"Fitting a Model"},{"location":"Supervised%20Learning/2%20Classification/2.1%20Logistic%20Regression/#computing-the-logistic-regression-estimate-with-gradient-descent","text":"The negative log-likelihood \\[ -\\log \\mathcal{L}(\\theta, \\mathcal{D})=\\sum_{i=1}^n \\operatorname{loss}\\left(g\\left(\\left\\langle\\theta, \\mathbf{x}_i\\right\\rangle\\right), y_i\\right), \\] with \\[ \\operatorname{loss}\\left(g\\left(\\left\\langle\\theta, \\mathbf{x}_i\\right\\rangle\\right), y_i\\right)=-y_i \\log \\left(g\\left(\\left\\langle\\theta, \\mathbf{x}_i\\right\\rangle\\right)\\right)-\\left(1-y_i\\right) \\log \\left(1-g\\left(\\left\\langle\\theta, \\mathbf{x}_i\\right\\rangle\\right)\\right) \\] is convex since \\(\\operatorname{loss}\\left(g\\left(\\left\\langle\\theta, \\mathbf{x}_i\\right\\rangle\\right), y_i\\right)\\) is convex, as we show later. Thus, we can approximate the logistic regression estimate \\(\\hat{\\theta}\\) with gradient descent: \\[ \\begin{aligned} \\theta^{k+1} &=\\theta^k-\\alpha \\nabla(-\\log \\mathcal{L}(\\theta, \\mathcal{D})) \\\\ &=\\theta^k-\\alpha \\sum_{i=1}^n \\nabla \\operatorname{loss}\\left(g\\left(\\left\\langle\\theta, \\mathbf{x}_i\\right\\rangle\\right), y_i\\right) \\end{aligned} \\] In practice we would actually often use stochastic gradient descent to minimize the function above, because of computational reasons. The stochastic gradient method is similar to gradient descent, and will be discussed in detail later in this course. We next compute the gradient of the loss above. Towards this goal, we first note that a convenient property of the logistic function is that its derivative can be written as: \\[ g^{\\prime}(z)=\\frac{-1}{\\left(1+e^{-z}\\right)^2} e^{-z}(-1)=\\frac{1}{\\left(1+e^{-z}\\right)}\\left(1-\\frac{1}{\\left(1+e^{-z}\\right)}\\right)=g(z)(1-g(z)), \\] where the first equality is by the chain rule. With this relation, the gradients can be computed as where the second inequality follows from the convenient relation \\(g^{\\prime}(z)=g(z)(1-g(z))\\) , shown above. Next, we show that the loss above is indeed convex in \\(\\theta\\) , as claimed. We can prove this as follows. Suppose that \\(d=1\\) . Then the likelihood is a function \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) . A differentiable function \\(f\\) is convex if and only if its second derivative obeys \\(f^{\\prime \\prime}(\\mathbf{x}) \\geq 0\\) for all \\(\\mathbf{x}\\) . For example \\(x^2\\) is convex and it's second derivative is 1 . This generalizes to multivariate functions as follows. Define the Hessian of a multivariate function \\(f: \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) as the \\(d \\times d\\) matrix \\(\\nabla^2 f(\\mathbf{x})\\) with entries \\[ \\left[\\nabla^2 f(\\mathbf{x})\\right]_{i j}=\\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j} f(\\mathbf{x}) . \\] A multivariate function is convex if and only if \\(\\nabla^2 f(\\mathbf{x})\\) is positive semidefinite (recall that a matrix \\(\\mathbf{A}\\) is positive semidefinite if \\(\\mathbf{z}^T \\mathbf{A} \\mathbf{z} \\geq 0\\) for all \\(\\mathbf{z}\\) ). Thus, all we need to do is to show that the Hessian of the loss if positive semidefinite. Towards this goal, we compute Thus, the Hessian can be written as \\[ \\nabla^2 \\operatorname{loss}\\left(y, h_\\theta(\\mathbf{x})\\right)=g(\\langle\\theta, \\mathbf{x}\\rangle)(1-g(\\langle\\theta, \\mathbf{x}\\rangle)) \\mathbf{x x}^T . \\] Since \\(g(\\langle\\theta, \\mathbf{x}\\rangle)(1-g(\\langle\\theta, \\mathbf{x}\\rangle))\\) is non-negative, and \\(\\mathbf{x x}^T\\) is positive semidefinite, the Hessian above is positive semidefinite, and therefore the loss is convex in \\(\\theta\\) . Since the negative log-likelihood \\(-\\log \\mathcal{L}(\\theta, \\mathcal{D})\\) is s a sum of convex functions, it is also convex. As a consequence, gradient descent will converge to a minimizer.","title":"Computing the logistic regression estimate with gradient descent"},{"location":"Supervised%20Learning/2%20Classification/2.1%20Logistic%20Regression/#multiclass-logistic-regression","text":"Let us next generalize logistic regression to the case with \\(K>2\\) classes instead of only two classes. This is called multiclass logistic regression or softmax regression . A key idea is to use what is called a one-hot-encoding \\(\\mathbf{y} \\in\\{0,1\\}^K\\) for representing the labels. The one-hot encoding \\(\\mathbf{y}\\) associated with a label \\(y=k\\) is equal to 1 only at the position \\(k\\) \\[ [\\mathbf{y}]_k=1 \\text {, if } y=k, \\quad \\text { and } \\quad[\\mathbf{y}]_j=0 \\text { for all } j \\neq k \\text {. } \\] We associate a parameter vector \\(\\theta_k \\in \\mathbb{R}^d\\) with each class. For a given feature vector, we can associate a score with each class \\(z_k=\\left\\langle\\theta_k, \\mathbf{x}\\right\\rangle\\) . Computing this score for each \\(k\\) yields the vector \\(\\mathbf{z}=\\left[z_1, \\ldots, z_K\\right]\\) . The generalization of the logistic function is the softmax function \\(\\sigma: \\mathbb{R}^d \\rightarrow[0,1]^d\\) , and is defined as: \\[ [\\sigma(\\mathbf{z})]_j=\\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}, \\quad \\text { for } k=1, \\ldots, K \\] Note that the components of the sofmax function \\(\\sigma(\\mathbf{z})\\) applied to any vector \\(\\mathbf{z}\\) sum to one and lie in the range \\([0,1]\\) . Thus, they can be interpreted as a probability distribution ( Categorical Distribution ). Given a feature vector \\(\\mathbf{x}\\) , the model then predict its label as the index of the largest component of \\(\\sigma(\\mathbf{z})\\) . We can estimate the parameters \\(\\theta\\) based on training data in a similar manner as before. Specifically, by summarizing the parameter vectors with the matrix \\(\\Theta=\\left[\\theta_1, \\ldots, \\theta_K\\right]\\) , we estimate the parameters by maximizing the likelihood or equivalently, minimizing the log-likeliehood: \\[ \\begin{aligned} \\hat{\\Theta} &=\\arg \\max _{\\Theta} \\mathcal{L}(\\Theta, \\mathcal{D}) \\\\ &=\\arg \\max _{\\Theta} \\prod_{i=1}^n \\mathrm{P}\\left[y_i \\mid \\mathbf{x}_i, \\Theta\\right] \\\\ &=\\arg \\max _{\\Theta} \\prod_{i=1}^n \\prod_{k=1}^k \\mathrm{P}\\left[y_i=k \\mid \\mathbf{x}_i, \\theta_k\\right]^{\\mathbb{1}\\left\\{y_i=k\\right\\}} \\\\ &=\\arg \\min _{\\Theta}-\\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{1}\\left\\{y_i=k\\right\\} \\log \\mathrm{P}\\left[y_i=k \\mid \\mathbf{x}_i, \\theta_k\\right] \\\\ &=\\arg \\min _{\\Theta}-\\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{1}\\left\\{y_i=k\\right\\} \\log \\left(\\frac{e^{\\left\\langle\\theta_k, \\mathbf{x}_i\\right\\rangle}}{\\sum_{j=1}^K e^{\\left\\langle\\theta_j, \\mathbf{x}_i\\right\\rangle}}\\right) . \\end{aligned} \\] CE \u5c31\u662f\u901a\u8fc7\u5bf9Likelihood (MLE) \u53d6log\u5316\u7b80\u5f97\u5230\u7684 As for the binary regression case, this is a convex optimization problem and can be solved with gradient descent.","title":"Multiclass logistic regression"},{"location":"Supervised%20Learning/2%20Classification/2.2%20Support%20Vector%20Machine/","text":"Support vector machines Support vector machines are a class of model for classification. We already learned about a logistic regression as another model for classification. In logistic regression we learn a parameter vector \\(\\boldsymbol{\\theta}\\) and classify an example as belonging to one of two (or \\(k\\) ) classes based on a decision boundary. Specifically, in logistic regression, we classified an example as belonging to class one if the inner product \\(\\langle\\boldsymbol{\\theta}, \\mathbf{x}\\rangle\\) is larger than a threshold. In a similar spirit, Support Vector Machines (SVMs) learn decision boundaries. Contrary to our previous discussion on logistic regression, for SVMs we will not make any distributional assumption on the data. logistic regression assumes the data follow a Benouri distribution As before, we consider a binary classification problem. Given a training dataset \\[ \\mathcal{D}=\\left\\{\\left(\\mathbf{x}_1, y_1\\right), \\ldots,\\left(\\mathbf{x}_n, y_n\\right)\\right\\} \\in \\mathbb{R}^d \\times\\{-1,1\\}, \\] our goal is to find a classifier based on separating the two classes by a hyperplane. A classifier \\(f\\) based on a hyperplane classifies a feature \\(\\mathbf{x}\\) as 1 if \\(\\langle\\boldsymbol{\\theta}, \\mathbf{x}\\rangle \\geq b\\) , where \\(b\\) is a threshold, and as \\(-1\\) otherwise, i.e., $$ f(\\mathbf{x})= \\begin{cases}1 & \\langle\\boldsymbol{\\theta}, \\mathbf{x}\\rangle \\geq b \\ -1 & \\langle\\boldsymbol{\\theta}, \\mathbf{x}\\rangle<b .\\end{cases} $$ In logistic regression, the threshold \\(b\\) is 0 The perceptron algorithm As a first step towards SVMs, we discuss the perceptron algorithm. The perceptron algorithm finds a hyperplane that perfectly separates the \\(+1\\) 's from the \\(-1\\) 's. The goal of the perceptron algorithm is to find a parameter vector \\(\\boldsymbol{\\theta}\\) such that for all \\(i\\) , $$ \\begin{cases}\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle \\geq b & \\text { if } y_i=1, \\ \\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle<b & \\text { if } y_i=-1 .\\end{cases} $$ Such a parameter vector \\(\\boldsymbol{\\theta}\\) does not necessarily exist: if it exists, we say that the data is linearly separable. This restriction to data that is linearly separable is a major issue limiting the applicability of perceptrons in its original form in practice, and we will discuss less restrictive SVMs later. Even if the data is linearly separable, then a parameter vector \\(\\boldsymbol{\\theta}\\) and a threshold \\(b\\) are not necessarily unique for separating the \\(+1\\) 's from the \\(-1\\) 's, thus it is not clear which hyperplane to choose. Hard-margin SVMs Suppose that the dataset \\(\\mathcal{D}\\) is is linearly separable. Then there might be different hyperplanes that separate the data. The concept of a margin enables us to distinguish between \"good\" and \"bad\" separating hyperplanes. Instead of finding an arbitrary hyperplane that separates the two classes, our goal is to find the particular hyperplane that maximizes the distance from the decision boundary to the training points. In more detail, we want to maximize a margin \\(m \\geq 0\\) such that i) all points classified as \\(+1\\) are on the positive side of the hyperplane, and their distance to the hyperplane is at least \\(m\\) , and ii) all points classified as \\(-1\\) are on the negative side of the hyperplane, and their distance to the hyperplane is at least \\(m\\) . To find such a hyperplane, we first note that the distance of a point \\(\\mathrm{x}\\) to a hyperplane. \\[ \\mathcal{H}=\\{\\mathbf{x}:\\langle\\boldsymbol{\\theta}, \\mathbf{x}\\rangle=b\\} \\] is given by \\[ \\frac{\\left|\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}-\\mathbf{x}_0\\right\\rangle\\right|}{\\|\\boldsymbol{\\theta}\\|_2}=\\frac{|\\langle\\boldsymbol{\\theta}, \\mathbf{x}\\rangle-b|}{\\|\\boldsymbol{\\theta}\\|_2} . \\] This can be seen as follows. By definition, the vector \\(\\boldsymbol{\\theta}\\) is perpendicular to the hyperplane \\(\\mathcal{H}\\) , i.e., to any vector that lies on the hyperplane. Specifically, consider any two points \\(\\mathbf{x}_0, \\mathbf{x}_1 \\in \\mathcal{H}\\) , and note that the vectors \\(\\mathbf{x}_1-\\mathbf{x}_0\\) , which lies on the hyperplane, is orthogonal to \\(\\boldsymbol{\\theta}\\) : $$ \\left\\langle\\mathbf{x}_1-\\mathbf{x}_0, \\boldsymbol{\\theta}\\right\\rangle=\\left\\langle\\mathbf{x}_1, \\boldsymbol{\\theta}\\right\\rangle-\\left\\langle\\mathbf{x}_0, \\boldsymbol{\\theta}\\right\\rangle=b-b=0 . $$ As a consequence of \\(\\boldsymbol{\\theta}\\) being perpendicular to \\(\\mathcal{H}\\) , the shortest distance of a point \\(\\mathbf{x}\\) to \\(\\mathcal{H}\\) is given by the projection of \\(\\mathrm{x}-\\mathrm{x}_0\\) onto \\(\\boldsymbol{\\theta}\\) , where \\(\\mathrm{x}_0 \\in \\mathcal{H}\\) . Thus, the distance of the point to the hyperplane is $$ \\frac{\\left|\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}-\\mathbf{x}_0\\right\\rangle\\right|}{|\\boldsymbol{\\theta}|_2}=\\frac{|\\langle\\boldsymbol{\\theta}, \\mathbf{x}\\rangle-b|}{|\\boldsymbol{\\theta}|_2} . $$ Now let's return to the problem of finding a hyperplane that maximizes the margin. All datapoints are classified as \\(+1(-1)\\) are on the positive (negative) side of the hyperplane, and their distance to the hyperplane is at least \\(m\\) , if for all \\(i=1, \\ldots, n\\) $$ y_i \\frac{\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle-b}{|\\boldsymbol{\\theta}|_2} \\geq m $$ This leads to the optimization problem: $$ \\max _{m, \\boldsymbol{\\theta}, b} m \\quad \\text { subject to } y_i \\frac{\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle-b}{|\\boldsymbol{\\theta}|_2} \\geq m, i=1, \\ldots, n \\text {. } $$ Here, we maximize over the parameters \\(b, \\boldsymbol{\\theta}\\) , and \\(m\\) . The margin \\(m\\) is maximized if there exists at least one point on the negative side, and at least one on the positive side whose distance to the hyperplane is equal to \\(m\\) ; these points are called the support vectors . We next simplify the optimization problem by removing the variable \\(m\\) . - Note that the solution of the optimization problem is invariant to scaling \\(\\boldsymbol{\\theta}\\) and \\(b\\) by a positive scalar. Therefore, we can fix the norm of \\(\\boldsymbol{\\theta}\\) to a particular value without changing the optimization problem. - Setting \\(\\|\\boldsymbol{\\theta}\\|_2=1 / m\\) , the optimization problem becomes \\[ \\min _{\\boldsymbol{\\theta}, b} \\frac{1}{2}\\|\\boldsymbol{\\theta}\\|_2^2 \\quad \\text { subject to } \\quad y_i\\left(\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle-b\\right) \\geq 1 . \\] This is the standard formulation of the hard-margin SVM optimization problem. Soft-margin SVMs The hard-margin SVM optimization problem only has a solution if the data is linearly separable. Even if the data is linearly separable, the solution of the hard-margin SVM optimization problem can vary significantly when including or excluding single points that change the support vectors. Soft-margin SVMs address these issues by allowing points to violate the margin. The idea is to introduce a slack variable \\(\\xi_i\\) for each data point, which allows points to violate the margin : \\[ y_i\\left(\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle-b\\right) \\geq 1-\\xi_i, \\quad \\xi_i \\geq 0 . \\] We take the slack variable in the objective into account by penalizing large values of \\(\\xi_i\\) : \\[ \\operatorname{minimize} \\frac{1}{2}\\|\\boldsymbol{\\theta}\\|_2^2+\\lambda \\sum_{i=1}^n \\xi_i \\quad \\text { subject to } \\quad y_i\\left(\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle-b\\right) \\geq 1-\\xi_i, \\quad \\xi_i \\geq 0 \\text {, } \\] where we optimize over \\(\\boldsymbol{\\theta}, b\\) , and the \\(\\xi_i\\) 's. We next show that this optimization problem can be expressed equivalently as the following unconstrained optimization problem \\[ \\operatorname{minimize} \\frac{1}{2}\\|\\boldsymbol{\\theta}\\|_2^2+\\lambda \\sum_{i=1}^n \\max \\left(1-y_i\\left(\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle-b\\right), 0\\right) . \\] if \\(1-y_i\\left(\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle-b\\right)\\) <0: <==> support vectors, no slack needed otherwise: need slack, and the slack variable should also be as small as possible Lagrangian dualitz and SVMs In order to gain additional insight into the optimization problem, and to see that the SVM algorithm can be written in terms of inner products between input feature vectors (which will turn out to be useful later), we next discuss duality theory, an important concept in optimization. This part is only provided as extra reading material, it won't be part of the exam. Feel free to skip this subsection, and continue with the \"Kernels\" section. Langrangian dualitz and SVMs Hard margin SVMs in the dual space","title":"Support vector machines"},{"location":"Supervised%20Learning/2%20Classification/2.2%20Support%20Vector%20Machine/#support-vector-machines","text":"Support vector machines are a class of model for classification. We already learned about a logistic regression as another model for classification. In logistic regression we learn a parameter vector \\(\\boldsymbol{\\theta}\\) and classify an example as belonging to one of two (or \\(k\\) ) classes based on a decision boundary. Specifically, in logistic regression, we classified an example as belonging to class one if the inner product \\(\\langle\\boldsymbol{\\theta}, \\mathbf{x}\\rangle\\) is larger than a threshold. In a similar spirit, Support Vector Machines (SVMs) learn decision boundaries. Contrary to our previous discussion on logistic regression, for SVMs we will not make any distributional assumption on the data. logistic regression assumes the data follow a Benouri distribution As before, we consider a binary classification problem. Given a training dataset \\[ \\mathcal{D}=\\left\\{\\left(\\mathbf{x}_1, y_1\\right), \\ldots,\\left(\\mathbf{x}_n, y_n\\right)\\right\\} \\in \\mathbb{R}^d \\times\\{-1,1\\}, \\] our goal is to find a classifier based on separating the two classes by a hyperplane. A classifier \\(f\\) based on a hyperplane classifies a feature \\(\\mathbf{x}\\) as 1 if \\(\\langle\\boldsymbol{\\theta}, \\mathbf{x}\\rangle \\geq b\\) , where \\(b\\) is a threshold, and as \\(-1\\) otherwise, i.e., $$ f(\\mathbf{x})= \\begin{cases}1 & \\langle\\boldsymbol{\\theta}, \\mathbf{x}\\rangle \\geq b \\ -1 & \\langle\\boldsymbol{\\theta}, \\mathbf{x}\\rangle<b .\\end{cases} $$ In logistic regression, the threshold \\(b\\) is 0","title":"Support vector machines"},{"location":"Supervised%20Learning/2%20Classification/2.2%20Support%20Vector%20Machine/#the-perceptron-algorithm","text":"As a first step towards SVMs, we discuss the perceptron algorithm. The perceptron algorithm finds a hyperplane that perfectly separates the \\(+1\\) 's from the \\(-1\\) 's. The goal of the perceptron algorithm is to find a parameter vector \\(\\boldsymbol{\\theta}\\) such that for all \\(i\\) , $$ \\begin{cases}\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle \\geq b & \\text { if } y_i=1, \\ \\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle<b & \\text { if } y_i=-1 .\\end{cases} $$ Such a parameter vector \\(\\boldsymbol{\\theta}\\) does not necessarily exist: if it exists, we say that the data is linearly separable. This restriction to data that is linearly separable is a major issue limiting the applicability of perceptrons in its original form in practice, and we will discuss less restrictive SVMs later. Even if the data is linearly separable, then a parameter vector \\(\\boldsymbol{\\theta}\\) and a threshold \\(b\\) are not necessarily unique for separating the \\(+1\\) 's from the \\(-1\\) 's, thus it is not clear which hyperplane to choose.","title":"The perceptron algorithm"},{"location":"Supervised%20Learning/2%20Classification/2.2%20Support%20Vector%20Machine/#hard-margin-svms","text":"Suppose that the dataset \\(\\mathcal{D}\\) is is linearly separable. Then there might be different hyperplanes that separate the data. The concept of a margin enables us to distinguish between \"good\" and \"bad\" separating hyperplanes. Instead of finding an arbitrary hyperplane that separates the two classes, our goal is to find the particular hyperplane that maximizes the distance from the decision boundary to the training points. In more detail, we want to maximize a margin \\(m \\geq 0\\) such that i) all points classified as \\(+1\\) are on the positive side of the hyperplane, and their distance to the hyperplane is at least \\(m\\) , and ii) all points classified as \\(-1\\) are on the negative side of the hyperplane, and their distance to the hyperplane is at least \\(m\\) . To find such a hyperplane, we first note that the distance of a point \\(\\mathrm{x}\\) to a hyperplane. \\[ \\mathcal{H}=\\{\\mathbf{x}:\\langle\\boldsymbol{\\theta}, \\mathbf{x}\\rangle=b\\} \\] is given by \\[ \\frac{\\left|\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}-\\mathbf{x}_0\\right\\rangle\\right|}{\\|\\boldsymbol{\\theta}\\|_2}=\\frac{|\\langle\\boldsymbol{\\theta}, \\mathbf{x}\\rangle-b|}{\\|\\boldsymbol{\\theta}\\|_2} . \\] This can be seen as follows. By definition, the vector \\(\\boldsymbol{\\theta}\\) is perpendicular to the hyperplane \\(\\mathcal{H}\\) , i.e., to any vector that lies on the hyperplane. Specifically, consider any two points \\(\\mathbf{x}_0, \\mathbf{x}_1 \\in \\mathcal{H}\\) , and note that the vectors \\(\\mathbf{x}_1-\\mathbf{x}_0\\) , which lies on the hyperplane, is orthogonal to \\(\\boldsymbol{\\theta}\\) : $$ \\left\\langle\\mathbf{x}_1-\\mathbf{x}_0, \\boldsymbol{\\theta}\\right\\rangle=\\left\\langle\\mathbf{x}_1, \\boldsymbol{\\theta}\\right\\rangle-\\left\\langle\\mathbf{x}_0, \\boldsymbol{\\theta}\\right\\rangle=b-b=0 . $$ As a consequence of \\(\\boldsymbol{\\theta}\\) being perpendicular to \\(\\mathcal{H}\\) , the shortest distance of a point \\(\\mathbf{x}\\) to \\(\\mathcal{H}\\) is given by the projection of \\(\\mathrm{x}-\\mathrm{x}_0\\) onto \\(\\boldsymbol{\\theta}\\) , where \\(\\mathrm{x}_0 \\in \\mathcal{H}\\) . Thus, the distance of the point to the hyperplane is $$ \\frac{\\left|\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}-\\mathbf{x}_0\\right\\rangle\\right|}{|\\boldsymbol{\\theta}|_2}=\\frac{|\\langle\\boldsymbol{\\theta}, \\mathbf{x}\\rangle-b|}{|\\boldsymbol{\\theta}|_2} . $$ Now let's return to the problem of finding a hyperplane that maximizes the margin. All datapoints are classified as \\(+1(-1)\\) are on the positive (negative) side of the hyperplane, and their distance to the hyperplane is at least \\(m\\) , if for all \\(i=1, \\ldots, n\\) $$ y_i \\frac{\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle-b}{|\\boldsymbol{\\theta}|_2} \\geq m $$ This leads to the optimization problem: $$ \\max _{m, \\boldsymbol{\\theta}, b} m \\quad \\text { subject to } y_i \\frac{\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle-b}{|\\boldsymbol{\\theta}|_2} \\geq m, i=1, \\ldots, n \\text {. } $$ Here, we maximize over the parameters \\(b, \\boldsymbol{\\theta}\\) , and \\(m\\) . The margin \\(m\\) is maximized if there exists at least one point on the negative side, and at least one on the positive side whose distance to the hyperplane is equal to \\(m\\) ; these points are called the support vectors . We next simplify the optimization problem by removing the variable \\(m\\) . - Note that the solution of the optimization problem is invariant to scaling \\(\\boldsymbol{\\theta}\\) and \\(b\\) by a positive scalar. Therefore, we can fix the norm of \\(\\boldsymbol{\\theta}\\) to a particular value without changing the optimization problem. - Setting \\(\\|\\boldsymbol{\\theta}\\|_2=1 / m\\) , the optimization problem becomes \\[ \\min _{\\boldsymbol{\\theta}, b} \\frac{1}{2}\\|\\boldsymbol{\\theta}\\|_2^2 \\quad \\text { subject to } \\quad y_i\\left(\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle-b\\right) \\geq 1 . \\] This is the standard formulation of the hard-margin SVM optimization problem.","title":"Hard-margin SVMs"},{"location":"Supervised%20Learning/2%20Classification/2.2%20Support%20Vector%20Machine/#soft-margin-svms","text":"The hard-margin SVM optimization problem only has a solution if the data is linearly separable. Even if the data is linearly separable, the solution of the hard-margin SVM optimization problem can vary significantly when including or excluding single points that change the support vectors. Soft-margin SVMs address these issues by allowing points to violate the margin. The idea is to introduce a slack variable \\(\\xi_i\\) for each data point, which allows points to violate the margin : \\[ y_i\\left(\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle-b\\right) \\geq 1-\\xi_i, \\quad \\xi_i \\geq 0 . \\] We take the slack variable in the objective into account by penalizing large values of \\(\\xi_i\\) : \\[ \\operatorname{minimize} \\frac{1}{2}\\|\\boldsymbol{\\theta}\\|_2^2+\\lambda \\sum_{i=1}^n \\xi_i \\quad \\text { subject to } \\quad y_i\\left(\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle-b\\right) \\geq 1-\\xi_i, \\quad \\xi_i \\geq 0 \\text {, } \\] where we optimize over \\(\\boldsymbol{\\theta}, b\\) , and the \\(\\xi_i\\) 's. We next show that this optimization problem can be expressed equivalently as the following unconstrained optimization problem \\[ \\operatorname{minimize} \\frac{1}{2}\\|\\boldsymbol{\\theta}\\|_2^2+\\lambda \\sum_{i=1}^n \\max \\left(1-y_i\\left(\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle-b\\right), 0\\right) . \\] if \\(1-y_i\\left(\\left\\langle\\boldsymbol{\\theta}, \\mathbf{x}_i\\right\\rangle-b\\right)\\) <0: <==> support vectors, no slack needed otherwise: need slack, and the slack variable should also be as small as possible","title":"Soft-margin SVMs"},{"location":"Supervised%20Learning/2%20Classification/2.2%20Support%20Vector%20Machine/#lagrangian-dualitz-and-svms","text":"In order to gain additional insight into the optimization problem, and to see that the SVM algorithm can be written in terms of inner products between input feature vectors (which will turn out to be useful later), we next discuss duality theory, an important concept in optimization. This part is only provided as extra reading material, it won't be part of the exam. Feel free to skip this subsection, and continue with the \"Kernels\" section.","title":"Lagrangian dualitz and SVMs"},{"location":"Supervised%20Learning/2%20Classification/2.2%20Support%20Vector%20Machine/#langrangian-dualitz-and-svms","text":"","title":"Langrangian dualitz and SVMs"},{"location":"Supervised%20Learning/2%20Classification/2.2%20Support%20Vector%20Machine/#hard-margin-svms-in-the-dual-space","text":"","title":"Hard margin SVMs in the dual space"}]}