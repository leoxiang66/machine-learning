# Nonlinear features and kernels
For all the supervised learning learning models we saw so far, the prediction is **linear** in feature vector. Specifically, for ridge regression (of which linear regression is a special case), we compute a model parameter vector $\hat{\boldsymbol{\theta}}$ from training data, and predict a response for a feature vector $\mathbf{x}$ as $y=\hat{\boldsymbol{\theta}}^T \mathbf{x}$. For binary classification with logistic regression and SVMs, we learn a vector $\hat{\boldsymbol{\theta}}$ and a threshold $b$ based on training data, and predict label $+1$ for the feature vector $\mathbf{x}$ if $\hat{\boldsymbol{\theta}}^T \mathbf{x} \geq b$, and predict $-1$ otherwise. Thus, those models learn a linear predictor; for classification this results in a linear decision boundary (i.e., the boundary that separates the $+1$ 's from the $-1$ 's is a plane).


We can however, easily learn non-linear decision boundaries, by working with **transformed features**. For example, for a feature vector $\mathbf{x} \in \mathbb{R}^d$ containing a single feature (i.e., $d=1$ ), we can define the *feature map*
$$
\phi(\mathbf{x})=\left[1, x, x^2, x^3\right]^T,
$$
and then apply our learning algorithms to the transformed feature vectors, both for training and prediction. Specifically, we can work with the vectors $\mathbf{x}^{\prime}=\phi(\mathbf{x})$ instead of the vectors $\mathbf{x}$. 

<u>That's all there is to know conceptually.</u> However, working with the feature maps can be computationally very expensive, since the feature maps can be very high-dimensional vectors. Therefore, for the rest of this section, we discuss kernels, which enable working efficiently with feature maps.

## Linear estimators
We start with emphasizing a common pattern in the supervised learning problems we have seen so far. In the problems we have considered, we fit a model $h_{\boldsymbol{\theta}}$ parameterized by a vector $\boldsymbol{\theta}$ by minimizing the regularized empirical risk

$$
\arg \min _{\boldsymbol{\theta}} J(\boldsymbol{\theta}), \quad J(\boldsymbol{\theta})=\frac{1}{n} \sum_{i=1}^n \operatorname{loss}\left(h_{\boldsymbol{\theta}}\left(\mathbf{x}_i\right), y_i\right)+r(\boldsymbol{\theta})
$$
> Problem (1)

Here, $h_{\boldsymbol{\theta}}: \mathbb{R}^d \rightarrow \mathcal{Y}$ and $\mathcal{Y}=\{1,-1\}$ for binary classification and $\mathcal{Y}=\mathbb{R}$ for regression problems, and $r$ is a regularizer.

<u>For binary and multi-class classification as well as regression, many classifiers and regressors are based on computing the inner product between a parameter vector and a feature vector,</u> i.e., $\langle\boldsymbol{\theta}, \mathbf{x}\rangle$. For those classifiers and regressors, both the loss and the prediction made by the classifier or regressor only depend on the feature vectors $\mathbf{x}_i$ through the inner products $\left\langle\boldsymbol{\theta}, \mathbf{x}_i\right\rangle$. Let us consider some examples. In the following, we formulate the loss as a function of the inner product $\langle\boldsymbol{\theta}, \mathbf{x}\rangle$, since we consider algorithms where the loss only depends on the model parameter $\boldsymbol{\theta}$ and the feature vectors $\mathbf{x}$ through the inner product $\langle\boldsymbol{\theta}, \mathbf{x}\rangle$. Specifically, we let

$$
\operatorname{loss}\left(h_{\boldsymbol{\theta}}(\mathbf{x}), y\right)=\operatorname{loss}(\langle\boldsymbol{\theta}, \mathbf{x}\rangle, y),
$$

with a slight abuse of notation. The loss function on the right hand side maps two real numbers to another real number, i.e., loss: $\mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$.

- For logistic regression, the loss can be formulated as a function of the inner products $\langle\boldsymbol{\theta}, \mathbf{x}\rangle$ as
$$
\operatorname{loss}(\langle\boldsymbol{\theta}, \mathbf{x}\rangle, y)=\log \left(1+e^{-y\langle\boldsymbol{\theta}, \mathbf{x}\rangle}\right)
$$
Thus, logistic regression fits into the regularized empirical risk minimization framework above with $r(\boldsymbol{\theta})=0$. Moreover, we classify a given feature vector $\mathbf{x}$ as belonging to class 1 if $\langle\boldsymbol{\theta}, \mathbf{x}\rangle>0$, and to class $-1$ otherwise.
- For ridge and linear regression, we used the squared error
$$
\operatorname{loss}(\langle\boldsymbol{\theta}, \mathbf{x}\rangle, y)=(\langle\boldsymbol{\theta}, \mathbf{x}\rangle-y)^2,
$$
and predicted a value associated with feature vector $\mathbf{x}$ as $\langle\boldsymbol{\theta}, \mathbf{x}\rangle$. In addition, we choose the regularizer as $r(\boldsymbol{\theta})=\frac{\lambda}{2}\|\boldsymbol{\theta}\|_2^2$.
- Support vector machines minimize the loss
$$
\operatorname{loss}(\langle\boldsymbol{\theta}, \mathbf{x}\rangle, y)=\max (1-y(\langle\boldsymbol{\theta}, \mathbf{x}\rangle-b), 0),
$$
and the regularizer is $r(\boldsymbol{\theta})=\frac{1}{2}\|\boldsymbol{\theta}\|_2^2$.
Recall that for ridge regression, the regularizer can be interpreted under a Bayesian viewpoint as a Gaussian prior over the feature vector $\boldsymbol{\theta}$. For support vector machines, we derived the regularized loss function based on the idea of maximizing the margin.

In general, regularization is useful for several reasons. It can make the problem of minimizing the empirical risk easier to solve numerically, and it can also lead to a better model, i.e., a model that generalizes better to unseen data or in other words has a smaller prediction or classification error.

## The representer theorem
Another common pattern of logistic regression, ridge regression, and SVMs, is that each method has a solution $\hat{\boldsymbol{\theta}}$ of the minimization problem (1) that obeys
$$
\hat{\boldsymbol{\theta}}=\sum_{i=1}^n \alpha_i \mathbf{x}_i,
$$
for some real valued parameters $\alpha_i$. For SVMs this can be shown by analyzing the dual problem, but we will see a simpler argument below. For ridge regression this follows directly from the closed form solution of the ridge regression optimization problem. It turns out that this property of a minimizer of the regularized empirical loss holds more generally.

**Theorem 1** (Representer theorem). Consider the regularized empirical risk minimization problem in (1), and assume that the regularizer $r$ has the form $r\left(\|\boldsymbol{\theta}\|_2\right)$, where $r: \mathbb{R} \rightarrow \mathbb{R}$ is a non-decreasing function, and $h_{\boldsymbol{\theta}}(\mathbf{x})=\langle\boldsymbol{\theta}, \mathbf{x}\rangle$. Also suppose that the loss $\operatorname{loss}(z, y)$ is convex in $z$. Then there exists a minimizer of (1) that can be written as

$$
\hat{\boldsymbol{\theta}}=\sum_{i=1}^n \alpha_i \mathbf{x}_i
$$

where $\alpha_i \in \mathbb{R}, i=1, \ldots, n$ are some coefficients.

Before providing a proof of the theorem, note that we can take $r=0$, and observe that the three examples above (ridge regression, logistic regression, and SVMs) all obey the assumptions of the theorem.

> Proof. To provide intuition, we only provide a proof for the special case that loss $(z, y)$ is differentiable with respect to $z$, and $r(\boldsymbol{\theta})=\frac{\lambda}{2}\|\boldsymbol{\theta}\|_2^2$ with $\lambda>0$. In this case the regularized empirical loss $J(\boldsymbol{\theta})$ is strictly convex, since $r(\boldsymbol{\theta})=\frac{\lambda}{2}\|\boldsymbol{\theta}\|_2$ is strictly convex. It therefore has a unique minimizer $\hat{\boldsymbol{\theta}}$ that obeys $\nabla J(\hat{\boldsymbol{\theta}})=0$.

> Let $\operatorname{loss}^{\prime}(z, y)$ be the derivative of the loss with respect to the first variable, $z$. By the chain rule, the gradient of the loss with respect to $\boldsymbol{\theta}$ obeys

> $$
\nabla \operatorname{loss}(\langle\boldsymbol{\theta}, \mathbf{x}\rangle, y)=\operatorname{loss}^{\prime}(\langle\boldsymbol{\theta}, \mathbf{x}\rangle, y) \mathbf{x}
$$

> and the gradient of $r(\boldsymbol{\theta})$ is given by $\lambda \boldsymbol{\theta}$. Setting the gradient of the regularized empirical loss equal to zero, we thus find that the minimizer $\hat{\boldsymbol{\theta}}$ obeys

> $$
0=\nabla J(\hat{\boldsymbol{\theta}})=\frac{1}{n} \sum_{i=1}^n \operatorname{loss}^{\prime}\left(\left\langle\hat{\boldsymbol{\theta}}, \mathbf{x}_i\right\rangle, y_i\right) \mathbf{x}_i+\lambda \hat{\boldsymbol{\theta}}
$$

> Thus the minimizer $\hat{\boldsymbol{\theta}}$ obeys

> $$
\hat{\boldsymbol{\theta}}=\sum_{i=1}^n-\frac{1}{n \lambda} \operatorname{loss}^{\prime}\left(\left\langle\hat{\boldsymbol{\theta}}, \mathbf{x}_i\right\rangle, y_i\right) \mathbf{x}_i
$$

> and setting $\alpha_i=-\frac{1}{n \lambda} \operatorname{loss}^{\prime}\left(\left\langle\hat{\boldsymbol{\theta}}, \mathbf{x}_i\right\rangle, y_i\right)$ completes the proof. Note that $\alpha_i$ is a real number (that depends on $\hat{\boldsymbol{\theta}}$, but that is irrelevant for the statement).

## Nonlinear features and kernels
From the representer theorem we see that we can write a minimizer of the empirical risk $J(\boldsymbol{\theta})$ as a linear combination of the training feature vectors $\left\{\mathbf{x}_i\right\}$, i.e., $\hat{\boldsymbol{\theta}}=\sum_{i=1}^n \alpha_i \mathbf{x}_i$. This means that we can make a prediction for $\mathbf{x}$ by computing
$$
\langle\boldsymbol{\theta}, \mathbf{x}\rangle=\sum_{i=1}^n \alpha_i\left\langle\mathbf{x}, \mathbf{x}_i\right\rangle .
$$
It also means that we can replace all appearances of $\langle\boldsymbol{\theta}, \mathbf{x}\rangle$ with $\sum_{i=1}^n \alpha_i\left\langle\mathbf{x}, \mathbf{x}_i\right\rangle$ and minimize over the coefficients $\left\{\alpha_i\right\}$, instead of minimizing over the coefficient vector $\boldsymbol{\theta}$. Thus, the entire algorithm (training and prediction) can be written in terms of inner products of feature vectors.

This is useful for learning non-linear decision boundaries in classification, or in general work with transformed features. Recall that in our previous discussion on regression we were fitting a (higher-order) polynomial instead of a linear function. To do so, we took our original features and transformed them. For example, for a feature vector $\mathbf{x}$ with a single feature, we could map it to
$$
\phi(\mathbf{x})=\left[1, x, x^2, x^3\right]^T,
$$
and then apply our learning algorithms to the transformed feature vector. The function $\phi$ mapping the original feature space to a higher dimensional feature space is called a feature map.

Rather than applying our learning method (SVMs, logistic regression, etc) to the original feature vector space, we can first transform the features with a feature map $\phi: \mathbb{R}^d \rightarrow \mathbb{R}^k$. Instead of applying the algorithm to the original training set, we instead apply it to the set $\left\{\left(\phi\left(\mathbf{x}_i\right), y_i\right)\right\}_{i=1}^n$. <mark>This idea is in general applicable to learning algorithms that can be written in terms of inner products of the feature vectors.</mark> We then simply replace all those inner products by inner products between the feature maps $\left\langle\phi(\mathbf{x}), \phi\left(\mathbf{x}^{\prime}\right)\right\rangle$. We associate a kernel with a feature map $\phi$, defined as
$$
K\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\left\langle\phi(\mathbf{x}), \phi\left(\mathbf{x}^{\prime}\right)\right\rangle .
$$
Recall that the regularized empirical loss, associated with the transformed features is, for $r(\boldsymbol{\theta})=$ $\frac{\lambda}{2}\|\boldsymbol{\theta}\|_2:$

$$
J(\boldsymbol{\theta})=\frac{1}{n} \sum_{i=1}^n \operatorname{loss}\left(\left\langle\boldsymbol{\theta}, \phi\left(\mathbf{x}_i\right)\right\rangle, y_i\right)+\frac{\lambda}{2}\|\boldsymbol{\theta}\|_2^2 .
$$
Using the representer theorem, we get

$$
\begin{aligned}
J(\boldsymbol{\alpha}) &=\frac{1}{n} \sum_{i=1}^n \operatorname{loss}\left(\left\langle\boldsymbol{\theta}, \phi\left(\mathbf{x}_i\right)\right\rangle, y_i\right)+\frac{\lambda}{2}\|\boldsymbol{\theta}\|_2^2 \\
&=\frac{1}{n} \sum_{i=1}^n \operatorname{loss}\left(\left\langle\sum_{j=1}^n \alpha_j \phi\left(\mathbf{x}_j\right), \phi\left(\mathbf{x}_i\right)\right\rangle, y_i\right)+\frac{\lambda}{2}\left\|\sum_{i=1}^n \alpha_i \phi\left(\mathbf{x}_i\right)\right\|_2^2 \\
&=\frac{1}{n} \sum_{i=1}^n \operatorname{loss}\left(\sum_{j=1}^n \alpha_j K\left(\mathbf{x}_j, \mathbf{x}_i\right), y_i\right)+\frac{\lambda}{2} \sum_{i, j} \alpha_i \alpha_j K\left(\mathbf{x}_i, \mathbf{x}_j\right) .
\end{aligned}
$$

Thus, the entire loss function only depends on the feature vectors through the kernel $K\left(\mathbf{x}_i, \mathbf{x}_j\right), i, j=$ $1, \ldots, n$.

We can now obtain the optimal coefficients by minimizing the empirical risk $J(\boldsymbol{\alpha})$ over the coefficients $\boldsymbol{\alpha}$, for example with gradient descent. Note that to minimize the risk, we only require the values of the kernel $K\left(\mathbf{x}_i, \mathbf{x}_j\right)$ for all pairs of feature vectors $i, j$.

Given the resulting coefficients $\hat{\boldsymbol{\alpha}}$, we can make a prediction again only based on kernel computations. Specifically, a prediction is made by computing $\hat{\boldsymbol{\theta}}^T \mathbf{x}$ (for regression, the prediction is $\hat{\boldsymbol{\theta}}^T \mathbf{x}$, and for classification it is 1 if $\hat{\boldsymbol{\theta}}^T \mathbf{x}>0$, and $-1$ else). Again we do that based on kernel evaluations only, specifically we compute
$$
\hat{\boldsymbol{\theta}}^T \mathbf{x}=\sum_{i=1}^n \alpha_i K\left(\mathbf{x}_i, \mathbf{x}\right)
$$





The idea of transforming the features with a feature map, and not applying the algorithm explicitly to the higher-dimensional features, but instead replacing all inner products between features with inner products between the transformed features, is often referred to as the "kernel trick".

The concept of a kernel is important, since we typically do not use the feature map explicitly, instead we work with the kernel, which can often be computed much more efficiently than transforming the original features and computing the inner product $\left\langle\phi(\mathbf{x}), \phi\left(\mathbf{x}^{\prime}\right)\right\rangle$ directly. Let us consider two examples.

- Polynomial kernel. Consider the feature map $\phi: \mathbb{R}^d \rightarrow \mathbb{R}^{d^2}$, defined as
$$
\phi(\mathbf{x})=\left[x_1 x_1, x_1 x_2, \ldots, x_1 x_d, x_2 x_1, \ldots, x_2 x_d, \ldots, x_d x_d\right]^T .
$$
With this definition, we have that
$$
\begin{aligned}
K\left(\mathbf{x}, \mathbf{x}^{\prime}\right) &=\sum_{i, j=1}^d x_i x_j x_i^{\prime} x_j^{\prime} \\
&=\left(\sum_{i=1}^d x_i x_i^{\prime}\right)\left(\sum_{j=1}^d x_j x_j^{\prime}\right) \\
&=\left(\left\langle\mathbf{x}, \mathbf{x}^{\prime}\right\rangle\right)^2
\end{aligned}
$$
Note that direct computation of the inner product $\left\langle\phi(\mathbf{x}), \phi\left(\mathbf{x}^{\prime}\right)\right\rangle$ has computational complexity $O\left(d^2\right)$, whereas computing $K\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$ via the equation above only has computational complexity $O(d)$.
A related kernel is
$$
K\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\left(\left\langle\mathbf{x}, \mathbf{x}^{\prime}\right\rangle+c\right)^2 .
$$
This kernel contains linear and quadratic features, and $c$ weights the linear contributions.
- Gaussian kernel. Another popular kernel is the Gaussian kernel:
$$
K\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=e^{-\frac{\left\|\mathbf{x}-\mathbf{x}^{\prime}\right\|_2^2}{2 \sigma^2}} .
$$
By Taylor's expansion
$$
e^x=1+x+\ldots+\frac{1}{k !} x^k, \ldots,
$$
we see that $e^{-\frac{\left\|\mathbf{x}-\mathbf{x}^{\prime}\right\|_2^2}{2 \sigma^2}}$ is a kernel associated with an infinite set of features corresponding to the polynomial terms.

Another way to think about kernels is to view them as similarity measures. In that view, large values of $K\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$ correspond to similar features, and small values to different features.

Suppose we come up with a kernel, based on some intuition of the data. We next discuss a condition on a kernel to have a feature map associated with it. In the following, we say a kernel is valid if there exists a function $\phi$ such that $K\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\left\langle\phi(\mathbf{x}), \phi\left(\mathbf{x}^{\prime}\right)\right\rangle$, for all $\mathbf{x}, \mathbf{x}^{\prime}$.

Consider any $n$ points (not necessarily the training set) $\mathbf{x}_1, \ldots, \mathbf{x}_n$, and let $\mathbf{K}$ be the $n \times n$ square matrix with entries
$$
[\mathbf{K}]_{i j}=K\left(\mathbf{x}_i, \mathbf{x}_j\right), i, j=1, \ldots, n .
$$
We call this the kernel matrix. If a kernel is valid, then the matrix $\mathbf{K}$ must be symmetric, due to
$$
[\mathbf{K}]_{i j}=K\left(\mathbf{x}_i, \mathbf{x}_j\right)=[\mathbf{K}]_{j i}
$$
Moreover, it is positive semidefinite, since for any $\mathbf{z}$, we have that
$$
\begin{aligned}
\mathbf{z}^T \mathbf{K z} &=\mathbf{z}^T\left[\phi\left(\mathbf{x}_1\right), \ldots, \phi\left(\mathbf{x}_n\right)\right]^T\left[\phi\left(\mathbf{x}_1\right), \ldots, \phi\left(\mathbf{x}_n\right)\right] \mathbf{z} \\
&=\left\|\left[\phi\left(\mathbf{x}_1\right), \ldots, \phi\left(\mathbf{x}_n\right)\right] \mathbf{z}\right\|_2^2 \\
& \geq 0
\end{aligned}
$$
The following theorem, due to Mercer, states that positive definiteness of $\mathbf{K}$ is necessary for $K$ to be a valid kernel.

**Theorem 2** (Mercer). The kernel $K: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$ is valid, i.e., there exists a function $\phi$ such that $K\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\left\langle\phi(\mathbf{x}), \phi\left(\mathbf{x}^{\prime}\right)\right\rangle$, for all $\mathbf{x}, \mathbf{x}^{\prime}$, if and only if for any set of points $\mathbf{x}_1, \ldots, \mathbf{x}_m, m<\infty$, the associated kernel matrix $\mathbf{K}$ is symmetric and positive semidefinite.


[Kernels & Kernel Regression](https://www.wolai.com/leoxiang66/cHbbFV8g4WFU9SF9CQAfT1)



## Support vectors machines with a Gaussian kernel

In order to understand kernels better, let us consider a concrete example. Suppose we train a support vector machine with a Gaussian kernel
$$
K\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=e^{-\gamma\left\|\mathbf{x}-\mathbf{x}^{\prime}\right\|_2^2}, \quad \mathbf{x}, \mathbf{x}^{\prime} \in \mathbb{R}^d,
$$
where $\gamma>0$ is a parameter of the kernel. If $\gamma$ is very large, then a support vector machine with a Gaussian kernel implements a nearest neighbor classifier, i.e., a classifier that classifies an example $\mathrm{x}$ as belonging to the same class as the closest point in the training set, i.e.,
$$
y=y_i, i \in \arg \min _i\left\|\mathbf{x}-\mathbf{x}_i\right\|_2^2 .
$$
To see this empirically, consider the numerical example at: [https://scikit-learn.org/stable/auto_examples/exercises/plot_iris_exercise.html](https://scikit-learn.org/stable/auto_examples/exercises/plot_iris_exercise.html), and observe how the decision boundary changes as you make $\gamma$ larger (for example, compare $\gamma=1$ to $\gamma=100)$.

In the following, we will verify this observation directly and using duality. We consider support vector machines with $b=0$, for simplicity.

Reasoning without duality: Let $\phi$ be a feature map, and suppose we apply SVMs to features transformed with the feature map. Recall that SVMs fits a model by solving the following empirical risk minimization problem:

$$
\hat{\boldsymbol{\theta}}=\arg \min _{\boldsymbol{\theta}} \frac{1}{n} \sum_{i=1}^n \max \left(1-y_i\left(\left\langle\boldsymbol{\theta}, \phi\left(\mathbf{x}_i\right)\right\rangle\right), 0\right)+\frac{\lambda}{2}\|\boldsymbol{\theta}\|_2^2 .
$$

Once we trained the method, we classify a new example $\mathbf{x}$ as

$$
\hat{y}(\mathbf{x})= \begin{cases}1, & \langle\boldsymbol{\theta}, \phi(\mathbf{x})\rangle>0 \\ -1, & \text { else. }\end{cases}
$$

Let $K$ be a kernel with associated feature map $\phi$. Since by the representer theorem the estimate $\hat{\boldsymbol{\theta}}$ defined in equation (2) obeys $\hat{\boldsymbol{\theta}}=\sum_{i=1}^n \alpha_i \phi\left(\mathbf{x}_i\right)$, we can use this expression for $\hat{\theta}$ and minimize over the $\alpha_i$ 's instead, i.e., we fit a model by minimizing

$$
\begin{aligned}
J(\boldsymbol{\alpha}) &=\frac{1}{n} \sum_{i=1}^n \max \left(1-y_i\left(\sum_{j=1}^n \alpha_j\left\langle\phi\left(\mathbf{x}_j\right), \phi\left(\mathbf{x}_i\right)\right\rangle\right), 0\right)+\frac{\lambda}{2} \sum_{i, j}^n \alpha_i \alpha_j\left\langle\phi\left(\mathbf{x}_j\right), \phi\left(\mathbf{x}_i\right)\right\rangle \\
&=\frac{1}{n} \sum_{i=1}^n \max \left(1-y_i\left(\sum_{j=1}^n \alpha_j K\left(\mathbf{x}_i, \mathbf{x}_j\right)\right), 0\right)+\frac{\lambda}{2} \sum_{i, j}^n \alpha_i \alpha_j K\left(\mathbf{x}_i, \mathbf{x}_j\right) \\
&=J(\alpha) .
\end{aligned}
$$

Now, suppose that $\gamma$ is very large, say it tends to $\infty$. Then $K\left(\mathbf{x}_i, \mathbf{x}_i\right)=1$ and $K\left(\mathbf{x}_i, \mathbf{x}_j\right)=0$ for $\mathbf{x}_i \neq \mathbf{x}_j$. With this, the regularized empirical loss becomes (assuming that $\mathbf{x}_i \neq \mathbf{x}_j$ for $j \neq i$ ):

$$
J(\boldsymbol{\alpha})=\frac{1}{n} \sum_{i=1}^n \max \left(1-y_i \alpha_i, 0\right)+\frac{\lambda}{2} \sum_{i=1}^n \alpha_i^2 K\left(\mathbf{x}_i, \mathbf{x}_i\right) .
$$

If $\lambda$ is large, a trivial solution is $\alpha_i=0$. This is not an interesting regime, the regime of relevance is when $\lambda$ is relatively small and the first term, corresponding to the empirical loss dominates. Then, the optimal solution is $y_i=\alpha_i$, which minimizes the empirical loss. With this choice of the $\alpha_i$ 's, an example $\mathbf{x}$ is classified as

$$
\begin{aligned}
\hat{y} &=\operatorname{sign}\left(\sum_{i=1}^n \alpha_i K\left(\mathbf{x}_i, \mathbf{x}\right)\right) . \\
&=\operatorname{sign}\left(\sum_{i=1}^n y_i K\left(\mathbf{x}_i, \mathbf{x}\right)\right) .
\end{aligned}
$$

For large $\gamma$, the closest point in the training set will have the largest impact in the sum above and thus the corresponding label dominates. Therefore, for large $\gamma$, this classifier behaves as a nearest neighbor classifier.

**Reasoning with duality**: The following is extra material. The optimal $\boldsymbol{\theta}$ can be obtained as $\boldsymbol{\theta}=\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i$ by solving the dual problem

$$
\max _\alpha \sum_i \alpha_i-\frac{1}{2} \sum_{i, j} \alpha_i \alpha_j y_i y_j K\left(\mathbf{x}_i, \mathbf{x}_j\right) \text { subject to } \alpha_i \geq 0 \text {, and } \sum_i \alpha_i y_i=0 \text {. }
$$

Suppose that $\gamma$ is very large. Then, $K\left(\mathbf{x}, \mathbf{x}^{\prime}\right) \approx 0$ if $\mathbf{x} \neq \mathbf{x}^{\prime}$ and as a consequence, the optimization problem reduces to

$$
\max _\alpha \sum_i \alpha_i-\frac{1}{2} \alpha_i^2 \text { subject to } \alpha_i \geq 0 \text {, and } \sum_i \alpha_i y_i=0 \text {. }
$$

Now also suppose for simplicity that half of the $y_i$ are equal to 1 and the other half are equal to $-1$. Then the solution to the optimization problem above is $\alpha_i=1$ for all $i$. In order to classify a new example $\mathbf{x}$, what the method does is it computes the inner product

$$
\langle\boldsymbol{\theta}, \mathbf{x}\rangle=\sum_{i=1}^n \alpha_i y_i K\left(\mathbf{x}_i, \mathbf{x}\right)=\sum_{i=1}^n y_i K\left(\mathbf{x}_i, \mathbf{x}\right)
$$

and checks whether this is larger than zero or not, and if it is, it predicts class one. Recall that we are considering a situation where $\gamma$ is really large. Then there is one term in the sum above that will dominate all the others, and that term is the term $y_i K\left(\mathbf{x}_i, \mathbf{x}\right)$ corresponding $\mathbf{x}_i$ that is closest to $\mathbf{x}$. The SVM will then predict the label corresponding to the training example $\mathbf{x}_i$ that is closest to $\mathbf{x}$. Thus, for the extreme case where $\gamma \rightarrow \infty$, all an SVM with a Gaussian kernel is doing is predicting the label of the training point closest to $\mathrm{x}$.