# Logistic Regression
In this lecture, we start discussing classification problems. In a classification problem, our goal is, given a feature vector $\mathbf{x}$, to predict its label $y$, which can only take on a small set of discrete values. A special case is binary classification, where the label $y$ only takes on two values, 0 and 1 for example. Most applications of machine learning are classification problems. A concrete classical example of a classification problem is spam filtering, where the vector $\mathbf{x}$ contains features of an email, and $y=1$ stands for spam and $y=0$ stands for not spam.

Formally, the classification problem is as follows: Given a training set $\left.\left\{\left(\mathbf{x}_1, y_1\right), \ldots, \mathbf{x}_n, y_n\right)\right\}$, where $\mathbf{x}_i \in \mathbb{R}^d$ and $y_i \in\{0,1, \ldots, K-1\}$, our goal is to find a classifier $h$ such that given an arbitrary feature vector $\mathbf{x}$, the classifier $h$ predicts the corresponding class $\{0,1, \ldots, K-1\}$.

Models for classification problems can be distinguished as being **generative** or **discriminative**. A discriminative model describes the distribution $\mathrm{P}[y \mid \mathbf{x}]$ directly and thus directly discriminates between the target value $y$ for any given feature vector $\mathbf{x}$. In contrast, a generative model learns $\mathrm{P}[\mathbf{x} \mid y]$ and $\mathrm{P}[y]$ directly, we will discuss an example of a generative model/classifier in the next lecture.


## Logistic Regression Model
For simplicity, we start with a binary classification problem with $y \in\{0,1\}$. We could approach the classification problem as a continuous-valued regression problem by ignoring the fact that $y$ is discretely valued, but that usually performs very poorly. Instead, we can change the parametric form of our classifier, to ensure that it only outputs values in the interval $[0,1]$. The output of the classifier can then be interpreted as the probability that the feature belongs to one class or the other. In the logistic regression model, the classifier takes the following form:
$$
h_\theta(\mathbf{x})=g(\langle\theta, \mathbf{x}\rangle),
$$
where
$$
g(z)=\left(1+e^{-z}\right)^{-1}
$$
is the **logistic function**, also called the **sigmoid function**. Figure 1 contains a plot. The logistic function smoothly varies between 0 and 1 . <u> We can also choose other functions instead of the logistic function, but we will see later that the logistic function has some advantages.</u> Let us interpret $h_\theta(\mathbf{x})$ as a probability:
$$
\mathrm{P}[y=1 \mid \mathbf{x}]=h_\theta(\mathbf{x}) \text { and } \mathrm{P}[y=0 \mid \mathbf{x}]=1-h_\theta(\mathbf{x}) .
$$
We then classify an example as belonging to class 1 if $\mathrm{P}[y=1 \mid \mathbf{x}]>1 / 2$ which is equivalent to $\langle\theta, \mathbf{x}\rangle>0$. Logistic regression is called a discriminative classifier, since it directly estimates the parameters of the distribution $\mathrm{P}[y \mid \mathbf{x}]$.

<mark>**逻辑回归模型P(y|x)
    - 为伯努利分布
    - discriminative** </mark>

![](https://i.imgur.com/ceMFXYL.png)


## Fitting a Model
We next discuss methods to learn the feature vector $\theta$ based on training data. In the lecture on linear regression, we saw that we can learn parameters based on modeling assumptions on how the training data was generated. Let us assume that we are given a training set $\left.\mathcal{D}=\left\{\left(\mathbf{x}_1, y_1\right), \ldots, \mathbf{x}_n, y_n\right)\right\}$, where $\mathbf{x}_i \in \mathbb{R}^d$ and $y_i \in\{0,1\}$. We furthermore assume that the feature vectors $\mathbf{x}_i$ are deterministic, but the labels are generated at random and independently across $i$ and follow the distribution

$$
\mathrm{P}\left[y_i=1 \mid \mathbf{x}_i, \theta\right]=h_\theta\left(\mathbf{x}_i\right) \text { and } \mathrm{P}\left[y_i=0 \mid \mathbf{x}_i, \theta\right]=1-h_\theta\left(\mathbf{x}_i\right) .
$$

As before, **we use the maximum likelihood estimate for the parameter $\theta$, i.e., the estimate that maximizes the probability that the data $\mathcal{D}$ is from the model $h_\theta$.** This amounts to finding the parameter $\theta$ that maximizes the likelihood function, defined as

![](https://i.imgur.com/h9uuA32.png)
The second equality follows by our assumption that each label $y_i$ is generated independently, and the third equality follows from our assumption that the data is generated by the model $h_\theta$.

As before, we maximize the log-likelihood instead (which we can do since log is strictly monotonic)

$$
\log \mathcal{L}(\theta, \mathcal{D})=\sum_{i=1}^n y_i \log \left(h\left(\mathbf{x}_i\right)\right)+\left(1-y_i\right) \log \left(1-h_\theta\left(\mathbf{x}_i\right)\right) .
$$

- <mark>Cross Entropy 就是通过对likelihood (MLE) **取log**化简得到的</mark>

In conclusion, the logistic regression estimate of the parameter $\theta$ is given by:

$$
\hat{\theta}=\arg \min _\theta \sum_{i=1}^n y_i \log \left(1 / h_\theta\left(\mathbf{x}_i\right)\right)+\left(1-y_i\right) \log 1 /\left(1-h_\theta\left(\mathbf{x}_i\right)\right) .
$$

Before discussing how to solve this optimization problem, we note that in regression, we started our discussion by fitting a model by minimizing the loss between the data predicted by the model

and the training data, and we choose the quadratic loss function $\operatorname{loss}(y, z)=(y-z)^2$ to measure the loss. The theoretical justification was that learning with the quadratic loss corresponds to maximum likelihood estimation under Gaussian noise.

We could approach logistic regression in the same manner. Specifically, we can fit a model by solving

$$
\hat{\theta}=\arg \min _\theta \frac{1}{n} \sum_{i=1}^n \operatorname{loss}\left(h_\theta\left(\mathbf{x}_i\right), y_i\right),
$$

where loss is some loss function. Logistic regression does exactly that, and chooses as the loss function the log-loss or the **negative cross entropy** defined as

$$
\operatorname{loss}(y, z)=-y \log (z)-(1-y) \log (1-z)=y \log (1 / z)+(1-y) \log (1 /(1-z))
$$


##  Computing the logistic regression estimate with gradient descent
The negative log-likelihood

$$
-\log \mathcal{L}(\theta, \mathcal{D})=\sum_{i=1}^n \operatorname{loss}\left(g\left(\left\langle\theta, \mathbf{x}_i\right\rangle\right), y_i\right),
$$

with

$$
\operatorname{loss}\left(g\left(\left\langle\theta, \mathbf{x}_i\right\rangle\right), y_i\right)=-y_i \log \left(g\left(\left\langle\theta, \mathbf{x}_i\right\rangle\right)\right)-\left(1-y_i\right) \log \left(1-g\left(\left\langle\theta, \mathbf{x}_i\right\rangle\right)\right)
$$

is convex since $\operatorname{loss}\left(g\left(\left\langle\theta, \mathbf{x}_i\right\rangle\right), y_i\right)$ is convex, as we show later. Thus, we can approximate the logistic regression estimate $\hat{\theta}$ with gradient descent:

$$
\begin{aligned}
\theta^{k+1} &=\theta^k-\alpha \nabla(-\log \mathcal{L}(\theta, \mathcal{D})) \\
&=\theta^k-\alpha \sum_{i=1}^n \nabla \operatorname{loss}\left(g\left(\left\langle\theta, \mathbf{x}_i\right\rangle\right), y_i\right)
\end{aligned}
$$

In practice we would actually often use stochastic gradient descent to minimize the function above, because of computational reasons. The stochastic gradient method is similar to gradient descent, and will be discussed in detail later in this course.

We next compute the gradient of the loss above. Towards this goal, we first note that <mark> a convenient property of the logistic function is that its derivative can be written as:</mark>

$$
g^{\prime}(z)=\frac{-1}{\left(1+e^{-z}\right)^2} e^{-z}(-1)=\frac{1}{\left(1+e^{-z}\right)}\left(1-\frac{1}{\left(1+e^{-z}\right)}\right)=g(z)(1-g(z)),
$$

where the first equality is by the chain rule. With this relation, the gradients can be computed as

![](https://i.imgur.com/oZG5YkS.png)

where the second inequality follows from the convenient relation $g^{\prime}(z)=g(z)(1-g(z))$, shown above.

Next, we show that the loss above is indeed convex in $\theta$, as claimed. We can prove this as follows. Suppose that $d=1$. Then the likelihood is a function $f: \mathbb{R} \rightarrow \mathbb{R}$. A differentiable function $f$ is convex if and only if its second derivative obeys $f^{\prime \prime}(\mathbf{x}) \geq 0$ for all $\mathbf{x}$. For example $x^2$ is convex and it's second derivative is 1 . This generalizes to multivariate functions as follows. Define the Hessian of a multivariate function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ as the $d \times d$ matrix $\nabla^2 f(\mathbf{x})$ with entries

$$
\left[\nabla^2 f(\mathbf{x})\right]_{i j}=\frac{\partial}{\partial \theta_i} \frac{\partial}{\partial \theta_j} f(\mathbf{x}) .
$$

A multivariate function is convex if and only if $\nabla^2 f(\mathbf{x})$ is positive semidefinite (recall that a matrix $\mathbf{A}$ is positive semidefinite if $\mathbf{z}^T \mathbf{A} \mathbf{z} \geq 0$ for all $\mathbf{z}$ ).

Thus, all we need to do is to show that the Hessian of the loss if positive semidefinite. Towards this goal, we compute

![](https://i.imgur.com/8eTWJQO.png)

Thus, the Hessian can be written as

$$
\nabla^2 \operatorname{loss}\left(y, h_\theta(\mathbf{x})\right)=g(\langle\theta, \mathbf{x}\rangle)(1-g(\langle\theta, \mathbf{x}\rangle)) \mathbf{x x}^T .
$$

Since $g(\langle\theta, \mathbf{x}\rangle)(1-g(\langle\theta, \mathbf{x}\rangle))$ is non-negative, and $\mathbf{x x}^T$ is positive semidefinite, the Hessian above is positive semidefinite, and therefore the loss is convex in $\theta$. Since the negative log-likelihood $-\log \mathcal{L}(\theta, \mathcal{D})$ is s a sum of convex functions, it is also convex. As a consequence, gradient descent will converge to a minimizer.

##  Multiclass logistic regression
Let us next generalize logistic regression to the case with $K>2$ classes instead of only two classes. This is called multiclass logistic regression or **softmax regression**.

A key idea is to use what is called a one-hot-encoding $\mathbf{y} \in\{0,1\}^K$ for representing the labels. The one-hot encoding $\mathbf{y}$ associated with a label $y=k$ is equal to 1 only at the position $k$ 

$$
[\mathbf{y}]_k=1 \text {, if } y=k, \quad \text { and } \quad[\mathbf{y}]_j=0 \text { for all } j \neq k \text {. }
$$

We associate a parameter vector $\theta_k \in \mathbb{R}^d$ with each class. For a given feature vector, we can associate a score with each class $z_k=\left\langle\theta_k, \mathbf{x}\right\rangle$. Computing this score for each $k$ yields the vector $\mathbf{z}=\left[z_1, \ldots, z_K\right]$. The generalization of the logistic function is the softmax function $\sigma: \mathbb{R}^d \rightarrow[0,1]^d$, and is defined as:

$$
[\sigma(\mathbf{z})]_j=\frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}, \quad \text { for } k=1, \ldots, K
$$

Note that the components of the sofmax function $\sigma(\mathbf{z})$ applied to any vector $\mathbf{z}$ sum to one and lie in the range $[0,1]$. Thus, they can be interpreted as a probability distribution (<mark>Categorical Distribution</mark>). Given a feature vector $\mathbf{x}$, the model then predict its label as the index of the largest component of $\sigma(\mathbf{z})$.

We can estimate the parameters $\theta$ based on training data in a similar manner as before. Specifically, by summarizing the parameter vectors with the matrix $\Theta=\left[\theta_1, \ldots, \theta_K\right]$, we estimate the parameters by maximizing the likelihood or equivalently, minimizing the log-likeliehood:

$$
\begin{aligned}
\hat{\Theta} &=\arg \max _{\Theta} \mathcal{L}(\Theta, \mathcal{D}) \\
&=\arg \max _{\Theta} \prod_{i=1}^n \mathrm{P}\left[y_i \mid \mathbf{x}_i, \Theta\right] \\
&=\arg \max _{\Theta} \prod_{i=1}^n \prod_{k=1}^k \mathrm{P}\left[y_i=k \mid \mathbf{x}_i, \theta_k\right]^{\mathbb{1}\left\{y_i=k\right\}} \\
&=\arg \min _{\Theta}-\sum_{i=1}^n \sum_{k=1}^K \mathbb{1}\left\{y_i=k\right\} \log \mathrm{P}\left[y_i=k \mid \mathbf{x}_i, \theta_k\right] \\
&=\arg \min _{\Theta}-\sum_{i=1}^n \sum_{k=1}^K \mathbb{1}\left\{y_i=k\right\} \log \left(\frac{e^{\left\langle\theta_k, \mathbf{x}_i\right\rangle}}{\sum_{j=1}^K e^{\left\langle\theta_j, \mathbf{x}_i\right\rangle}}\right) .
\end{aligned}
$$

- <mark>CE 就是通过对Likelihood (MLE) 取log化简得到的</mark>

As for the binary regression case, this is a convex optimization problem and can be solved with gradient descent.