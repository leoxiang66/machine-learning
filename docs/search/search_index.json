{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to My ML Notes","title":"Welcome to My ML Notes"},{"location":"#welcome-to-my-ml-notes","text":"","title":"Welcome to My ML Notes"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/","text":"Linear Regression Linear regression Intro Dataset: \\(\\mathcal{D}=\\left\\{\\left(y_1, \\mathbf{x}_1\\right), \\ldots,\\left(y_n, \\mathbf{x}_n\\right)\\right\\}\\) Assume \\(y=h(\\mathbf{x})+z\\) Our ultimate goal is to predict the value \\(y\\) corresponding to a feature vector \\(\\mathbf{x}\\) that is not in the training set. Towards this goal, we \"learn\" the function from the dataset, which yields an estimate \\(\\hat{h}\\) of the function \\(h^*\\) , and then predict \\(y=\\hat{h}(\\mathbf{x})\\) . Since learning an arbitrary function is intractable and not a well posed problem, we assume that the function \\(h\\) lies in some hypothesis class \\(\\mathcal{H}\\) of allowable functions. Typically, we assume that the set of functions \\(\\mathcal{H}\\) is parameterized, meaning that there is some (parameter or weight) vector \\(\\boldsymbol{\\theta}\\) which determines a function in the class \\(\\mathcal{H}\\) . A concrete example is the class of all linear function, which is given as the set: suppose the function \\(h\\) is fully determined by the vector \\(\\boldsymbol{\\theta}\\) ; we write \\(h_{\\boldsymbol{\\theta}}\\) to make this dependence explicit. With the class of functions parameterized by a vector \\(\\boldsymbol{\\theta}\\) , the problem of learning \\(h\\) reduces to the problem of estimating the parameter \\(\\boldsymbol{\\theta}\\) . Learn the parameter We choose a cost function loss: \\(\\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}\\) that measures how well (for a fixed \\(\\boldsymbol{\\theta}\\) ) the predictions \\(h_{\\boldsymbol{\\theta}}(\\mathbf{x})\\) describe the output \\(y\\) , then find the parameter that minimizes that loss: $$ \\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\sum_{i=1}^n \\operatorname{loss}\\left(h_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_i\\right), y_i\\right) $$ Here, and in the following we use the hat-notation \\(\\hat{\\boldsymbol{\\theta}}\\) to indicate that this variable is estimated based on data. Loss function The most popular metric in linear regression is the sum of the squares of the fitting error: ( least square cost function/MSE ) $$ \\hat{R}(\\boldsymbol{\\theta})=\\frac{1}{n} \\sum_{i=1}^n\\left(\\left\\langle\\mathbf{x}_i, \\boldsymbol{\\theta}\\right\\rangle-y_i\\right)^2 $$ $$ =\\frac{1}{n}|\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\theta}|_2^2 $$ The least squares cost function is convenient for a variety of reasons: - it can be numerically minimized efficiently as it is convex (convexity is defined below) - it has intuitive geometric and probabilistic interpretations, as we will see later. Least squares estimate: the parameter vector that minimizes this cost function: $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\hat{R}(\\boldsymbol{\\theta}) $$ \u8ba8\u8bba\u7ebf\u6027\u56de\u5f52\u4e2dn\u548cd\u7684\u5173\u7cfb Before discussing how \\(\\hat{\\theta}_{\\mathrm{LS}}\\) can be computed, let us comment on the relation of the number of examples, \\(n\\) , and the number of features, \\(d\\) . Suppose that at least \\(n\\) columns of \\(\\mathbf{X}\\) are linearly independent. Then we must have \\(n \\leq d\\) , and regardless of \\(\\mathbf{y}\\) , we can find a parameter vector \\(\\boldsymbol{\\theta}\\) (or many different ones) such that \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) , and thus describe the independent variable \\(y\\) exactly. - In the linear regression model, this is only possible if there are fewer examples than number of model parameters \\((n \\leq d)\\) . In that regime, even if \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) have nothing to do with each other, we achieve a perfect fit or zero loss. In this regime, we are likely overfitting: the model is too flexible given the data. Thus, fitting a linear model without additional assumptions only makes sense in the so called over-determined or under-parameterized regime, where the number of training examples is larger than the number of features \\((n \\geq d)\\) . If we find a good approximation in the \\((n \\geq d)\\) -regime, then this is an indication that the linear model is capturing structure of the problem. n<=d \u4e0d\u8bbax\u548cy\u7684\u5173\u7cfb\uff0c \u90fd\u80fd\u627e\u5230\u4e00\u4e2a \\(\\theta\\) , \u4f7f\u5f97 \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) overfitting not make senses n>=d: \u5982\u679c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u627e\u5230\u4e86\u4e00\u4e2a\u597d\u7684\u8fd1\u4f3c\u6a21\u578b\uff0c \u5219\u8be5\u6a21\u578b\u6bd4\u8f83\u51c6\u786e Least squares estimation We next show that \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) can be computed in closed form. Assume that \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) has full column rank. - This implies that the number or training examples, \\(n\\) , is larger than the number of model parameters, \\(d\\) . - features of \\(x\\) are linearly independent In the regime \\(n \\geq d\\) , if \\(\\mathbf{X}\\) does not have full column rank, then the features are linearly dependent and thus redundant. Proposition 1. Suppose that \\(\\mathbf{X}\\) has full column rank \\(d\\) . Then for any \\(\\mathbf{y}\\) , we have $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y} $$ There are at least two proofs of this proposition. Analysis of the least squares estimate and in what sense co-linear features are \"bad\" Thus far, we have not commented on the features of the regression model. Intuitively, if the features are very similar to each other, least squares estimation should be difficult. For example, suppose all the features (i.e., the entries of the \\(\\mathbf{x}_i\\) ) are linear functions of each other. Then the columns of \\(\\mathbf{X}\\) are linearly dependent and \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) is not unique. In statistics it is common to refer to features with this property as co-linear . A set of variables is said to be perfectly multi-colinear if there is one or more exact linear relationship among some of the variables \\(x_{i 1}, \\ldots, x_{i d}\\) ; for example the first two variables are co-linear if \\(x_{i 1}=-2 x_{i 2}\\) for all \\(i\\) . In practice, features are usually not co-linear but can be highly correlated, which leads to instable least squares estimates. Case1. \\(X\\) has full column rank and \\(y\\) is generated according to a linear model with additive Gaussian noise: $$ \\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}^*+\\mathbf{z} $$ Here, the entries of \\(\\mathbf{z}\\) are independent zero-mean Gaussian noise with variance \\(1 / n\\) , Then: the expected error is 0, or in statistical terms, the LSE is unbiased Case2. Features of \\(X\\) are strongly correlated where \\(\\sigma_i\\) are the singular values of \\(\\mathbf{X}\\) , and expectation is over the Gaussian noise \\(\\mathbf{z}\\) then \\(1 / \\sigma_{\\min }^2\\) is large and the bound becomes large. This is the case if some of the features are highly correlated, known as multicollinearity in the statistics literature. On the other hand, as expected, the error decays in \\(n\\) , the larger \\(n\\) , the smaller the error. proof of the error function:","title":"Linear Regression"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#linear-regression","text":"","title":"Linear Regression"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#linear-regression_1","text":"","title":"Linear regression"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#intro","text":"Dataset: \\(\\mathcal{D}=\\left\\{\\left(y_1, \\mathbf{x}_1\\right), \\ldots,\\left(y_n, \\mathbf{x}_n\\right)\\right\\}\\) Assume \\(y=h(\\mathbf{x})+z\\) Our ultimate goal is to predict the value \\(y\\) corresponding to a feature vector \\(\\mathbf{x}\\) that is not in the training set. Towards this goal, we \"learn\" the function from the dataset, which yields an estimate \\(\\hat{h}\\) of the function \\(h^*\\) , and then predict \\(y=\\hat{h}(\\mathbf{x})\\) . Since learning an arbitrary function is intractable and not a well posed problem, we assume that the function \\(h\\) lies in some hypothesis class \\(\\mathcal{H}\\) of allowable functions. Typically, we assume that the set of functions \\(\\mathcal{H}\\) is parameterized, meaning that there is some (parameter or weight) vector \\(\\boldsymbol{\\theta}\\) which determines a function in the class \\(\\mathcal{H}\\) . A concrete example is the class of all linear function, which is given as the set: suppose the function \\(h\\) is fully determined by the vector \\(\\boldsymbol{\\theta}\\) ; we write \\(h_{\\boldsymbol{\\theta}}\\) to make this dependence explicit. With the class of functions parameterized by a vector \\(\\boldsymbol{\\theta}\\) , the problem of learning \\(h\\) reduces to the problem of estimating the parameter \\(\\boldsymbol{\\theta}\\) .","title":"Intro"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#learn-the-parameter","text":"We choose a cost function loss: \\(\\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}\\) that measures how well (for a fixed \\(\\boldsymbol{\\theta}\\) ) the predictions \\(h_{\\boldsymbol{\\theta}}(\\mathbf{x})\\) describe the output \\(y\\) , then find the parameter that minimizes that loss: $$ \\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\sum_{i=1}^n \\operatorname{loss}\\left(h_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_i\\right), y_i\\right) $$ Here, and in the following we use the hat-notation \\(\\hat{\\boldsymbol{\\theta}}\\) to indicate that this variable is estimated based on data.","title":"Learn the parameter"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#loss-function","text":"The most popular metric in linear regression is the sum of the squares of the fitting error: ( least square cost function/MSE ) $$ \\hat{R}(\\boldsymbol{\\theta})=\\frac{1}{n} \\sum_{i=1}^n\\left(\\left\\langle\\mathbf{x}_i, \\boldsymbol{\\theta}\\right\\rangle-y_i\\right)^2 $$ $$ =\\frac{1}{n}|\\mathbf{y}-\\mathbf{X} \\boldsymbol{\\theta}|_2^2 $$ The least squares cost function is convenient for a variety of reasons: - it can be numerically minimized efficiently as it is convex (convexity is defined below) - it has intuitive geometric and probabilistic interpretations, as we will see later. Least squares estimate: the parameter vector that minimizes this cost function: $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\hat{R}(\\boldsymbol{\\theta}) $$","title":"Loss function"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#nd","text":"Before discussing how \\(\\hat{\\theta}_{\\mathrm{LS}}\\) can be computed, let us comment on the relation of the number of examples, \\(n\\) , and the number of features, \\(d\\) . Suppose that at least \\(n\\) columns of \\(\\mathbf{X}\\) are linearly independent. Then we must have \\(n \\leq d\\) , and regardless of \\(\\mathbf{y}\\) , we can find a parameter vector \\(\\boldsymbol{\\theta}\\) (or many different ones) such that \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) , and thus describe the independent variable \\(y\\) exactly. - In the linear regression model, this is only possible if there are fewer examples than number of model parameters \\((n \\leq d)\\) . In that regime, even if \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) have nothing to do with each other, we achieve a perfect fit or zero loss. In this regime, we are likely overfitting: the model is too flexible given the data. Thus, fitting a linear model without additional assumptions only makes sense in the so called over-determined or under-parameterized regime, where the number of training examples is larger than the number of features \\((n \\geq d)\\) . If we find a good approximation in the \\((n \\geq d)\\) -regime, then this is an indication that the linear model is capturing structure of the problem. n<=d \u4e0d\u8bbax\u548cy\u7684\u5173\u7cfb\uff0c \u90fd\u80fd\u627e\u5230\u4e00\u4e2a \\(\\theta\\) , \u4f7f\u5f97 \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}\\) overfitting not make senses n>=d: \u5982\u679c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u627e\u5230\u4e86\u4e00\u4e2a\u597d\u7684\u8fd1\u4f3c\u6a21\u578b\uff0c \u5219\u8be5\u6a21\u578b\u6bd4\u8f83\u51c6\u786e","title":"\u8ba8\u8bba\u7ebf\u6027\u56de\u5f52\u4e2dn\u548cd\u7684\u5173\u7cfb"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#least-squares-estimation","text":"We next show that \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) can be computed in closed form. Assume that \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) has full column rank. - This implies that the number or training examples, \\(n\\) , is larger than the number of model parameters, \\(d\\) . - features of \\(x\\) are linearly independent In the regime \\(n \\geq d\\) , if \\(\\mathbf{X}\\) does not have full column rank, then the features are linearly dependent and thus redundant. Proposition 1. Suppose that \\(\\mathbf{X}\\) has full column rank \\(d\\) . Then for any \\(\\mathbf{y}\\) , we have $$ \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y} $$ There are at least two proofs of this proposition.","title":"Least squares estimation"},{"location":"Supervised%20Learning/1%20Regression/1.1%20Linear%20Regression/#analysis-of-the-least-squares-estimate-and-in-what-sense-co-linear-features-are-bad","text":"Thus far, we have not commented on the features of the regression model. Intuitively, if the features are very similar to each other, least squares estimation should be difficult. For example, suppose all the features (i.e., the entries of the \\(\\mathbf{x}_i\\) ) are linear functions of each other. Then the columns of \\(\\mathbf{X}\\) are linearly dependent and \\(\\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}\\) is not unique. In statistics it is common to refer to features with this property as co-linear . A set of variables is said to be perfectly multi-colinear if there is one or more exact linear relationship among some of the variables \\(x_{i 1}, \\ldots, x_{i d}\\) ; for example the first two variables are co-linear if \\(x_{i 1}=-2 x_{i 2}\\) for all \\(i\\) . In practice, features are usually not co-linear but can be highly correlated, which leads to instable least squares estimates. Case1. \\(X\\) has full column rank and \\(y\\) is generated according to a linear model with additive Gaussian noise: $$ \\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}^*+\\mathbf{z} $$ Here, the entries of \\(\\mathbf{z}\\) are independent zero-mean Gaussian noise with variance \\(1 / n\\) , Then: the expected error is 0, or in statistical terms, the LSE is unbiased Case2. Features of \\(X\\) are strongly correlated where \\(\\sigma_i\\) are the singular values of \\(\\mathbf{X}\\) , and expectation is over the Gaussian noise \\(\\mathbf{z}\\) then \\(1 / \\sigma_{\\min }^2\\) is large and the bound becomes large. This is the case if some of the features are highly correlated, known as multicollinearity in the statistics literature. On the other hand, as expected, the error decays in \\(n\\) , the larger \\(n\\) , the smaller the error. proof of the error function:","title":"Analysis of the least squares estimate and in what sense co-linear features are \"bad\""},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/","text":"Ridge Regression \u5cad\u56de\u5f52 Motivation \u4e0a\u4e00\u8282\u8bb2\u7684\u7ebf\u6027\u56de\u5f52\u4e2d\uff0cexpected error: \u5982\u679c Features of \\(X\\) are (strongly) correlated specifically if its smallest singular value, \\(\\sigma_{\\min }^2(\\mathbf{X})\\) , is close to zero. In this case, \\(1 / \\sigma_{\\min }^2(\\mathbf{X})\\) is large, and therefore the expected error above is large. solution: ridge regression Ridge regression Ridge regression: \u6700\u5c0f\u4e8c\u4e58\u6cd5 + l2 norm regularization This additional term promotes solutions that yield a good fit while at the same time enforcing the coefficients of \\(\\theta\\) to be sufficiently small. Also known as: ridge regression (statistics) Tkhonov regularization/weight decay (machine learning) In order to avoid this, we can add a term penalizing the norm of the coefficient vector \\(\\boldsymbol{\\theta}\\) . This additional term promotes solutions that yield a good fit while at the same time enforcing the coefficients of \\(\\theta\\) to be sufficiently small. Incorporating assumptions about the solution-in this case that the coefficients should not be too large - is called regularization. A least squares penalty combined with \\(\\ell2\\) -norm regularization is called ridge regression in statistics and is also known as Tikhonov regularization and weight decay in machine learning. The ridge regression estimate is: Lambda parameter \\(\\lambda\\) is a fixed regularization parameter. different values of \\(\\lambda\\) give us different ridge regression estimates. Observe that for \\(\\lambda \\rightarrow 0\\) , the ridge regression estimate converges to the least squares estimate, and for \\(\\lambda \\rightarrow \\infty\\) , the ridge regression estimate converges to zero. Ridge regression estimate If \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) has full column rank \\(d\\) , the ridge regression estimate has a closed-form solution: Equation 5. proof: Recall that \\(\\mathbf{X} \\hat{\\theta}_{\\mathrm{LS}}\\) is the orthogonal projection of the data onto the column space of the feature matrix, i.e., \\(\\mathbf{X} \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\mathbf{U U}^T \\mathbf{y}\\) , where \\(\\mathbf{U} \\in \\mathbb{R}^{n \\times d}\\) is an orthogonal basis for the column space of \\(\\mathbf{X}\\) . The approximation \\(\\mathbf{X} \\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) can no longer be interpreted as the orthogonal projection of the data onto the column space of the feature matrix. However, it can be interpreted as a modified projection, where each component of the data in the direction of the left singular vector \\(\\mathbf{u}_i\\) of the feature matrix is shrunk by a factor of \\(\\sigma_i^2 /\\left(\\sigma_i^2+\\lambda\\right)\\) . Here, \\(\\sigma_i\\) is the singular value of the feature matrix corresponding to the singular vector \\(\\mathbf{u}_i\\) . In more detail, consider the singular value decomposition \\(\\mathbf{X}=\\mathbf{U} \\Sigma \\mathbf{V}^T\\) , and let \\(\\mathbf{u}_i\\) be the \\(i\\) -th column of \\(\\mathbf{U}\\) , i.e., \\(\\mathbf{U}=\\left[\\mathbf{u}_1, \\ldots, \\mathbf{u}_d\\right]\\) . Then we have equation 5 . where we used that for two square matrixes that are invertible, we have that \\((\\mathbf{A B})^{-1}=\\mathbf{B}^{-1} \\mathbf{A}^{-1}\\) . Next, note that \\(\\left(\\Sigma^2+\\lambda \\mathbf{I}\\right)\\) is a diagonal matrix with \\(\\sigma_i^2+\\lambda, i=1, \\ldots, d\\) on its diagonal, thus \\(\\left(\\Sigma^2+\\lambda \\mathbf{I}\\right)^{-1}\\) is a diagonal matrix with \\(1 /\\left(\\sigma_i^2+\\lambda\\right), i=1, \\ldots, d\\) on its diagonal. As a consequence, This concludes the proof of equation (5) Bias-variance tradeoff of the ridge regression estimate Assumption: the data indeed follows a linear model, i.e., \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}^*+\\mathbf{z}\\) the ride regression estimator can be decomposed into a signal and a noise term: Analysis suppose that the noise \\(\\mathbf{z}\\) is a zero-mean random variable. Then the expectation of \\(\\boldsymbol{\\theta}_{\\text {ridge }}\\) equals the first term \\(\\hat{\\theta}_{\\text {ridge }}^{\\text {signal }}\\) which is deterministic. The second term, \\(\\hat{\\theta}_{\\text {ridge }}^{\\text {noise }}\\) , is random and determines the variance of the ridge estimator. If \\(\\lambda=0\\) , the first term becomes \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}^{\\text {signal }}=\\boldsymbol{\\theta}^*\\) , otherwise the first term is not equal to \\(\\boldsymbol{\\theta}^*\\) . Thus, ridge regression is a biased estimator (for \\(\\lambda>0\\) ). Increasing \\(\\lambda\\) moves the mean of the estimate \\(\\hat{\\theta}_{\\text {ridge }}\\) farther from the true model parameter \\(\\theta^*\\) , and thus increases the bias of the estimate, which is defined as \\(\\mathbb{E}\\left[\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right]-\\boldsymbol{\\theta}^*=\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}^{\\text {signal }}-\\boldsymbol{\\theta}^*\\) \\(\\lambda\\) \u8d8a\u5927\uff0c \\(\\hat{\\theta}_{\\text{ridge}}\\) \u7684\u7edd\u5bf9\u503c\u8d8a\u5c0f --> 0 But in exchange, increasing \\(\\lambda\\) also shrinks the impact of the noise. Thus, choosing \\(\\lambda\\) appropriately enables us to adapt to the conditioning of the feature matrix and to the noise level for achieving a good tradeoff between the bias and the variance. \u56e0\u6b64\uff0c\u9009\u62e9\u9002\u5f53\u7684\u03bb\u4f7f\u6211\u4eec\u80fd\u591f\u9002\u5e94\u7279\u5f81\u77e9\u9635\u7684\u6761\u4ef6\u548c\u566a\u58f0\u6c34\u5e73\uff0c\u4ee5\u5728\u504f\u5dee\u548c\u65b9\u5dee\u4e4b\u95f4\u5b9e\u73b0\u826f\u597d\u7684\u6298\u8877\u3002 Note that the optimal choice depends on the noise which is unknown to us, the conditioning of the feature matrix, thus we cannot simply compute the optimal choice based on the data. Later in the course we will discuss methods that enable us to determine a good choice of \\(\\lambda\\) based on the data. Compared with linear regression where normalized MSE = \\(\\left\\|\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right\\|_2^2 /\\|\\boldsymbol{\\theta}\\|_2^2\\) \u53ef\u4ee5\u770b\u5230\u4f7f\u5f97loss\u6700\u5c0f\u7684 \\(\\lambda \\neq 0\\)","title":"Ridge Regression \u5cad\u56de\u5f52"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#ridge-regression","text":"","title":"Ridge Regression \u5cad\u56de\u5f52"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#motivation","text":"\u4e0a\u4e00\u8282\u8bb2\u7684\u7ebf\u6027\u56de\u5f52\u4e2d\uff0cexpected error: \u5982\u679c Features of \\(X\\) are (strongly) correlated specifically if its smallest singular value, \\(\\sigma_{\\min }^2(\\mathbf{X})\\) , is close to zero. In this case, \\(1 / \\sigma_{\\min }^2(\\mathbf{X})\\) is large, and therefore the expected error above is large. solution: ridge regression","title":"Motivation"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#ridge-regression_1","text":"Ridge regression: \u6700\u5c0f\u4e8c\u4e58\u6cd5 + l2 norm regularization This additional term promotes solutions that yield a good fit while at the same time enforcing the coefficients of \\(\\theta\\) to be sufficiently small. Also known as: ridge regression (statistics) Tkhonov regularization/weight decay (machine learning) In order to avoid this, we can add a term penalizing the norm of the coefficient vector \\(\\boldsymbol{\\theta}\\) . This additional term promotes solutions that yield a good fit while at the same time enforcing the coefficients of \\(\\theta\\) to be sufficiently small. Incorporating assumptions about the solution-in this case that the coefficients should not be too large - is called regularization. A least squares penalty combined with \\(\\ell2\\) -norm regularization is called ridge regression in statistics and is also known as Tikhonov regularization and weight decay in machine learning. The ridge regression estimate is:","title":"Ridge regression"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#lambda-parameter","text":"\\(\\lambda\\) is a fixed regularization parameter. different values of \\(\\lambda\\) give us different ridge regression estimates. Observe that for \\(\\lambda \\rightarrow 0\\) , the ridge regression estimate converges to the least squares estimate, and for \\(\\lambda \\rightarrow \\infty\\) , the ridge regression estimate converges to zero.","title":"Lambda parameter"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#ridge-regression-estimate","text":"If \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) has full column rank \\(d\\) , the ridge regression estimate has a closed-form solution: Equation 5. proof: Recall that \\(\\mathbf{X} \\hat{\\theta}_{\\mathrm{LS}}\\) is the orthogonal projection of the data onto the column space of the feature matrix, i.e., \\(\\mathbf{X} \\hat{\\boldsymbol{\\theta}}_{\\mathrm{LS}}=\\mathbf{U U}^T \\mathbf{y}\\) , where \\(\\mathbf{U} \\in \\mathbb{R}^{n \\times d}\\) is an orthogonal basis for the column space of \\(\\mathbf{X}\\) . The approximation \\(\\mathbf{X} \\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\) can no longer be interpreted as the orthogonal projection of the data onto the column space of the feature matrix. However, it can be interpreted as a modified projection, where each component of the data in the direction of the left singular vector \\(\\mathbf{u}_i\\) of the feature matrix is shrunk by a factor of \\(\\sigma_i^2 /\\left(\\sigma_i^2+\\lambda\\right)\\) . Here, \\(\\sigma_i\\) is the singular value of the feature matrix corresponding to the singular vector \\(\\mathbf{u}_i\\) . In more detail, consider the singular value decomposition \\(\\mathbf{X}=\\mathbf{U} \\Sigma \\mathbf{V}^T\\) , and let \\(\\mathbf{u}_i\\) be the \\(i\\) -th column of \\(\\mathbf{U}\\) , i.e., \\(\\mathbf{U}=\\left[\\mathbf{u}_1, \\ldots, \\mathbf{u}_d\\right]\\) . Then we have equation 5 . where we used that for two square matrixes that are invertible, we have that \\((\\mathbf{A B})^{-1}=\\mathbf{B}^{-1} \\mathbf{A}^{-1}\\) . Next, note that \\(\\left(\\Sigma^2+\\lambda \\mathbf{I}\\right)\\) is a diagonal matrix with \\(\\sigma_i^2+\\lambda, i=1, \\ldots, d\\) on its diagonal, thus \\(\\left(\\Sigma^2+\\lambda \\mathbf{I}\\right)^{-1}\\) is a diagonal matrix with \\(1 /\\left(\\sigma_i^2+\\lambda\\right), i=1, \\ldots, d\\) on its diagonal. As a consequence, This concludes the proof of equation (5)","title":"Ridge regression estimate"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#bias-variance-tradeoff-of-the-ridge-regression-estimate","text":"Assumption: the data indeed follows a linear model, i.e., \\(\\mathbf{y}=\\mathbf{X} \\boldsymbol{\\theta}^*+\\mathbf{z}\\) the ride regression estimator can be decomposed into a signal and a noise term:","title":"Bias-variance tradeoff of the ridge regression estimate"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#analysis","text":"suppose that the noise \\(\\mathbf{z}\\) is a zero-mean random variable. Then the expectation of \\(\\boldsymbol{\\theta}_{\\text {ridge }}\\) equals the first term \\(\\hat{\\theta}_{\\text {ridge }}^{\\text {signal }}\\) which is deterministic. The second term, \\(\\hat{\\theta}_{\\text {ridge }}^{\\text {noise }}\\) , is random and determines the variance of the ridge estimator. If \\(\\lambda=0\\) , the first term becomes \\(\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}^{\\text {signal }}=\\boldsymbol{\\theta}^*\\) , otherwise the first term is not equal to \\(\\boldsymbol{\\theta}^*\\) . Thus, ridge regression is a biased estimator (for \\(\\lambda>0\\) ). Increasing \\(\\lambda\\) moves the mean of the estimate \\(\\hat{\\theta}_{\\text {ridge }}\\) farther from the true model parameter \\(\\theta^*\\) , and thus increases the bias of the estimate, which is defined as \\(\\mathbb{E}\\left[\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right]-\\boldsymbol{\\theta}^*=\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}^{\\text {signal }}-\\boldsymbol{\\theta}^*\\) \\(\\lambda\\) \u8d8a\u5927\uff0c \\(\\hat{\\theta}_{\\text{ridge}}\\) \u7684\u7edd\u5bf9\u503c\u8d8a\u5c0f --> 0 But in exchange, increasing \\(\\lambda\\) also shrinks the impact of the noise. Thus, choosing \\(\\lambda\\) appropriately enables us to adapt to the conditioning of the feature matrix and to the noise level for achieving a good tradeoff between the bias and the variance. \u56e0\u6b64\uff0c\u9009\u62e9\u9002\u5f53\u7684\u03bb\u4f7f\u6211\u4eec\u80fd\u591f\u9002\u5e94\u7279\u5f81\u77e9\u9635\u7684\u6761\u4ef6\u548c\u566a\u58f0\u6c34\u5e73\uff0c\u4ee5\u5728\u504f\u5dee\u548c\u65b9\u5dee\u4e4b\u95f4\u5b9e\u73b0\u826f\u597d\u7684\u6298\u8877\u3002 Note that the optimal choice depends on the noise which is unknown to us, the conditioning of the feature matrix, thus we cannot simply compute the optimal choice based on the data. Later in the course we will discuss methods that enable us to determine a good choice of \\(\\lambda\\) based on the data.","title":"Analysis"},{"location":"Supervised%20Learning/1%20Regression/1.2%20Ridge%20Regression/#compared-with-linear-regression","text":"where normalized MSE = \\(\\left\\|\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}}_{\\text {ridge }}\\right\\|_2^2 /\\|\\boldsymbol{\\theta}\\|_2^2\\) \u53ef\u4ee5\u770b\u5230\u4f7f\u5f97loss\u6700\u5c0f\u7684 \\(\\lambda \\neq 0\\)","title":"Compared with linear regression"}]}