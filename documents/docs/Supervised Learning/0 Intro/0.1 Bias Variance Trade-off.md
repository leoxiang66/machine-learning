# Bias-Variance tradeoff


Our primary goal is finding a model that has high prediction performance on previously unseen examples.  In this section, we discuss the bias-variance tradeoff for predicting $y$ based on $\mathbf{x}$ and our learned model.

To start, we ask the question: How can we measure the effectiveness of a learning algorithm? To address this question in the context of learning, let us again consider the regression model
$$
y=h(\mathbf{x})+z,
$$
but now we assume that both the feature vector $\mathbf{x}$ and the noise $z$ are drawn from some distribution. Then $y$ is a random variable and its distribution is fully determined by $h$ and the distributions of $z$ and $\mathbf{x}$. Stated differently, $y$ and $\mathbf{x}$ are drawn from a joint distribution that is determined by the feature and noise distributions as well as $h$.

In this context, the learning problem is as follows: We obtain a dataset $\mathcal{D}=\left\{\left(y_1, \mathbf{x}_1\right), \ldots,\left(y_n, \mathbf{x}_n\right)\right\}$ by drawing $n$ examples from the joint distribution. The learning algorithm takes the dataset and returns an estimate of the regression function $\hat{h}$. As a concrete example of learning a regression function, we can compute the ridge regression estimate $\hat{\boldsymbol{\theta}}_{\text {ridge }}$ which then yields the following estimate of the regression function:
$$
\hat{h}(\mathbf{x})=\left\langle\mathbf{x}, \hat{\boldsymbol{\theta}}_{\text {ridge }}\right\rangle .
$$
Note that the $\hat{h}$ is a function of the data, $\mathcal{D}$.
It is natural to measure performance of the learned estimate $\hat{h}$ by measuring its performance on an new example, drawn from the joint distribution, and then averaging over many of such examples. Thus, we consider the expectation:

$$
R(\hat{h})=\mathbb{E}_{\mathbf{x}, y}\left[(\hat{h}(\mathbf{x})-y)^2\right] .
$$
Here, expectation is with respect to the newly drawn sample.
Recall that the estimate $\hat{h}$ is a function of the dataset, to make this dependency explicit, we write $\hat{h}=\hat{h}_{\mathcal{D}}$ in what follows. Since we are interested in the performance of a learning algorithm that selects $\hat{h}$ based on the randomly chosen dataset $\mathcal{D}$, we consider the expectation over the dataset:
![](https://i.imgur.com/Oj5cYos.png)

This error corresponds to the performance of the learner's prediction averaged over examples, and averaged over many data sets $\mathcal{D}$.

With the relation $y=h(\mathbf{x})+z$, we can decompose the error as follows:

![](https://i.imgur.com/H1PaP2m.png)


where the last equality follows from $z$ being independent of $\mathbf{x}$ and $\mathcal{D}$ and having zero mean. We next decompose the first term further. Towards this end, we condition on $\mathbf{x}$ (think about $\mathbf{x}$ as a fixed vector, not a random variable for the calculations below). Then the first term above becomes (all expectations below are with respect to the random dataset $\mathcal{D}$ ):

![](https://i.imgur.com/TnDRSuV.png)

where the middle cross term is equal to zero. Thus we have

$$
\mathbb{E}_{\mathcal{D}}[R(\hat{h})]=\mathbb{E}_{\mathbf{x}}\left[\left(h(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[\hat{h}_{\mathcal{D}}(\mathbf{x})\right]\right)^2\right]+\mathbb{E}_{\mathcal{D}, \mathbf{x}}\left[\left(\hat{h}_{\mathcal{D}}(\mathbf{x})-\mathbb{E}\left[\hat{h}_{\mathcal{D}}(\mathbf{x})\right]\right)^2\right]+\operatorname{Var}[z] .
$$


This is called a **bias-variance decomposition**.

- The first term is the bias of the method. It measures how well the average hypothesis can estimate the true underlying function $h$. A low bias means that $\hat{h}$ accurately estimates the underlying hypothesis $h$.
- The second term is the variance of the method. The variance of the method measures the variance of the hypothesis over the training sets.
- The third term is the variance of $z$, which is the irreducible error. The irreducible error term is the error that we cannot control or eliminate.