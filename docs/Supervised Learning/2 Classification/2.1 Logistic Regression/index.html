
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-8.5.10">
    
    
      
        <title>Logistic Regression - Machine Learning Notes</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.472b142f.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.08040f6c.min.css">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="None" data-md-color-accent="None">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#logistic-regression" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Machine Learning Notes" class="md-header__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Machine Learning Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Logistic Regression
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Machine Learning Notes" class="md-nav__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Machine Learning Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Welcome to My ML Notes
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Optimization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Optimization" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Optimization
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Optimization/1%20Convexity/" class="md-nav__link">
        Convex Optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Optimization/2%20Gradient%20Descent/" class="md-nav__link">
        Gradient Descent
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Supervised Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Supervised Learning" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Supervised Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_1" type="checkbox" id="__nav_3_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_1">
          0 Intro
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="0 Intro" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          0 Intro
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0%20Intro/0.1%20Bias%20Variance%20Trade-off/" class="md-nav__link">
        Bias-Variance tradeoff
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../0%20Intro/0.2%20Model%20Selection%20and%20Validation/" class="md-nav__link">
        Model selection and validation
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_2" type="checkbox" id="__nav_3_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_2">
          1 Regression
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="1 Regression" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          1 Regression
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1%20Regression/1.1%20Linear%20Regression/" class="md-nav__link">
        Linear Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../1%20Regression/1.2%20Ridge%20Regression/" class="md-nav__link">
        Ridge Regression 岭回归
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_3" type="checkbox" id="__nav_3_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3_3">
          2 Classification
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="2 Classification" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          2 Classification
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Logistic Regression
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Logistic Regression
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#logistic-regression-model" class="md-nav__link">
    Logistic Regression Model
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fitting-a-model" class="md-nav__link">
    Fitting a Model
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computing-the-logistic-regression-estimate-with-gradient-descent" class="md-nav__link">
    Computing the logistic regression estimate with gradient descent
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multiclass-logistic-regression" class="md-nav__link">
    Multiclass logistic regression
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#logistic-regression-model" class="md-nav__link">
    Logistic Regression Model
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fitting-a-model" class="md-nav__link">
    Fitting a Model
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computing-the-logistic-regression-estimate-with-gradient-descent" class="md-nav__link">
    Computing the logistic regression estimate with gradient descent
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multiclass-logistic-regression" class="md-nav__link">
    Multiclass logistic regression
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="logistic-regression">Logistic Regression</h1>
<p>In this lecture, we start discussing classification problems. In a classification problem, our goal is, given a feature vector <span class="arithmatex">\(\mathbf{x}\)</span>, to predict its label <span class="arithmatex">\(y\)</span>, which can only take on a small set of discrete values. A special case is binary classification, where the label <span class="arithmatex">\(y\)</span> only takes on two values, 0 and 1 for example. Most applications of machine learning are classification problems. A concrete classical example of a classification problem is spam filtering, where the vector <span class="arithmatex">\(\mathbf{x}\)</span> contains features of an email, and <span class="arithmatex">\(y=1\)</span> stands for spam and <span class="arithmatex">\(y=0\)</span> stands for not spam.</p>
<p>Formally, the classification problem is as follows: Given a training set <span class="arithmatex">\(\left.\left\{\left(\mathbf{x}_1, y_1\right), \ldots, \mathbf{x}_n, y_n\right)\right\}\)</span>, where <span class="arithmatex">\(\mathbf{x}_i \in \mathbb{R}^d\)</span> and <span class="arithmatex">\(y_i \in\{0,1, \ldots, K-1\}\)</span>, our goal is to find a classifier <span class="arithmatex">\(h\)</span> such that given an arbitrary feature vector <span class="arithmatex">\(\mathbf{x}\)</span>, the classifier <span class="arithmatex">\(h\)</span> predicts the corresponding class <span class="arithmatex">\(\{0,1, \ldots, K-1\}\)</span>.</p>
<p>Models for classification problems can be distinguished as being <strong>generative</strong> or <strong>discriminative</strong>. A discriminative model describes the distribution <span class="arithmatex">\(\mathrm{P}[y \mid \mathbf{x}]\)</span> directly and thus directly discriminates between the target value <span class="arithmatex">\(y\)</span> for any given feature vector <span class="arithmatex">\(\mathbf{x}\)</span>. In contrast, a generative model learns <span class="arithmatex">\(\mathrm{P}[\mathbf{x} \mid y]\)</span> and <span class="arithmatex">\(\mathrm{P}[y]\)</span> directly, we will discuss an example of a generative model/classifier in the next lecture.</p>
<h2 id="logistic-regression-model">Logistic Regression Model</h2>
<p>For simplicity, we start with a binary classification problem with <span class="arithmatex">\(y \in\{0,1\}\)</span>. We could approach the classification problem as a continuous-valued regression problem by ignoring the fact that <span class="arithmatex">\(y\)</span> is discretely valued, but that usually performs very poorly. Instead, we can change the parametric form of our classifier, to ensure that it only outputs values in the interval <span class="arithmatex">\([0,1]\)</span>. The output of the classifier can then be interpreted as the probability that the feature belongs to one class or the other. In the logistic regression model, the classifier takes the following form:
$$
h_\theta(\mathbf{x})=g(\langle\theta, \mathbf{x}\rangle),
$$
where
$$
g(z)=\left(1+e^{-z}\right)^{-1}
$$
is the <strong>logistic function</strong>, also called the <strong>sigmoid function</strong>. Figure 1 contains a plot. The logistic function smoothly varies between 0 and 1 . <u> We can also choose other functions instead of the logistic function, but we will see later that the logistic function has some advantages.</u> Let us interpret <span class="arithmatex">\(h_\theta(\mathbf{x})\)</span> as a probability:
$$
\mathrm{P}[y=1 \mid \mathbf{x}]=h_\theta(\mathbf{x}) \text { and } \mathrm{P}[y=0 \mid \mathbf{x}]=1-h_\theta(\mathbf{x}) .
$$
We then classify an example as belonging to class 1 if <span class="arithmatex">\(\mathrm{P}[y=1 \mid \mathbf{x}]&gt;1 / 2\)</span> which is equivalent to <span class="arithmatex">\(\langle\theta, \mathbf{x}\rangle&gt;0\)</span>. Logistic regression is called a discriminative classifier, since it directly estimates the parameters of the distribution <span class="arithmatex">\(\mathrm{P}[y \mid \mathbf{x}]\)</span>.</p>
<p><mark><strong>逻辑回归模型P(y|x)
    - 为伯努利分布
    - discriminative</strong> </mark></p>
<p><img alt="" src="https://i.imgur.com/ceMFXYL.png" /></p>
<h2 id="fitting-a-model">Fitting a Model</h2>
<p>We next discuss methods to learn the feature vector <span class="arithmatex">\(\theta\)</span> based on training data. In the lecture on linear regression, we saw that we can learn parameters based on modeling assumptions on how the training data was generated. Let us assume that we are given a training set <span class="arithmatex">\(\left.\mathcal{D}=\left\{\left(\mathbf{x}_1, y_1\right), \ldots, \mathbf{x}_n, y_n\right)\right\}\)</span>, where <span class="arithmatex">\(\mathbf{x}_i \in \mathbb{R}^d\)</span> and <span class="arithmatex">\(y_i \in\{0,1\}\)</span>. We furthermore assume that the feature vectors <span class="arithmatex">\(\mathbf{x}_i\)</span> are deterministic, but the labels are generated at random and independently across <span class="arithmatex">\(i\)</span> and follow the distribution</p>
<div class="arithmatex">\[
\mathrm{P}\left[y_i=1 \mid \mathbf{x}_i, \theta\right]=h_\theta\left(\mathbf{x}_i\right) \text { and } \mathrm{P}\left[y_i=0 \mid \mathbf{x}_i, \theta\right]=1-h_\theta\left(\mathbf{x}_i\right) .
\]</div>
<p>As before, <strong>we use the maximum likelihood estimate for the parameter <span class="arithmatex">\(\theta\)</span>, i.e., the estimate that maximizes the probability that the data <span class="arithmatex">\(\mathcal{D}\)</span> is from the model <span class="arithmatex">\(h_\theta\)</span>.</strong> This amounts to finding the parameter <span class="arithmatex">\(\theta\)</span> that maximizes the likelihood function, defined as</p>
<p><img alt="" src="https://i.imgur.com/h9uuA32.png" />
The second equality follows by our assumption that each label <span class="arithmatex">\(y_i\)</span> is generated independently, and the third equality follows from our assumption that the data is generated by the model <span class="arithmatex">\(h_\theta\)</span>.</p>
<p>As before, we maximize the log-likelihood instead (which we can do since log is strictly monotonic)</p>
<div class="arithmatex">\[
\log \mathcal{L}(\theta, \mathcal{D})=\sum_{i=1}^n y_i \log \left(h\left(\mathbf{x}_i\right)\right)+\left(1-y_i\right) \log \left(1-h_\theta\left(\mathbf{x}_i\right)\right) .
\]</div>
<ul>
<li><mark>Cross Entropy 就是通过对likelihood (MLE) <strong>取log</strong>化简得到的</mark></li>
</ul>
<p>In conclusion, the logistic regression estimate of the parameter <span class="arithmatex">\(\theta\)</span> is given by:</p>
<div class="arithmatex">\[
\hat{\theta}=\arg \min _\theta \sum_{i=1}^n y_i \log \left(1 / h_\theta\left(\mathbf{x}_i\right)\right)+\left(1-y_i\right) \log 1 /\left(1-h_\theta\left(\mathbf{x}_i\right)\right) .
\]</div>
<p>Before discussing how to solve this optimization problem, we note that in regression, we started our discussion by fitting a model by minimizing the loss between the data predicted by the model</p>
<p>and the training data, and we choose the quadratic loss function <span class="arithmatex">\(\operatorname{loss}(y, z)=(y-z)^2\)</span> to measure the loss. The theoretical justification was that learning with the quadratic loss corresponds to maximum likelihood estimation under Gaussian noise.</p>
<p>We could approach logistic regression in the same manner. Specifically, we can fit a model by solving</p>
<div class="arithmatex">\[
\hat{\theta}=\arg \min _\theta \frac{1}{n} \sum_{i=1}^n \operatorname{loss}\left(h_\theta\left(\mathbf{x}_i\right), y_i\right),
\]</div>
<p>where loss is some loss function. Logistic regression does exactly that, and chooses as the loss function the log-loss or the <strong>negative cross entropy</strong> defined as</p>
<div class="arithmatex">\[
\operatorname{loss}(y, z)=-y \log (z)-(1-y) \log (1-z)=y \log (1 / z)+(1-y) \log (1 /(1-z))
\]</div>
<h2 id="computing-the-logistic-regression-estimate-with-gradient-descent">Computing the logistic regression estimate with gradient descent</h2>
<p>The negative log-likelihood</p>
<div class="arithmatex">\[
-\log \mathcal{L}(\theta, \mathcal{D})=\sum_{i=1}^n \operatorname{loss}\left(g\left(\left\langle\theta, \mathbf{x}_i\right\rangle\right), y_i\right),
\]</div>
<p>with</p>
<div class="arithmatex">\[
\operatorname{loss}\left(g\left(\left\langle\theta, \mathbf{x}_i\right\rangle\right), y_i\right)=-y_i \log \left(g\left(\left\langle\theta, \mathbf{x}_i\right\rangle\right)\right)-\left(1-y_i\right) \log \left(1-g\left(\left\langle\theta, \mathbf{x}_i\right\rangle\right)\right)
\]</div>
<p>is convex since <span class="arithmatex">\(\operatorname{loss}\left(g\left(\left\langle\theta, \mathbf{x}_i\right\rangle\right), y_i\right)\)</span> is convex, as we show later. Thus, we can approximate the logistic regression estimate <span class="arithmatex">\(\hat{\theta}\)</span> with gradient descent:</p>
<div class="arithmatex">\[
\begin{aligned}
\theta^{k+1} &amp;=\theta^k-\alpha \nabla(-\log \mathcal{L}(\theta, \mathcal{D})) \\
&amp;=\theta^k-\alpha \sum_{i=1}^n \nabla \operatorname{loss}\left(g\left(\left\langle\theta, \mathbf{x}_i\right\rangle\right), y_i\right)
\end{aligned}
\]</div>
<p>In practice we would actually often use stochastic gradient descent to minimize the function above, because of computational reasons. The stochastic gradient method is similar to gradient descent, and will be discussed in detail later in this course.</p>
<p>We next compute the gradient of the loss above. Towards this goal, we first note that <mark> a convenient property of the logistic function is that its derivative can be written as:</mark></p>
<div class="arithmatex">\[
g^{\prime}(z)=\frac{-1}{\left(1+e^{-z}\right)^2} e^{-z}(-1)=\frac{1}{\left(1+e^{-z}\right)}\left(1-\frac{1}{\left(1+e^{-z}\right)}\right)=g(z)(1-g(z)),
\]</div>
<p>where the first equality is by the chain rule. With this relation, the gradients can be computed as</p>
<p><img alt="" src="https://i.imgur.com/oZG5YkS.png" /></p>
<p>where the second inequality follows from the convenient relation <span class="arithmatex">\(g^{\prime}(z)=g(z)(1-g(z))\)</span>, shown above.</p>
<p>Next, we show that the loss above is indeed convex in <span class="arithmatex">\(\theta\)</span>, as claimed. We can prove this as follows. Suppose that <span class="arithmatex">\(d=1\)</span>. Then the likelihood is a function <span class="arithmatex">\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span>. A differentiable function <span class="arithmatex">\(f\)</span> is convex if and only if its second derivative obeys <span class="arithmatex">\(f^{\prime \prime}(\mathbf{x}) \geq 0\)</span> for all <span class="arithmatex">\(\mathbf{x}\)</span>. For example <span class="arithmatex">\(x^2\)</span> is convex and it's second derivative is 1 . This generalizes to multivariate functions as follows. Define the Hessian of a multivariate function <span class="arithmatex">\(f: \mathbb{R}^d \rightarrow \mathbb{R}\)</span> as the <span class="arithmatex">\(d \times d\)</span> matrix <span class="arithmatex">\(\nabla^2 f(\mathbf{x})\)</span> with entries</p>
<div class="arithmatex">\[
\left[\nabla^2 f(\mathbf{x})\right]_{i j}=\frac{\partial}{\partial \theta_i} \frac{\partial}{\partial \theta_j} f(\mathbf{x}) .
\]</div>
<p>A multivariate function is convex if and only if <span class="arithmatex">\(\nabla^2 f(\mathbf{x})\)</span> is positive semidefinite (recall that a matrix <span class="arithmatex">\(\mathbf{A}\)</span> is positive semidefinite if <span class="arithmatex">\(\mathbf{z}^T \mathbf{A} \mathbf{z} \geq 0\)</span> for all <span class="arithmatex">\(\mathbf{z}\)</span> ).</p>
<p>Thus, all we need to do is to show that the Hessian of the loss if positive semidefinite. Towards this goal, we compute</p>
<p><img alt="" src="https://i.imgur.com/8eTWJQO.png" /></p>
<p>Thus, the Hessian can be written as</p>
<div class="arithmatex">\[
\nabla^2 \operatorname{loss}\left(y, h_\theta(\mathbf{x})\right)=g(\langle\theta, \mathbf{x}\rangle)(1-g(\langle\theta, \mathbf{x}\rangle)) \mathbf{x x}^T .
\]</div>
<p>Since <span class="arithmatex">\(g(\langle\theta, \mathbf{x}\rangle)(1-g(\langle\theta, \mathbf{x}\rangle))\)</span> is non-negative, and <span class="arithmatex">\(\mathbf{x x}^T\)</span> is positive semidefinite, the Hessian above is positive semidefinite, and therefore the loss is convex in <span class="arithmatex">\(\theta\)</span>. Since the negative log-likelihood <span class="arithmatex">\(-\log \mathcal{L}(\theta, \mathcal{D})\)</span> is s a sum of convex functions, it is also convex. As a consequence, gradient descent will converge to a minimizer.</p>
<h2 id="multiclass-logistic-regression">Multiclass logistic regression</h2>
<p>Let us next generalize logistic regression to the case with <span class="arithmatex">\(K&gt;2\)</span> classes instead of only two classes. This is called multiclass logistic regression or <strong>softmax regression</strong>.</p>
<p>A key idea is to use what is called a one-hot-encoding <span class="arithmatex">\(\mathbf{y} \in\{0,1\}^K\)</span> for representing the labels. The one-hot encoding <span class="arithmatex">\(\mathbf{y}\)</span> associated with a label <span class="arithmatex">\(y=k\)</span> is equal to 1 only at the position <span class="arithmatex">\(k\)</span> </p>
<div class="arithmatex">\[
[\mathbf{y}]_k=1 \text {, if } y=k, \quad \text { and } \quad[\mathbf{y}]_j=0 \text { for all } j \neq k \text {. }
\]</div>
<p>We associate a parameter vector <span class="arithmatex">\(\theta_k \in \mathbb{R}^d\)</span> with each class. For a given feature vector, we can associate a score with each class <span class="arithmatex">\(z_k=\left\langle\theta_k, \mathbf{x}\right\rangle\)</span>. Computing this score for each <span class="arithmatex">\(k\)</span> yields the vector <span class="arithmatex">\(\mathbf{z}=\left[z_1, \ldots, z_K\right]\)</span>. The generalization of the logistic function is the softmax function <span class="arithmatex">\(\sigma: \mathbb{R}^d \rightarrow[0,1]^d\)</span>, and is defined as:</p>
<div class="arithmatex">\[
[\sigma(\mathbf{z})]_j=\frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}, \quad \text { for } k=1, \ldots, K
\]</div>
<p>Note that the components of the sofmax function <span class="arithmatex">\(\sigma(\mathbf{z})\)</span> applied to any vector <span class="arithmatex">\(\mathbf{z}\)</span> sum to one and lie in the range <span class="arithmatex">\([0,1]\)</span>. Thus, they can be interpreted as a probability distribution (<mark>Categorical Distribution</mark>). Given a feature vector <span class="arithmatex">\(\mathbf{x}\)</span>, the model then predict its label as the index of the largest component of <span class="arithmatex">\(\sigma(\mathbf{z})\)</span>.</p>
<p>We can estimate the parameters <span class="arithmatex">\(\theta\)</span> based on training data in a similar manner as before. Specifically, by summarizing the parameter vectors with the matrix <span class="arithmatex">\(\Theta=\left[\theta_1, \ldots, \theta_K\right]\)</span>, we estimate the parameters by maximizing the likelihood or equivalently, minimizing the log-likeliehood:</p>
<div class="arithmatex">\[
\begin{aligned}
\hat{\Theta} &amp;=\arg \max _{\Theta} \mathcal{L}(\Theta, \mathcal{D}) \\
&amp;=\arg \max _{\Theta} \prod_{i=1}^n \mathrm{P}\left[y_i \mid \mathbf{x}_i, \Theta\right] \\
&amp;=\arg \max _{\Theta} \prod_{i=1}^n \prod_{k=1}^k \mathrm{P}\left[y_i=k \mid \mathbf{x}_i, \theta_k\right]^{\mathbb{1}\left\{y_i=k\right\}} \\
&amp;=\arg \min _{\Theta}-\sum_{i=1}^n \sum_{k=1}^K \mathbb{1}\left\{y_i=k\right\} \log \mathrm{P}\left[y_i=k \mid \mathbf{x}_i, \theta_k\right] \\
&amp;=\arg \min _{\Theta}-\sum_{i=1}^n \sum_{k=1}^K \mathbb{1}\left\{y_i=k\right\} \log \left(\frac{e^{\left\langle\theta_k, \mathbf{x}_i\right\rangle}}{\sum_{j=1}^K e^{\left\langle\theta_j, \mathbf{x}_i\right\rangle}}\right) .
\end{aligned}
\]</div>
<ul>
<li><mark>CE 就是通过对Likelihood (MLE) 取log化简得到的</mark></li>
</ul>
<p>As for the binary regression case, this is a convex optimization problem and can be solved with gradient descent.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../../1%20Regression/1.2%20Ridge%20Regression/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Ridge Regression 岭回归" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Ridge Regression 岭回归
            </div>
          </div>
        </a>
      
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.16e2a7d4.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.d6c3db9e.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>