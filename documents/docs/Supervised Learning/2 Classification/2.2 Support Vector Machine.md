# Support vector machines
Support vector machines are a class of model for classification. We already learned about a logistic regression as another model for classification. In logistic regression we learn a parameter vector $\boldsymbol{\theta}$ and classify an example as belonging to one of two (or $k$ ) classes based on a decision boundary. Specifically, in logistic regression, we classified an example as belonging to class one if the inner product $\langle\boldsymbol{\theta}, \mathbf{x}\rangle$ is larger than a threshold. 

In a similar spirit, Support Vector Machines (SVMs) learn decision boundaries. <mark>Contrary to our previous discussion on logistic regression, for SVMs we will not make any distributional assumption on the data.</mark>

- logistic regression assumes the data follow a Benouri distribution


As before, we consider a binary classification problem. Given a training dataset

$$
\mathcal{D}=\left\{\left(\mathbf{x}_1, y_1\right), \ldots,\left(\mathbf{x}_n, y_n\right)\right\} \in \mathbb{R}^d \times\{-1,1\},
$$

our goal is to find a classifier based on separating the two classes by a hyperplane. A classifier $f$ based on a hyperplane classifies a feature $\mathbf{x}$ as 1 if $\langle\boldsymbol{\theta}, \mathbf{x}\rangle \geq b$, where $b$ is a threshold, and as $-1$ otherwise, i.e.,
$$
f(\mathbf{x})= \begin{cases}1 & \langle\boldsymbol{\theta}, \mathbf{x}\rangle \geq b \\ -1 & \langle\boldsymbol{\theta}, \mathbf{x}\rangle<b .\end{cases}
$$

- In logistic regression, the threshold $b$ is 0

## The perceptron algorithm
As a first step towards SVMs, we discuss the perceptron algorithm. The perceptron algorithm finds a hyperplane that perfectly separates the $+1$ 's from the $-1$ 's. The goal of the perceptron algorithm is to find a parameter vector $\boldsymbol{\theta}$ such that for all $i$,
$$
\begin{cases}\left\langle\boldsymbol{\theta}, \mathbf{x}_i\right\rangle \geq b & \text { if } y_i=1, \\ \left\langle\boldsymbol{\theta}, \mathbf{x}_i\right\rangle<b & \text { if } y_i=-1 .\end{cases}
$$

**Such a parameter vector $\boldsymbol{\theta}$ does not necessarily exist:** 

- if it exists, we say that the data is linearly separable. This restriction to data that is linearly separable is a major issue limiting the applicability of perceptrons in its original form in practice, and we will discuss less restrictive SVMs later. Even if the data is linearly separable, then a parameter vector $\boldsymbol{\theta}$ and a threshold $b$ are not necessarily unique for separating the $+1$ 's from the $-1$ 's, thus it is not clear which hyperplane to choose.



## Hard-margin SVMs
**Suppose that the dataset $\mathcal{D}$ is is linearly separable.**

- Then there might be different hyperplanes that separate the data. The concept of a **margin** enables us to distinguish between "good" and "bad" separating hyperplanes. 
- Instead of finding an arbitrary hyperplane that separates the two classes, our goal is to find the particular hyperplane that <u>maximizes the distance from the decision boundary to the training points.</u> 
    - In more detail, we want to maximize a margin $m \geq 0$ such that
    - i) all points classified as $+1$ are on the positive side of the hyperplane, and their distance to the hyperplane is at least $m$, and
    - ii) all points classified as $-1$ are on the negative side of the hyperplane, and their distance to the hyperplane is at least $m$.

To find such a hyperplane, we first note that the distance of a point $\mathrm{x}$ to a hyperplane.

$$
\mathcal{H}=\{\mathbf{x}:\langle\boldsymbol{\theta}, \mathbf{x}\rangle=b\}
$$

is given by

$$
\frac{\left|\left\langle\boldsymbol{\theta}, \mathbf{x}-\mathbf{x}_0\right\rangle\right|}{\|\boldsymbol{\theta}\|_2}=\frac{|\langle\boldsymbol{\theta}, \mathbf{x}\rangle-b|}{\|\boldsymbol{\theta}\|_2} .
$$

> This can be seen as follows. By definition, the vector $\boldsymbol{\theta}$ is perpendicular to the hyperplane $\mathcal{H}$, i.e., to any vector that lies on the hyperplane. Specifically, consider any two points $\mathbf{x}_0, \mathbf{x}_1 \in \mathcal{H}$, and note that the vectors $\mathbf{x}_1-\mathbf{x}_0$, which lies on the hyperplane, is orthogonal to $\boldsymbol{\theta}$ : 
$$
\left\langle\mathbf{x}_1-\mathbf{x}_0, \boldsymbol{\theta}\right\rangle=\left\langle\mathbf{x}_1, \boldsymbol{\theta}\right\rangle-\left\langle\mathbf{x}_0, \boldsymbol{\theta}\right\rangle=b-b=0 .
$$
As a consequence of $\boldsymbol{\theta}$ being perpendicular to $\mathcal{H}$, the shortest distance of a point $\mathbf{x}$ to $\mathcal{H}$ is given by the projection of $\mathrm{x}-\mathrm{x}_0$ onto $\boldsymbol{\theta}$, where $\mathrm{x}_0 \in \mathcal{H}$. Thus, the distance of the point to the hyperplane is
$$
\frac{\left|\left\langle\boldsymbol{\theta}, \mathbf{x}-\mathbf{x}_0\right\rangle\right|}{\|\boldsymbol{\theta}\|_2}=\frac{|\langle\boldsymbol{\theta}, \mathbf{x}\rangle-b|}{\|\boldsymbol{\theta}\|_2} .
$$

Now let's return to the problem of finding a hyperplane that maximizes the margin. All datapoints are classified as $+1(-1)$ are on the positive (negative) side of the hyperplane, and their distance to the hyperplane is at least $m$, if for all $i=1, \ldots, n$
$$
y_i \frac{\left\langle\boldsymbol{\theta}, \mathbf{x}_i\right\rangle-b}{\|\boldsymbol{\theta}\|_2} \geq m
$$

This leads to the optimization problem:
$$
\max _{m, \boldsymbol{\theta}, b} m \quad \text { subject to } y_i \frac{\left\langle\boldsymbol{\theta}, \mathbf{x}_i\right\rangle-b}{\|\boldsymbol{\theta}\|_2} \geq m, i=1, \ldots, n \text {. }
$$
Here, we maximize over the parameters $b, \boldsymbol{\theta}$, and $m$. The margin $m$ is maximized if there exists at least one point on the negative side, and at least one on the positive side whose distance to the hyperplane is equal to $m$; these points are called the **support vectors**.

We next simplify the optimization problem by removing the variable $m$. 
- Note that the solution of the optimization problem is invariant to scaling $\boldsymbol{\theta}$ and $b$ by a positive scalar. Therefore, we can fix the norm of $\boldsymbol{\theta}$ to a particular value without changing the optimization problem. 
    - Setting $\|\boldsymbol{\theta}\|_2=1 / m$, the optimization problem becomes 


$$
\min _{\boldsymbol{\theta}, b} \frac{1}{2}\|\boldsymbol{\theta}\|_2^2 \quad \text { subject to } \quad y_i\left(\left\langle\boldsymbol{\theta}, \mathbf{x}_i\right\rangle-b\right) \geq 1 .
$$

<mark>This is the standard formulation of the hard-margin SVM optimization problem.</mark> 

## Soft-margin SVMs
**The hard-margin SVM optimization problem only has a solution if the data is linearly separable.**

**Even if the data is linearly separable, the solution of the hard-margin SVM optimization problem can vary significantly when including or excluding single points that change the support vectors.** 

<mark> Soft-margin SVMs address these issues by allowing points to violate the margin. The idea is to introduce a slack variable $\xi_i$ for each data point, which allows points to violate the margin</mark>:

$$
y_i\left(\left\langle\boldsymbol{\theta}, \mathbf{x}_i\right\rangle-b\right) \geq 1-\xi_i, \quad \xi_i \geq 0 .
$$

We take the slack variable in the objective into account by penalizing large values of $\xi_i$ :

$$
\operatorname{minimize} \frac{1}{2}\|\boldsymbol{\theta}\|_2^2+\lambda \sum_{i=1}^n \xi_i \quad \text { subject to } \quad y_i\left(\left\langle\boldsymbol{\theta}, \mathbf{x}_i\right\rangle-b\right) \geq 1-\xi_i, \quad \xi_i \geq 0 \text {, }
$$

where we optimize over $\boldsymbol{\theta}, b$, and the $\xi_i$ 's.

We next show that this optimization problem can be expressed equivalently as the following unconstrained optimization problem

$$
\operatorname{minimize} \frac{1}{2}\|\boldsymbol{\theta}\|_2^2+\lambda \sum_{i=1}^n \max \left(1-y_i\left(\left\langle\boldsymbol{\theta}, \mathbf{x}_i\right\rangle-b\right), 0\right) .
$$

- if $1-y_i\left(\left\langle\boldsymbol{\theta}, \mathbf{x}_i\right\rangle-b\right)$ <0: <==> support vectors, no slack needed
- otherwise: need slack, and the slack variable should also be as small as possible

> ![](https://i.imgur.com/JEuPyqZ.png)

## Lagrangian dualitz and SVMs
In order to gain additional insight into the optimization problem, and to see that the SVM algorithm can be written in terms of inner products between input feature vectors (which will turn out to be useful later), we next discuss duality theory, an important concept in optimization.

This part is only provided as extra reading material, it won't be part of the exam. Feel free to skip this subsection, and continue with the "Kernels" section.

### Langrangian dualitz and SVMs
![](https://i.imgur.com/W6nUzHC.png)
![](https://i.imgur.com/jZlUCqh.png)
![](https://i.imgur.com/00fAdQ6.png)




### Hard margin SVMs in the dual space
![](https://i.imgur.com/Xn1VZsU.png)
![](https://i.imgur.com/c3BWODY.png)
![](https://i.imgur.com/hFmKcJK.png)
